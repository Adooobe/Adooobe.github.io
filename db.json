{"meta":{"version":1,"warehouse":"4.0.2"},"models":{"Asset":[{"_id":"themes/apollo/source/favicon.png_origin","path":"favicon.png_origin","modified":0,"renderable":1},{"_id":"themes/apollo/source/favicon.png","path":"favicon.png","modified":0,"renderable":1},{"_id":"themes/apollo/source/scss/apollo.scss","path":"scss/apollo.scss","modified":0,"renderable":1},{"_id":"themes/apollo/source/font/sourcesanspro.woff","path":"font/sourcesanspro.woff","modified":0,"renderable":1},{"_id":"themes/apollo/source/font/sourcesanspro.woff2","path":"font/sourcesanspro.woff2","modified":0,"renderable":1},{"_id":"themes/apollo/source/css/apollo.css","path":"css/apollo.css","modified":0,"renderable":1}],"Cache":[{"_id":"source/_posts/apache-arrow.md","hash":"ff655839ebef3158ecc1b7b9c186521853fb1a7c","modified":1697705876332},{"_id":"source/_posts/hudi-cdc.md","hash":"cbf029f06381765f9bee9c1eed41b27140102a48","modified":1705304499746},{"_id":"source/_posts/bloom.md","hash":"aace8095dbf9d746cb7963ed46b935b6fc5c6f5e","modified":1700017621011},{"_id":"source/_posts/lsm-tree.md","hash":"e9e9a202b9881bcae87d12800038113d2286bf4d","modified":1697701890642},{"_id":"source/_posts/z-order.md","hash":"524387a9cf0e1e46dda1df1a9250983ae9ebb9a2","modified":1698026295691},{"_id":"source/_posts/lsm-tree/leveled_compaction.png","hash":"269c1dc419a0d8e0e10b60201e845e961d711bd8","modified":1697172400625},{"_id":"source/_posts/lsm-tree/lsm_tree.png","hash":"a05ee23c764f9ec6b8bbd8beb7966a0d1d948d53","modified":1696336037896},{"_id":"source/_posts/bloom/counting_bloom_filter.png","hash":"4341a36a29824acb03930ce2f78859b0e2552e54","modified":1696322485561},{"_id":"source/_posts/lsm-tree/range_query.png","hash":"77509946cf1f8c4bf0cb35688ba35c074b84ae55","modified":1696730612527},{"_id":"source/_posts/bloom/scalable_bloom_filter.png","hash":"b3465257148d1e6220207fe4905b12be5be476ee","modified":1696319286528},{"_id":"source/_posts/z-order/linear-order.png","hash":"615614221b1f0deb5af09b531fd7a5ad44741597","modified":1697679014790},{"_id":"source/_posts/z-order/z-order.png","hash":"92ac0fa8c4b8a79b47ef892416a48d311e0b12ce","modified":1697679005841},{"_id":"source/_posts/bloom/scalable_bloom_filter_slices.png","hash":"cceccf33246196109f987e7c2a6d79e8c1991baf","modified":1696320101570},{"_id":"source/_posts/lsm-tree/merge_policy.png","hash":"1722696f22a890231bb1c3dd0afdcc1c8672770d","modified":1696731591775},{"_id":"source/_posts/lsm-tree/sstable.png","hash":"cb853e8d7b8d27de9758742434584beb24ef58a0","modified":1696667281071},{"_id":"source/_posts/bloom/cuckoo_filter.png","hash":"9d5bd8c551ee5e928eeec9b4b03ddefcf0538050","modified":1698311184605},{"_id":"source/_posts/lsm-tree/sorted_runs.png","hash":"e14bf61c546d7831745d6231b4f79c70b13d2e39","modified":1696742589621},{"_id":"source/_posts/lsm-tree/sparse_index.png","hash":"c8ff0e09d371b6aeacba785d84f50f9f3396fad2","modified":1696657426319},{"_id":"source/_posts/lsm-tree/size_tiered_compaction.png","hash":"70926f3735c514da0b78b52f4c7a5feb9c8e4aae","modified":1697078833192},{"_id":"themes/apollo/.gitignore","hash":"a006beea0877a0aa3610ee00e73f62cb1d45125b","modified":1696033836459},{"_id":"themes/apollo/LICENSE","hash":"6e31ac9076bfc8f09ae47977419eee4edfb63e5b","modified":1696033836450},{"_id":"themes/apollo/README.md","hash":"b2cdcb158721c13eee6a2ae237d0488a738ed8c5","modified":1696033836458},{"_id":"themes/apollo/gulpfile.js","hash":"857a026b6643a2cd52c65d4ae0dc7fe9618206ee","modified":1696033836459},{"_id":"themes/apollo/package.json","hash":"eb1e76ec0b7ed6c6c7b2bd32b4f1e1bbe15800ca","modified":1696033836459},{"_id":"themes/apollo/_config.yml","hash":"d381543761117d88d3254855612dc9dd3f35c332","modified":1696300027974},{"_id":"themes/apollo/doc/doc-zh.md","hash":"2a6a81840cdaf497969268a12d8f62c98cc38103","modified":1696033836460},{"_id":"themes/apollo/doc/doc-en.md","hash":"409e931a444c02a57b64a0a44dde6e66c1881ca0","modified":1696033836460},{"_id":"themes/apollo/languages/zh-cn.yml","hash":"22a2d16fe8c0dddb016b5325b9b9c182a1b49ae1","modified":1696033836458},{"_id":"themes/apollo/layout/archive.jade","hash":"62797414355bf4474092bc3a32726c8340820ffb","modified":1696033836453},{"_id":"themes/apollo/languages/en.yml","hash":"ca168b190932229884db1de755ec2f793c758a16","modified":1696033836458},{"_id":"themes/apollo/layout/index.jade","hash":"55f2f1b4b5364a0e09cb18e1112664c6415fb881","modified":1696033836453},{"_id":"themes/apollo/layout/post.jade","hash":"245c26244c075c3632d1545c3b228ee9d112f15d","modified":1696033836453},{"_id":"themes/apollo/source/favicon.png_origin","hash":"a9cdcb22d1e74d5480323e19d1983de5a6873b8c","modified":1696033836457},{"_id":"themes/apollo/layout/mixins/paginator.jade","hash":"f4ee2fb61a32e199b48cf93771749edc8a007391","modified":1696033836453},{"_id":"themes/apollo/layout/mixins/post.jade","hash":"f23c6c40e14cdf16783b3c2baf736c9dce18408c","modified":1696033836453},{"_id":"themes/apollo/layout/partial/layout.jade","hash":"529c2ec06cfbc3d5b6d66dd320db50dfab5577a6","modified":1696033836452},{"_id":"themes/apollo/layout/partial/comment.jade","hash":"ff0a2c269c2434da2ac5529872f1d6184a71f96d","modified":1696033836451},{"_id":"themes/apollo/layout/partial/head.jade","hash":"43d2db73f0247a9a3ed00ecb95b537a872b7201a","modified":1696033836451},{"_id":"themes/apollo/layout/partial/nav.jade","hash":"f4842d9d3d763fbb823d112a6f49f24cc42a0ad4","modified":1696033836451},{"_id":"themes/apollo/layout/partial/scripts.jade","hash":"6bff591ae3d1ff6750f239c4c933ad61f009f36a","modified":1696033836452},{"_id":"themes/apollo/layout/partial/copyright.jade","hash":"74c38c649c1781669f30a05587f07459ae8944db","modified":1696048884120},{"_id":"themes/apollo/source/scss/apollo.scss","hash":"e0092f469264b55b25e0d441274f1c812147e7d1","modified":1696033836454},{"_id":"themes/apollo/source/css/apollo.css","hash":"714851e684a7fb17e21ddeed56a1112b432eaf94","modified":1696666644460},{"_id":"themes/apollo/source/scss/_partial/base.scss","hash":"ae967b2049ecb9b8c4e139ecce32fd9fb5358ac5","modified":1696033836455},{"_id":"themes/apollo/source/scss/_partial/copyright.scss","hash":"1309667e3000037170cfbb5b8c9c65f4ffcf6814","modified":1696033836455},{"_id":"themes/apollo/source/scss/_partial/header.scss","hash":"153bde88bf8ffeae4ffd813d8cc694dd83d33d94","modified":1696033836456},{"_id":"themes/apollo/source/scss/_partial/footer.scss","hash":"094aca6e52f11b139ac7980ca03fa7b9d8fc7b2f","modified":1696033836456},{"_id":"themes/apollo/source/scss/_partial/mq.scss","hash":"fc5dda52cfbb10e27e2471e03f4606fb3d588225","modified":1696033836455},{"_id":"themes/apollo/source/scss/_partial/home-post-list.scss","hash":"6b5c59f3d2295944f934aee2c1156012a3306d5d","modified":1696033836456},{"_id":"themes/apollo/source/scss/_partial/normalize.scss","hash":"fd0b27bed6f103ea95b08f698ea663ff576dbcf1","modified":1696033836456},{"_id":"themes/apollo/source/scss/_partial/post.scss","hash":"7a2579935781913e4c7bce71e3de9f1449107c51","modified":1696666424231},{"_id":"themes/apollo/source/font/sourcesanspro.woff","hash":"a6722c9b6439b7a020a9be3d3178970757a9265c","modified":1696033836457},{"_id":"themes/apollo/source/scss/_partial/archive-post-list.scss","hash":"d2f740a7d48349b7536777c795f82ab740836d0f","modified":1696033836455},{"_id":"themes/apollo/source/font/sourcesanspro.woff2","hash":"da65f527a8da65d5eb6721626d28cfdb46ab104a","modified":1696033836457},{"_id":"themes/apollo/source/favicon.png","hash":"db29234c8b16c9f841ad8f0801d77bbf40024ef7","modified":1696744340339},{"_id":"public/sitemap.xml","hash":"9d8bbc26a68357f41d1c2becc5313b0eaaae0d63","modified":1712472783654},{"_id":"public/sitemap.txt","hash":"c24bdde11a783bf5a6f8be199f109e7f4a0bbc8d","modified":1712064691733},{"_id":"public/atom.xml","hash":"145d6922e04132b4828c9f29b52b71d0a41d57ee","modified":1712472783654},{"_id":"public/2023/10/23/hudi-cdc/index.html","hash":"1c10458d4571bcd81f5a8dcaa63cb9e3cb571824","modified":1704113352950},{"_id":"public/2023/10/19/z-order/index.html","hash":"99e5de89b4210d890b728b15eae8bbed020324d9","modified":1704113352950},{"_id":"public/2023/10/08/apache-arrow/index.html","hash":"8c6d3e90a9aef8b90ebf241154ec24449738f3b9","modified":1704113352950},{"_id":"public/2023/10/02/lsm-tree/index.html","hash":"aa70e1690aaf6de6af5bcaf0dc4a1321386d257f","modified":1704113352950},{"_id":"public/archives/index.html","hash":"4e79fa201001d942ad47cc0de8ad378840a6af4d","modified":1704113352950},{"_id":"public/index.html","hash":"6bf1d2bd989416aa32a9d24a59634119eccb879a","modified":1712109510595},{"_id":"public/tags/bigdata/index.html","hash":"1c530a2e6998298497888bceb0109ddb165553c5","modified":1704113352950},{"_id":"public/tags/Bloom-Filter/index.html","hash":"e5a359743b0e0924988ef671595b190e1ece5f44","modified":1704113352950},{"_id":"public/tags/LSM-Storage/index.html","hash":"c7009a8a28c7bae742a39bd192272eeca425caf1","modified":1704113352950},{"_id":"public/tags/big-data-index/index.html","hash":"b891c5a3b47bb117ee9c511694773981d45eb5e8","modified":1704113352950},{"_id":"public/2023/10/02/bloom/index.html","hash":"2e1afabd38b2b15adcb2417eb051f5bc4b6d2640","modified":1712109510595},{"_id":"public/favicon.png_origin","hash":"a9cdcb22d1e74d5480323e19d1983de5a6873b8c","modified":1698375131624},{"_id":"public/scss/apollo.scss","hash":"e0092f469264b55b25e0d441274f1c812147e7d1","modified":1698375131624},{"_id":"public/favicon.png","hash":"db29234c8b16c9f841ad8f0801d77bbf40024ef7","modified":1698375131624},{"_id":"public/font/sourcesanspro.woff2","hash":"da65f527a8da65d5eb6721626d28cfdb46ab104a","modified":1698375131624},{"_id":"public/2023/10/02/bloom/counting_bloom_filter.png","hash":"4341a36a29824acb03930ce2f78859b0e2552e54","modified":1698375131624},{"_id":"public/2023/10/02/bloom/scalable_bloom_filter.png","hash":"b3465257148d1e6220207fe4905b12be5be476ee","modified":1698375131624},{"_id":"public/2023/10/02/bloom/scalable_bloom_filter_slices.png","hash":"cceccf33246196109f987e7c2a6d79e8c1991baf","modified":1698375131624},{"_id":"public/2023/10/19/z-order/linear-order.png","hash":"615614221b1f0deb5af09b531fd7a5ad44741597","modified":1698375131624},{"_id":"public/2023/10/19/z-order/z-order.png","hash":"92ac0fa8c4b8a79b47ef892416a48d311e0b12ce","modified":1698375131624},{"_id":"public/2023/10/02/lsm-tree/leveled_compaction.png","hash":"269c1dc419a0d8e0e10b60201e845e961d711bd8","modified":1698375131624},{"_id":"public/2023/10/02/lsm-tree/lsm_tree.png","hash":"a05ee23c764f9ec6b8bbd8beb7966a0d1d948d53","modified":1698375131624},{"_id":"public/2023/10/02/lsm-tree/range_query.png","hash":"77509946cf1f8c4bf0cb35688ba35c074b84ae55","modified":1698375131624},{"_id":"public/font/sourcesanspro.woff","hash":"a6722c9b6439b7a020a9be3d3178970757a9265c","modified":1698375131624},{"_id":"public/2023/10/02/bloom/cuckoo_filter.png","hash":"9d5bd8c551ee5e928eeec9b4b03ddefcf0538050","modified":1698375131624},{"_id":"public/2023/10/02/lsm-tree/merge_policy.png","hash":"1722696f22a890231bb1c3dd0afdcc1c8672770d","modified":1698375131624},{"_id":"public/2023/10/02/lsm-tree/sstable.png","hash":"cb853e8d7b8d27de9758742434584beb24ef58a0","modified":1698375131624},{"_id":"public/css/apollo.css","hash":"714851e684a7fb17e21ddeed56a1112b432eaf94","modified":1698375131624},{"_id":"public/2023/10/02/lsm-tree/sorted_runs.png","hash":"e14bf61c546d7831745d6231b4f79c70b13d2e39","modified":1698375131624},{"_id":"public/2023/10/02/lsm-tree/sparse_index.png","hash":"c8ff0e09d371b6aeacba785d84f50f9f3396fad2","modified":1698375131624},{"_id":"public/2023/10/02/lsm-tree/size_tiered_compaction.png","hash":"70926f3735c514da0b78b52f4c7a5feb9c8e4aae","modified":1698375131624},{"_id":"source/_posts/snowflake-stream.md","hash":"25daff109493e16abc7fa5a5828e0a5dd7528863","modified":1703833804831},{"_id":"source/_posts/mysql-cdc.md","hash":"e539d7ef542cf8b28979d1dcd94429b275231131","modified":1712472780908},{"_id":"source/_posts/snowflake-stream/buffer_pool.png","hash":"8dc58a1e9e515a63890a2b7fbf45ebaa77457715","modified":1700017572780},{"_id":"public/2023/12/21/mysql-cdc/index.html","hash":"20eeaf4b866983be38f29e494b77d91313edca70","modified":1712472783654},{"_id":"public/2023/10/30/snowflake-stream/index.html","hash":"21d19b20536ebbae91050fd96785ea94e15be570","modified":1704113352950},{"_id":"public/tags/stream-cdc-snowflake/index.html","hash":"718723abe72526d08b7d78632c61ed4297f831c2","modified":1704113352950},{"_id":"public/tags/cdc-mysql-binlog/index.html","hash":"c8b4f161ec092f3f7a2404ce4d7b215bdb2abc69","modified":1704113352950},{"_id":"public/2023/10/30/snowflake-stream/buffer_pool.png","hash":"8dc58a1e9e515a63890a2b7fbf45ebaa77457715","modified":1703206359417},{"_id":"source/_posts/mysql-cdc/table_map_event.png","hash":"91f1c6d0faae352fc00f263bf5d952e2b49900ae","modified":1712468813068},{"_id":"source/_posts/mysql-cdc/row_events.png","hash":"32cd9d1929bfbf3846f3bf45d27118859b40e618","modified":1712470597306},{"_id":"public/2023/12/21/mysql-cdc/table_map_event.png","hash":"91f1c6d0faae352fc00f263bf5d952e2b49900ae","modified":1712472783654},{"_id":"public/2023/12/21/mysql-cdc/row_events.png","hash":"32cd9d1929bfbf3846f3bf45d27118859b40e618","modified":1712472783654}],"Category":[],"Data":[],"Page":[],"Post":[{"title":"Apache Arrow For the first time","date":"2023-10-08T12:17:14.000Z","_content":"内存格式，和parquet文件存储格式差不多，但是某些行为上和parquet不一致，比如对待null值的处理是不一样的。\n至于为什么能节省序列化和反序列化，arrow/arrow flight/arrow2之间的爱恨情仇还得再看","source":"_posts/apache-arrow.md","raw":"---\ntitle: Apache Arrow For the first time\ndate: 2023-10-08 20:17:14\ntags: bigdata \n---\n内存格式，和parquet文件存储格式差不多，但是某些行为上和parquet不一致，比如对待null值的处理是不一样的。\n至于为什么能节省序列化和反序列化，arrow/arrow flight/arrow2之间的爱恨情仇还得再看","slug":"apache-arrow","published":1,"updated":"2023-10-19T08:57:56.332Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clo80pb0t00005cc3h1h812r4","content":"<p>内存格式，和parquet文件存储格式差不多，但是某些行为上和parquet不一致，比如对待null值的处理是不一样的。<br>至于为什么能节省序列化和反序列化，arrow/arrow flight/arrow2之间的爱恨情仇还得再看</p>\n","site":{"data":{}},"excerpt":"","more":"<p>内存格式，和parquet文件存储格式差不多，但是某些行为上和parquet不一致，比如对待null值的处理是不一样的。<br>至于为什么能节省序列化和反序列化，arrow/arrow flight/arrow2之间的爱恨情仇还得再看</p>\n"},{"title":"hudi-cdc","date":"2023-10-23T03:16:06.000Z","_content":"## Two choices\n\n### Diff\n\n* Based on Time-Travel\n* Without any redundant data\n* Low query performance\n* Improved by data split(file, bucket .etc)\n\n### CDF\n\n* Need addtional metadata column and writing cost\n* Better query performance\n\nIn CDF(DeltaLake, Hudi, Databend and Snowflake all chose CDF as CDC implementation), there are also some aspects that require our careful consideration as follows:\n\n* when to persistence：we have a large number of different operators applied in the database system; we need to identify which ones require the CDC (Change Data Capture) hidden columns to be persisted.\n* Write: Regarding different types of write operations in various lake formats, the corresponding changes in file metadata should be clarified. The first point that can be optimized under the CDF scheme is to determine, based on different operations, whether there is a need to persist CDC data. This means that some operation's change information is directly read from the CDC file, while for other operations, the modification information is extracted and converted from the ordinary data files. However, as each lake format has its own definition for different operations, a detailed analysis specific to the format is necessary. For instance, operations in DeltaLake like Insert Into/Insert Overwrite, Update, Delete, and Merge are what we normally understand. Meanwhile, Hudi introduces concepts like recordKey as a primary key and preCombineField for comparisons, meaning that even a simple Insert Into SQL syntax might actually correspond to an Update operation in practical logic.\n  * For the Insert Into operation, DeltaLake does not read any other existing files, but only adds new data files. As such, when executing the Insert Into operation under the CDF framework with DeltaLake, there is no need for additional data persistence; one only needs to load these batch files at query time and transform the data into the agreed output format. However, within Hudi, because of the logic that combines data or optimizes by writing data into existing small files, it is necessary to read and rewrite existing files; thus, persistence of CDC data is required.\n  * For the drop partition operation, both DeltaLake (not supported by the community version, but supported by Alibaba Cloud's EMR version) and Hudi directly mark all data files in that partition as deleted. There is no need to persist any information; during querying, these files are identified, loaded, and each record is marked as a delete operation type, along with other information such as timestamps.\n  * For the update operation, the most fundamental operation must be to load the data files that meet the where condition, update the data that satisfies the where condition, and then write the updated data along with the unmodified data directly into a new file. In this case, an expansion of the CDF (Capability Data File) capability is required.\n* Read:\n\n## Databend\n\n## Reference\n\n1. https://github.com/apache/hudi/blob/master/rfc/rfc-51/rfc-51.md#approvers\n2. https://github.com/apache/hudi/pull/5885/commits\n","source":"_posts/hudi-cdc.md","raw":"---\ntitle: hudi-cdc\ndate: 2023-10-23 11:16:06\ntags:\n---\n## Two choices\n\n### Diff\n\n* Based on Time-Travel\n* Without any redundant data\n* Low query performance\n* Improved by data split(file, bucket .etc)\n\n### CDF\n\n* Need addtional metadata column and writing cost\n* Better query performance\n\nIn CDF(DeltaLake, Hudi, Databend and Snowflake all chose CDF as CDC implementation), there are also some aspects that require our careful consideration as follows:\n\n* when to persistence：we have a large number of different operators applied in the database system; we need to identify which ones require the CDC (Change Data Capture) hidden columns to be persisted.\n* Write: Regarding different types of write operations in various lake formats, the corresponding changes in file metadata should be clarified. The first point that can be optimized under the CDF scheme is to determine, based on different operations, whether there is a need to persist CDC data. This means that some operation's change information is directly read from the CDC file, while for other operations, the modification information is extracted and converted from the ordinary data files. However, as each lake format has its own definition for different operations, a detailed analysis specific to the format is necessary. For instance, operations in DeltaLake like Insert Into/Insert Overwrite, Update, Delete, and Merge are what we normally understand. Meanwhile, Hudi introduces concepts like recordKey as a primary key and preCombineField for comparisons, meaning that even a simple Insert Into SQL syntax might actually correspond to an Update operation in practical logic.\n  * For the Insert Into operation, DeltaLake does not read any other existing files, but only adds new data files. As such, when executing the Insert Into operation under the CDF framework with DeltaLake, there is no need for additional data persistence; one only needs to load these batch files at query time and transform the data into the agreed output format. However, within Hudi, because of the logic that combines data or optimizes by writing data into existing small files, it is necessary to read and rewrite existing files; thus, persistence of CDC data is required.\n  * For the drop partition operation, both DeltaLake (not supported by the community version, but supported by Alibaba Cloud's EMR version) and Hudi directly mark all data files in that partition as deleted. There is no need to persist any information; during querying, these files are identified, loaded, and each record is marked as a delete operation type, along with other information such as timestamps.\n  * For the update operation, the most fundamental operation must be to load the data files that meet the where condition, update the data that satisfies the where condition, and then write the updated data along with the unmodified data directly into a new file. In this case, an expansion of the CDF (Capability Data File) capability is required.\n* Read:\n\n## Databend\n\n## Reference\n\n1. https://github.com/apache/hudi/blob/master/rfc/rfc-51/rfc-51.md#approvers\n2. https://github.com/apache/hudi/pull/5885/commits\n","slug":"hudi-cdc","published":1,"updated":"2024-01-15T07:41:39.746Z","_id":"clo80pb0w00015cc312pe9nf7","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"Two-choices\"><a href=\"#Two-choices\" class=\"headerlink\" title=\"Two choices\"></a>Two choices</h2><h3 id=\"Diff\"><a href=\"#Diff\" class=\"headerlink\" title=\"Diff\"></a>Diff</h3><ul>\n<li>Based on Time-Travel</li>\n<li>Without any redundant data</li>\n<li>Low query performance</li>\n<li>Improved by data split(file, bucket .etc)</li>\n</ul>\n<h3 id=\"CDF\"><a href=\"#CDF\" class=\"headerlink\" title=\"CDF\"></a>CDF</h3><ul>\n<li>Need addtional metadata column and writing cost</li>\n<li>Better query performance</li>\n</ul>\n<p>In CDF(DeltaLake, Hudi, Databend and Snowflake all chose CDF as CDC implementation), there are also some aspects that require our careful consideration as follows:</p>\n<ul>\n<li>when to persistence：we have a large number of different operators applied in the database system; we need to identify which ones require the CDC (Change Data Capture) hidden columns to be persisted.</li>\n<li>Write: Regarding different types of write operations in various lake formats, the corresponding changes in file metadata should be clarified. The first point that can be optimized under the CDF scheme is to determine, based on different operations, whether there is a need to persist CDC data. This means that some operation’s change information is directly read from the CDC file, while for other operations, the modification information is extracted and converted from the ordinary data files. However, as each lake format has its own definition for different operations, a detailed analysis specific to the format is necessary. For instance, operations in DeltaLake like Insert Into/Insert Overwrite, Update, Delete, and Merge are what we normally understand. Meanwhile, Hudi introduces concepts like recordKey as a primary key and preCombineField for comparisons, meaning that even a simple Insert Into SQL syntax might actually correspond to an Update operation in practical logic.<ul>\n<li>For the Insert Into operation, DeltaLake does not read any other existing files, but only adds new data files. As such, when executing the Insert Into operation under the CDF framework with DeltaLake, there is no need for additional data persistence; one only needs to load these batch files at query time and transform the data into the agreed output format. However, within Hudi, because of the logic that combines data or optimizes by writing data into existing small files, it is necessary to read and rewrite existing files; thus, persistence of CDC data is required.</li>\n<li>For the drop partition operation, both DeltaLake (not supported by the community version, but supported by Alibaba Cloud’s EMR version) and Hudi directly mark all data files in that partition as deleted. There is no need to persist any information; during querying, these files are identified, loaded, and each record is marked as a delete operation type, along with other information such as timestamps.</li>\n<li>For the update operation, the most fundamental operation must be to load the data files that meet the where condition, update the data that satisfies the where condition, and then write the updated data along with the unmodified data directly into a new file. In this case, an expansion of the CDF (Capability Data File) capability is required.</li>\n</ul>\n</li>\n<li>Read:</li>\n</ul>\n<h2 id=\"Databend\"><a href=\"#Databend\" class=\"headerlink\" title=\"Databend\"></a>Databend</h2><h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li><a href=\"https://github.com/apache/hudi/blob/master/rfc/rfc-51/rfc-51.md#approvers\">https://github.com/apache/hudi/blob/master/rfc/rfc-51/rfc-51.md#approvers</a></li>\n<li><a href=\"https://github.com/apache/hudi/pull/5885/commits\">https://github.com/apache/hudi/pull/5885/commits</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Two-choices\"><a href=\"#Two-choices\" class=\"headerlink\" title=\"Two choices\"></a>Two choices</h2><h3 id=\"Diff\"><a href=\"#Diff\" class=\"headerlink\" title=\"Diff\"></a>Diff</h3><ul>\n<li>Based on Time-Travel</li>\n<li>Without any redundant data</li>\n<li>Low query performance</li>\n<li>Improved by data split(file, bucket .etc)</li>\n</ul>\n<h3 id=\"CDF\"><a href=\"#CDF\" class=\"headerlink\" title=\"CDF\"></a>CDF</h3><ul>\n<li>Need addtional metadata column and writing cost</li>\n<li>Better query performance</li>\n</ul>\n<p>In CDF(DeltaLake, Hudi, Databend and Snowflake all chose CDF as CDC implementation), there are also some aspects that require our careful consideration as follows:</p>\n<ul>\n<li>when to persistence：we have a large number of different operators applied in the database system; we need to identify which ones require the CDC (Change Data Capture) hidden columns to be persisted.</li>\n<li>Write: Regarding different types of write operations in various lake formats, the corresponding changes in file metadata should be clarified. The first point that can be optimized under the CDF scheme is to determine, based on different operations, whether there is a need to persist CDC data. This means that some operation’s change information is directly read from the CDC file, while for other operations, the modification information is extracted and converted from the ordinary data files. However, as each lake format has its own definition for different operations, a detailed analysis specific to the format is necessary. For instance, operations in DeltaLake like Insert Into/Insert Overwrite, Update, Delete, and Merge are what we normally understand. Meanwhile, Hudi introduces concepts like recordKey as a primary key and preCombineField for comparisons, meaning that even a simple Insert Into SQL syntax might actually correspond to an Update operation in practical logic.<ul>\n<li>For the Insert Into operation, DeltaLake does not read any other existing files, but only adds new data files. As such, when executing the Insert Into operation under the CDF framework with DeltaLake, there is no need for additional data persistence; one only needs to load these batch files at query time and transform the data into the agreed output format. However, within Hudi, because of the logic that combines data or optimizes by writing data into existing small files, it is necessary to read and rewrite existing files; thus, persistence of CDC data is required.</li>\n<li>For the drop partition operation, both DeltaLake (not supported by the community version, but supported by Alibaba Cloud’s EMR version) and Hudi directly mark all data files in that partition as deleted. There is no need to persist any information; during querying, these files are identified, loaded, and each record is marked as a delete operation type, along with other information such as timestamps.</li>\n<li>For the update operation, the most fundamental operation must be to load the data files that meet the where condition, update the data that satisfies the where condition, and then write the updated data along with the unmodified data directly into a new file. In this case, an expansion of the CDF (Capability Data File) capability is required.</li>\n</ul>\n</li>\n<li>Read:</li>\n</ul>\n<h2 id=\"Databend\"><a href=\"#Databend\" class=\"headerlink\" title=\"Databend\"></a>Databend</h2><h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li><a href=\"https://github.com/apache/hudi/blob/master/rfc/rfc-51/rfc-51.md#approvers\">https://github.com/apache/hudi/blob/master/rfc/rfc-51/rfc-51.md#approvers</a></li>\n<li><a href=\"https://github.com/apache/hudi/pull/5885/commits\">https://github.com/apache/hudi/pull/5885/commits</a></li>\n</ol>\n"},{"title":"Bloom Filter Implementation and Optimization","date":"2023-10-02T11:11:52.000Z","mathjax":true,"_content":"# Bloom Filter\n\n## What is Bloom Filter\n\nA bloom filter is a probabilistic data structure that is based on hashing. It is extremely space efficient and is typically used to add elements to a set and test if an element is in a set. Though, the elements themselves are not added to a set. Instead a hash of the elements is added to the set.\n\n## Implementation\n\n```python\nimport hashlib\n\nclass BloomFilter:\n    def __init__(self, m, k):\n        self.m = m\n        self.k = k\n        self.data = [0]*m\n        self.n = 0\n    def insert(self, element):\n        if self.k == 1:\n            hash1 = h1(element) % self.m\n            self.data[hash1] = 1\n        elif self.k == 2:\n            hash1 = h1(element) % self.m\n            hash2 = h2(element) % self.m\n            self.data[hash1] = 1\n            self.data[hash2] = 1\n        self.n += 1\n    def search(self, element):\n        if self.k == 1:\n            hash1 = h1(element) % self.m\n            if self.data[hash1] == 0:\n                return \"Not in Bloom Filter\"\n        elif self.k == 2:\n            hash1 = h1(element) % self.m\n            hash2 = h2(element) % self.m\n            if self.data[hash1] == 0 or self.data[hash2] == 0:\n                return \"Not in Bloom Filter\"\n        p = (1.0 - ((1.0 - 1.0/self.m) * (self.k*self.n))) * self.k\n        return \"Might be in Bloom Filter with false positive probability \"+str(prob)\n\ndef h1(w):\n    h = hashlib.md5(w)\n    return hash(h.digest().encode('base64')[:6])%10\n\ndef h2(w):\n    h = hashlib.sha256(w)\n    return hash(h.digest().encode('base64')[:6])%10\n```\n\nIn this sample, we implement a simple Bloom Filter which has two hash function(controlled by *k*) to compute the position of input element in the bit array (named data, array size is *m*).\n\nWhen an element inserts into BloomFilter, two positions will be changed to 1 in the bit array computed by hash function so that if the element comes again, BloomFilter will get and check if the two positions are both 1. So when other element comes, if any position is not 1, the element doesn't exist in the data array.\n\nIt is worth noting that different elements can get same hash values computed by hash function. It means one grid in the bit array is not independently used by unique element but shared. In other words, it can be covered.\n\nSo, in practical applications, how do we determine the number of hash functions to choose? How much space should be allocated for the array? What is the expected number of elements to be stored? How do we control the error? You can refer to the calculation formulas to determine how to design a bloom filter.\n\nn = ceil(m / (-k / log(1 - exp(log(p) / k))))\n\np = pow(1 - exp(-k / (m / n)), k)\n\nm = ceil((n * log(p)) / log(1 / pow(2, log(2))));\n\nk = round((m / n) * log(2));\n\n## drawback\n\nAs mentioned above, Bloom Filter can creat false positives that Bloom Filter has a certain probability of mistakenly identifying non-existent elements.\n\nWe have two choices of parameters when building a bloom filter, `m` and `k`. They should each be chosen to dampen the number of false positives as much as possible while still maintaining whatever space requirement the filter needs.\n\nIf we have a bloom filter with `m` bits and `k` hash functions, the probability that a certain bit will still be zero after one insertion is\n\n$(1-1/m)^k$\n\nThen, after `n` insertions, the probability of it still being zero after `n` insertions is\n\n$(1-1/m)^{nk}$\n\nSo, that means the probability of a false positive is\n\n$(1-(1-1/m)^{nk})^k$\n\nIn each of these equations, raising the value of k (the number of hash functions) will make the probability of a false positive less likely. However, it is not computationally efficient to have an enormous value for *k*. To minimize this equation, we must choose the best *k*. We do it this way because we assume that the programmer has already chosen an **m** based on their space constraints and that they have some idea what their potential *n* will be. So the *k* value that minimizes that equation is\n\n$k=ln(2)⋅m/n$\n\n### Hudi Upsert\n\n### LSM-Tree\n\nSee [LSM-Tree](https://adooobe.github.io/2023/10/02/lsm-tree/)\n\n### Others\n\n1. [White List Question](https://zhuanlan.zhihu.com/p/294069121)\n2. [Redis Cache Breakdown](https://www.51cto.com/article/753025.html)\n\n## Improvement\n\nThere are two limitations that have always restricted the usage of Bloom Filter, bounded source and append-only. The following are several improvement methods that revolve around addressing these two issues.\n\n#### Scalable Bloom Filters (SBF)\n\n![scalable bloom filter structure](./bloom/scalable_bloom_filter.png#pic_center)\n\nwhen the filter reaches some fulfillness threshold, it becomes read-only and new bigger and writable filter is created in its place. If in its turn it becomes saturated, the operation is repeated. Every new filter, in order to keep the false positives rate close to the targeted one, has more hash functions than the previous filter.\n\nIn Scalable Bloom filter the membership test is applied on all created (read-only + writable) filters. If the item is missing in all filters, it's considered as not existing. In the other side, if one of the filters reports its existence, it means that the element may be in the dataset. An important point to notice here is that Scalable Bloom filter uses a variant of Bloom filters where the bit vector is divided in *k* slices where each stores *M/k* bits (*M* is the size of whole bit vector). Since the number of slices is equal to the number of hash functions, each hash function works on its own slice:\n\n![scalable bloom filter slices](scalable_bloom_filter_slices.png#pic_center)\n\nThanks to the slices each element is always described by *k* bits resulting on more robust filter without the elements more prone to the false positives than the others.\n\n#### Counting Bloom Filter (CBF)\n\nFor the second question, CBF provides ability to delete elements in Bloom Filters. But unfortunately, the premise is that we must ensure that the deleted element is present in the Bloom filter.\n\n![counting bloom filter structure](counting_bloom_filter.png#pic_center)\n\n#### Cuckoo Filter\n\nAs an “improved version” of the Bloom Filter, the Cuckoo Filter uses the minimum space cost to achieve a reduction in false positives and support for reverse deletion.\n\n![Cuckoo Filter](cuckoo_filter.png#pic_center)\n\nIt only uses two hash functions H1 and H2 (on two hash tables/buckets T1 and T2) for Cuckoo Filter to compute the postion of input elements. If an element E1 comes into Cuckoo Filter:\n\n1. Compute the position on hashtable(or bucket) T1 with H1 and insert if the position P1 empty\n2. If the P1 has other element, compute the position P2 on hashtable T2 with H2 and try to insert again\n3. If the P2 is still not empty, kick out the element of the position on T2 then insert E1 into P2\n\nThere are serveral issue on this design:\n\n1. kick-out loop\n2. how to solve the element kicked out\n3. size of hashtable\n\nLuckily, the designer makes a brilliant idea to solve all three problems at once, `MaxLoop` and `Resize`.\n\nCuckoo Filter would record the number of loop and when it comes to the `MaxLoop`, hash table would be resized and rehash (I think it needs to be allocated addtional memory to save the set of the elements and it can do rehash)\n\n## Reference\n\n1. https://www.waitingforcode.com/big-data-algorithms/scalable-bloom-filter/read\n2. Baquero, C., & Almeida, J. (2007, January). Scalable bloom filters. In European Conference on Principles of Data Mining and Knowledge Discovery (pp. 244-256). Springer, Berlin, Heidelberg.\n3. https://www.xiemingzhao.com/posts/cuckooFilter.html\n4. https://www.cs.cmu.edu/~dga/papers/cuckoo-conext2014.pdf\n","source":"_posts/bloom.md","raw":"---\ntitle: Bloom Filter Implementation and Optimization\ndate: 2023-10-02 19:11:52\ntags: Bloom-Filter\nmathjax: true\n---\n# Bloom Filter\n\n## What is Bloom Filter\n\nA bloom filter is a probabilistic data structure that is based on hashing. It is extremely space efficient and is typically used to add elements to a set and test if an element is in a set. Though, the elements themselves are not added to a set. Instead a hash of the elements is added to the set.\n\n## Implementation\n\n```python\nimport hashlib\n\nclass BloomFilter:\n    def __init__(self, m, k):\n        self.m = m\n        self.k = k\n        self.data = [0]*m\n        self.n = 0\n    def insert(self, element):\n        if self.k == 1:\n            hash1 = h1(element) % self.m\n            self.data[hash1] = 1\n        elif self.k == 2:\n            hash1 = h1(element) % self.m\n            hash2 = h2(element) % self.m\n            self.data[hash1] = 1\n            self.data[hash2] = 1\n        self.n += 1\n    def search(self, element):\n        if self.k == 1:\n            hash1 = h1(element) % self.m\n            if self.data[hash1] == 0:\n                return \"Not in Bloom Filter\"\n        elif self.k == 2:\n            hash1 = h1(element) % self.m\n            hash2 = h2(element) % self.m\n            if self.data[hash1] == 0 or self.data[hash2] == 0:\n                return \"Not in Bloom Filter\"\n        p = (1.0 - ((1.0 - 1.0/self.m) * (self.k*self.n))) * self.k\n        return \"Might be in Bloom Filter with false positive probability \"+str(prob)\n\ndef h1(w):\n    h = hashlib.md5(w)\n    return hash(h.digest().encode('base64')[:6])%10\n\ndef h2(w):\n    h = hashlib.sha256(w)\n    return hash(h.digest().encode('base64')[:6])%10\n```\n\nIn this sample, we implement a simple Bloom Filter which has two hash function(controlled by *k*) to compute the position of input element in the bit array (named data, array size is *m*).\n\nWhen an element inserts into BloomFilter, two positions will be changed to 1 in the bit array computed by hash function so that if the element comes again, BloomFilter will get and check if the two positions are both 1. So when other element comes, if any position is not 1, the element doesn't exist in the data array.\n\nIt is worth noting that different elements can get same hash values computed by hash function. It means one grid in the bit array is not independently used by unique element but shared. In other words, it can be covered.\n\nSo, in practical applications, how do we determine the number of hash functions to choose? How much space should be allocated for the array? What is the expected number of elements to be stored? How do we control the error? You can refer to the calculation formulas to determine how to design a bloom filter.\n\nn = ceil(m / (-k / log(1 - exp(log(p) / k))))\n\np = pow(1 - exp(-k / (m / n)), k)\n\nm = ceil((n * log(p)) / log(1 / pow(2, log(2))));\n\nk = round((m / n) * log(2));\n\n## drawback\n\nAs mentioned above, Bloom Filter can creat false positives that Bloom Filter has a certain probability of mistakenly identifying non-existent elements.\n\nWe have two choices of parameters when building a bloom filter, `m` and `k`. They should each be chosen to dampen the number of false positives as much as possible while still maintaining whatever space requirement the filter needs.\n\nIf we have a bloom filter with `m` bits and `k` hash functions, the probability that a certain bit will still be zero after one insertion is\n\n$(1-1/m)^k$\n\nThen, after `n` insertions, the probability of it still being zero after `n` insertions is\n\n$(1-1/m)^{nk}$\n\nSo, that means the probability of a false positive is\n\n$(1-(1-1/m)^{nk})^k$\n\nIn each of these equations, raising the value of k (the number of hash functions) will make the probability of a false positive less likely. However, it is not computationally efficient to have an enormous value for *k*. To minimize this equation, we must choose the best *k*. We do it this way because we assume that the programmer has already chosen an **m** based on their space constraints and that they have some idea what their potential *n* will be. So the *k* value that minimizes that equation is\n\n$k=ln(2)⋅m/n$\n\n### Hudi Upsert\n\n### LSM-Tree\n\nSee [LSM-Tree](https://adooobe.github.io/2023/10/02/lsm-tree/)\n\n### Others\n\n1. [White List Question](https://zhuanlan.zhihu.com/p/294069121)\n2. [Redis Cache Breakdown](https://www.51cto.com/article/753025.html)\n\n## Improvement\n\nThere are two limitations that have always restricted the usage of Bloom Filter, bounded source and append-only. The following are several improvement methods that revolve around addressing these two issues.\n\n#### Scalable Bloom Filters (SBF)\n\n![scalable bloom filter structure](./bloom/scalable_bloom_filter.png#pic_center)\n\nwhen the filter reaches some fulfillness threshold, it becomes read-only and new bigger and writable filter is created in its place. If in its turn it becomes saturated, the operation is repeated. Every new filter, in order to keep the false positives rate close to the targeted one, has more hash functions than the previous filter.\n\nIn Scalable Bloom filter the membership test is applied on all created (read-only + writable) filters. If the item is missing in all filters, it's considered as not existing. In the other side, if one of the filters reports its existence, it means that the element may be in the dataset. An important point to notice here is that Scalable Bloom filter uses a variant of Bloom filters where the bit vector is divided in *k* slices where each stores *M/k* bits (*M* is the size of whole bit vector). Since the number of slices is equal to the number of hash functions, each hash function works on its own slice:\n\n![scalable bloom filter slices](scalable_bloom_filter_slices.png#pic_center)\n\nThanks to the slices each element is always described by *k* bits resulting on more robust filter without the elements more prone to the false positives than the others.\n\n#### Counting Bloom Filter (CBF)\n\nFor the second question, CBF provides ability to delete elements in Bloom Filters. But unfortunately, the premise is that we must ensure that the deleted element is present in the Bloom filter.\n\n![counting bloom filter structure](counting_bloom_filter.png#pic_center)\n\n#### Cuckoo Filter\n\nAs an “improved version” of the Bloom Filter, the Cuckoo Filter uses the minimum space cost to achieve a reduction in false positives and support for reverse deletion.\n\n![Cuckoo Filter](cuckoo_filter.png#pic_center)\n\nIt only uses two hash functions H1 and H2 (on two hash tables/buckets T1 and T2) for Cuckoo Filter to compute the postion of input elements. If an element E1 comes into Cuckoo Filter:\n\n1. Compute the position on hashtable(or bucket) T1 with H1 and insert if the position P1 empty\n2. If the P1 has other element, compute the position P2 on hashtable T2 with H2 and try to insert again\n3. If the P2 is still not empty, kick out the element of the position on T2 then insert E1 into P2\n\nThere are serveral issue on this design:\n\n1. kick-out loop\n2. how to solve the element kicked out\n3. size of hashtable\n\nLuckily, the designer makes a brilliant idea to solve all three problems at once, `MaxLoop` and `Resize`.\n\nCuckoo Filter would record the number of loop and when it comes to the `MaxLoop`, hash table would be resized and rehash (I think it needs to be allocated addtional memory to save the set of the elements and it can do rehash)\n\n## Reference\n\n1. https://www.waitingforcode.com/big-data-algorithms/scalable-bloom-filter/read\n2. Baquero, C., & Almeida, J. (2007, January). Scalable bloom filters. In European Conference on Principles of Data Mining and Knowledge Discovery (pp. 244-256). Springer, Berlin, Heidelberg.\n3. https://www.xiemingzhao.com/posts/cuckooFilter.html\n4. https://www.cs.cmu.edu/~dga/papers/cuckoo-conext2014.pdf\n","slug":"bloom","published":1,"updated":"2023-11-15T03:07:01.011Z","_id":"clo80pb0x00035cc36g0v2e5w","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"Bloom-Filter\"><a href=\"#Bloom-Filter\" class=\"headerlink\" title=\"Bloom Filter\"></a>Bloom Filter</h1><h2 id=\"What-is-Bloom-Filter\"><a href=\"#What-is-Bloom-Filter\" class=\"headerlink\" title=\"What is Bloom Filter\"></a>What is Bloom Filter</h2><p>A bloom filter is a probabilistic data structure that is based on hashing. It is extremely space efficient and is typically used to add elements to a set and test if an element is in a set. Though, the elements themselves are not added to a set. Instead a hash of the elements is added to the set.</p>\n<h2 id=\"Implementation\"><a href=\"#Implementation\" class=\"headerlink\" title=\"Implementation\"></a>Implementation</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> hashlib</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">BloomFilter</span>:</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, m, k</span>):</span><br><span class=\"line\">        self.m = m</span><br><span class=\"line\">        self.k = k</span><br><span class=\"line\">        self.data = [<span class=\"number\">0</span>]*m</span><br><span class=\"line\">        self.n = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">insert</span>(<span class=\"params\">self, element</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.k == <span class=\"number\">1</span>:</span><br><span class=\"line\">            hash1 = h1(element) % self.m</span><br><span class=\"line\">            self.data[hash1] = <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">elif</span> self.k == <span class=\"number\">2</span>:</span><br><span class=\"line\">            hash1 = h1(element) % self.m</span><br><span class=\"line\">            hash2 = h2(element) % self.m</span><br><span class=\"line\">            self.data[hash1] = <span class=\"number\">1</span></span><br><span class=\"line\">            self.data[hash2] = <span class=\"number\">1</span></span><br><span class=\"line\">        self.n += <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">search</span>(<span class=\"params\">self, element</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.k == <span class=\"number\">1</span>:</span><br><span class=\"line\">            hash1 = h1(element) % self.m</span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.data[hash1] == <span class=\"number\">0</span>:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"string\">\"Not in Bloom Filter\"</span></span><br><span class=\"line\">        <span class=\"keyword\">elif</span> self.k == <span class=\"number\">2</span>:</span><br><span class=\"line\">            hash1 = h1(element) % self.m</span><br><span class=\"line\">            hash2 = h2(element) % self.m</span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.data[hash1] == <span class=\"number\">0</span> <span class=\"keyword\">or</span> self.data[hash2] == <span class=\"number\">0</span>:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"string\">\"Not in Bloom Filter\"</span></span><br><span class=\"line\">        p = (<span class=\"number\">1.0</span> - ((<span class=\"number\">1.0</span> - <span class=\"number\">1.0</span>/self.m) * (self.k*self.n))) * self.k</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">\"Might be in Bloom Filter with false positive probability \"</span>+<span class=\"built_in\">str</span>(prob)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">h1</span>(<span class=\"params\">w</span>):</span><br><span class=\"line\">    h = hashlib.md5(w)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">hash</span>(h.digest().encode(<span class=\"string\">'base64'</span>)[:<span class=\"number\">6</span>])%<span class=\"number\">10</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">h2</span>(<span class=\"params\">w</span>):</span><br><span class=\"line\">    h = hashlib.sha256(w)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">hash</span>(h.digest().encode(<span class=\"string\">'base64'</span>)[:<span class=\"number\">6</span>])%<span class=\"number\">10</span></span><br></pre></td></tr></table></figure>\n<p>In this sample, we implement a simple Bloom Filter which has two hash function(controlled by <em>k</em>) to compute the position of input element in the bit array (named data, array size is <em>m</em>).</p>\n<p>When an element inserts into BloomFilter, two positions will be changed to 1 in the bit array computed by hash function so that if the element comes again, BloomFilter will get and check if the two positions are both 1. So when other element comes, if any position is not 1, the element doesn’t exist in the data array.</p>\n<p>It is worth noting that different elements can get same hash values computed by hash function. It means one grid in the bit array is not independently used by unique element but shared. In other words, it can be covered.</p>\n<p>So, in practical applications, how do we determine the number of hash functions to choose? How much space should be allocated for the array? What is the expected number of elements to be stored? How do we control the error? You can refer to the calculation formulas to determine how to design a bloom filter.</p>\n<p>n = ceil(m / (-k / log(1 - exp(log(p) / k))))</p>\n<p>p = pow(1 - exp(-k / (m / n)), k)</p>\n<p>m = ceil((n * log(p)) / log(1 / pow(2, log(2))));</p>\n<p>k = round((m / n) * log(2));</p>\n<h2 id=\"drawback\"><a href=\"#drawback\" class=\"headerlink\" title=\"drawback\"></a>drawback</h2><p>As mentioned above, Bloom Filter can creat false positives that Bloom Filter has a certain probability of mistakenly identifying non-existent elements.</p>\n<p>We have two choices of parameters when building a bloom filter, <code>m</code> and <code>k</code>. They should each be chosen to dampen the number of false positives as much as possible while still maintaining whatever space requirement the filter needs.</p>\n<p>If we have a bloom filter with <code>m</code> bits and <code>k</code> hash functions, the probability that a certain bit will still be zero after one insertion is</p>\n<p><mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.566ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"10.927ex\" height=\"2.497ex\" role=\"img\" focusable=\"false\" viewbox=\"0 -853.7 4829.8 1103.7\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mo\"><path data-c=\"28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\"/></g><g data-mml-node=\"mn\" transform=\"translate(389,0)\"><path data-c=\"31\" d=\"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z\"/></g><g data-mml-node=\"mo\" transform=\"translate(1111.2,0)\"><path data-c=\"2212\" d=\"M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z\"/></g><g data-mml-node=\"mn\" transform=\"translate(2111.4,0)\"><path data-c=\"31\" d=\"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z\"/></g><g data-mml-node=\"TeXAtom\" data-mjx-texclass=\"ORD\" transform=\"translate(2611.4,0)\"><g data-mml-node=\"mo\"><path data-c=\"2F\" d=\"M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z\"/></g></g><g data-mml-node=\"mi\" transform=\"translate(3111.4,0)\"><path data-c=\"1D45A\" d=\"M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z\"/></g><g data-mml-node=\"msup\" transform=\"translate(3989.4,0)\"><g data-mml-node=\"mo\"><path data-c=\"29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\"/></g><g data-mml-node=\"mi\" transform=\"translate(422,363) scale(0.707)\"><path data-c=\"1D458\" d=\"M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z\"/></g></g></g></g></svg></mjx-container></p>\n<p>Then, after <code>n</code> insertions, the probability of it still being zero after <code>n</code> insertions is</p>\n<p><mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.566ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"11.887ex\" height=\"2.497ex\" role=\"img\" focusable=\"false\" viewbox=\"0 -853.7 5254.1 1103.7\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mo\"><path data-c=\"28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\"/></g><g data-mml-node=\"mn\" transform=\"translate(389,0)\"><path data-c=\"31\" d=\"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z\"/></g><g data-mml-node=\"mo\" transform=\"translate(1111.2,0)\"><path data-c=\"2212\" d=\"M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z\"/></g><g data-mml-node=\"mn\" transform=\"translate(2111.4,0)\"><path data-c=\"31\" d=\"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z\"/></g><g data-mml-node=\"TeXAtom\" data-mjx-texclass=\"ORD\" transform=\"translate(2611.4,0)\"><g data-mml-node=\"mo\"><path data-c=\"2F\" d=\"M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z\"/></g></g><g data-mml-node=\"mi\" transform=\"translate(3111.4,0)\"><path data-c=\"1D45A\" d=\"M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z\"/></g><g data-mml-node=\"msup\" transform=\"translate(3989.4,0)\"><g data-mml-node=\"mo\"><path data-c=\"29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\"/></g><g data-mml-node=\"TeXAtom\" transform=\"translate(422,363) scale(0.707)\" data-mjx-texclass=\"ORD\"><g data-mml-node=\"mi\"><path data-c=\"1D45B\" d=\"M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z\"/></g><g data-mml-node=\"mi\" transform=\"translate(600,0)\"><path data-c=\"1D458\" d=\"M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z\"/></g></g></g></g></g></svg></mjx-container></p>\n<p>So, that means the probability of a false positive is</p>\n<p><mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.566ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"18.566ex\" height=\"2.497ex\" role=\"img\" focusable=\"false\" viewbox=\"0 -853.7 8206 1103.7\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mo\"><path data-c=\"28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\"/></g><g data-mml-node=\"mn\" transform=\"translate(389,0)\"><path data-c=\"31\" d=\"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z\"/></g><g data-mml-node=\"mo\" transform=\"translate(1111.2,0)\"><path data-c=\"2212\" d=\"M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z\"/></g><g data-mml-node=\"mo\" transform=\"translate(2111.4,0)\"><path data-c=\"28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\"/></g><g data-mml-node=\"mn\" transform=\"translate(2500.4,0)\"><path data-c=\"31\" d=\"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z\"/></g><g data-mml-node=\"mo\" transform=\"translate(3222.7,0)\"><path data-c=\"2212\" d=\"M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z\"/></g><g data-mml-node=\"mn\" transform=\"translate(4222.9,0)\"><path data-c=\"31\" d=\"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z\"/></g><g data-mml-node=\"TeXAtom\" data-mjx-texclass=\"ORD\" transform=\"translate(4722.9,0)\"><g data-mml-node=\"mo\"><path data-c=\"2F\" d=\"M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z\"/></g></g><g data-mml-node=\"mi\" transform=\"translate(5222.9,0)\"><path data-c=\"1D45A\" d=\"M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z\"/></g><g data-mml-node=\"msup\" transform=\"translate(6100.9,0)\"><g data-mml-node=\"mo\"><path data-c=\"29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\"/></g><g data-mml-node=\"TeXAtom\" transform=\"translate(422,363) scale(0.707)\" data-mjx-texclass=\"ORD\"><g data-mml-node=\"mi\"><path data-c=\"1D45B\" d=\"M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z\"/></g><g data-mml-node=\"mi\" transform=\"translate(600,0)\"><path data-c=\"1D458\" d=\"M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z\"/></g></g></g><g data-mml-node=\"msup\" transform=\"translate(7365.6,0)\"><g data-mml-node=\"mo\"><path data-c=\"29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\"/></g><g data-mml-node=\"mi\" transform=\"translate(422,363) scale(0.707)\"><path data-c=\"1D458\" d=\"M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z\"/></g></g></g></g></svg></mjx-container></p>\n<p>In each of these equations, raising the value of k (the number of hash functions) will make the probability of a false positive less likely. However, it is not computationally efficient to have an enormous value for <em>k</em>. To minimize this equation, we must choose the best <em>k</em>. We do it this way because we assume that the programmer has already chosen an <strong>m</strong> based on their space constraints and that they have some idea what their potential <em>n</em> will be. So the <em>k</em> value that minimizes that equation is</p>\n<p><mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.566ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"15.229ex\" height=\"2.262ex\" role=\"img\" focusable=\"false\" viewbox=\"0 -750 6731 1000\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><path data-c=\"1D458\" d=\"M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z\"/></g><g data-mml-node=\"mo\" transform=\"translate(798.8,0)\"><path data-c=\"3D\" d=\"M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z\"/></g><g data-mml-node=\"mi\" transform=\"translate(1854.6,0)\"><path data-c=\"1D459\" d=\"M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z\"/></g><g data-mml-node=\"mi\" transform=\"translate(2152.6,0)\"><path data-c=\"1D45B\" d=\"M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z\"/></g><g data-mml-node=\"mo\" transform=\"translate(2752.6,0)\"><path data-c=\"28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\"/></g><g data-mml-node=\"mn\" transform=\"translate(3141.6,0)\"><path data-c=\"32\" d=\"M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z\"/></g><g data-mml-node=\"mo\" transform=\"translate(3641.6,0)\"><path data-c=\"29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\"/></g><g data-mml-node=\"mo\" transform=\"translate(4252.8,0)\"><path data-c=\"22C5\" d=\"M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z\"/></g><g data-mml-node=\"mi\" transform=\"translate(4753,0)\"><path data-c=\"1D45A\" d=\"M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z\"/></g><g data-mml-node=\"TeXAtom\" data-mjx-texclass=\"ORD\" transform=\"translate(5631,0)\"><g data-mml-node=\"mo\"><path data-c=\"2F\" d=\"M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z\"/></g></g><g data-mml-node=\"mi\" transform=\"translate(6131,0)\"><path data-c=\"1D45B\" d=\"M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z\"/></g></g></g></svg></mjx-container></p>\n<h3 id=\"Hudi-Upsert\"><a href=\"#Hudi-Upsert\" class=\"headerlink\" title=\"Hudi Upsert\"></a>Hudi Upsert</h3><h3 id=\"LSM-Tree\"><a href=\"#LSM-Tree\" class=\"headerlink\" title=\"LSM-Tree\"></a>LSM-Tree</h3><p>See <a href=\"https://adooobe.github.io/2023/10/02/lsm-tree/\">LSM-Tree</a></p>\n<h3 id=\"Others\"><a href=\"#Others\" class=\"headerlink\" title=\"Others\"></a>Others</h3><ol>\n<li><a href=\"https://zhuanlan.zhihu.com/p/294069121\">White List Question</a></li>\n<li><a href=\"https://www.51cto.com/article/753025.html\">Redis Cache Breakdown</a></li>\n</ol>\n<h2 id=\"Improvement\"><a href=\"#Improvement\" class=\"headerlink\" title=\"Improvement\"></a>Improvement</h2><p>There are two limitations that have always restricted the usage of Bloom Filter, bounded source and append-only. The following are several improvement methods that revolve around addressing these two issues.</p>\n<h4 id=\"Scalable-Bloom-Filters-SBF\"><a href=\"#Scalable-Bloom-Filters-SBF\" class=\"headerlink\" title=\"Scalable Bloom Filters (SBF)\"></a>Scalable Bloom Filters (SBF)</h4><p><img src=\"/2023/10/02/bloom/scalable_bloom_filter.png#pic_center\" alt=\"scalable bloom filter structure\"></p>\n<p>when the filter reaches some fulfillness threshold, it becomes read-only and new bigger and writable filter is created in its place. If in its turn it becomes saturated, the operation is repeated. Every new filter, in order to keep the false positives rate close to the targeted one, has more hash functions than the previous filter.</p>\n<p>In Scalable Bloom filter the membership test is applied on all created (read-only + writable) filters. If the item is missing in all filters, it’s considered as not existing. In the other side, if one of the filters reports its existence, it means that the element may be in the dataset. An important point to notice here is that Scalable Bloom filter uses a variant of Bloom filters where the bit vector is divided in <em>k</em> slices where each stores <em>M/k</em> bits (<em>M</em> is the size of whole bit vector). Since the number of slices is equal to the number of hash functions, each hash function works on its own slice:</p>\n<p><img src=\"/2023/10/02/bloom/scalable_bloom_filter_slices.png#pic_center\" alt=\"scalable bloom filter slices\"></p>\n<p>Thanks to the slices each element is always described by <em>k</em> bits resulting on more robust filter without the elements more prone to the false positives than the others.</p>\n<h4 id=\"Counting-Bloom-Filter-CBF\"><a href=\"#Counting-Bloom-Filter-CBF\" class=\"headerlink\" title=\"Counting Bloom Filter (CBF)\"></a>Counting Bloom Filter (CBF)</h4><p>For the second question, CBF provides ability to delete elements in Bloom Filters. But unfortunately, the premise is that we must ensure that the deleted element is present in the Bloom filter.</p>\n<p><img src=\"/2023/10/02/bloom/counting_bloom_filter.png#pic_center\" alt=\"counting bloom filter structure\"></p>\n<h4 id=\"Cuckoo-Filter\"><a href=\"#Cuckoo-Filter\" class=\"headerlink\" title=\"Cuckoo Filter\"></a>Cuckoo Filter</h4><p>As an “improved version” of the Bloom Filter, the Cuckoo Filter uses the minimum space cost to achieve a reduction in false positives and support for reverse deletion.</p>\n<p><img src=\"/2023/10/02/bloom/cuckoo_filter.png#pic_center\" alt=\"Cuckoo Filter\"></p>\n<p>It only uses two hash functions H1 and H2 (on two hash tables/buckets T1 and T2) for Cuckoo Filter to compute the postion of input elements. If an element E1 comes into Cuckoo Filter:</p>\n<ol>\n<li>Compute the position on hashtable(or bucket) T1 with H1 and insert if the position P1 empty</li>\n<li>If the P1 has other element, compute the position P2 on hashtable T2 with H2 and try to insert again</li>\n<li>If the P2 is still not empty, kick out the element of the position on T2 then insert E1 into P2</li>\n</ol>\n<p>There are serveral issue on this design:</p>\n<ol>\n<li>kick-out loop</li>\n<li>how to solve the element kicked out</li>\n<li>size of hashtable</li>\n</ol>\n<p>Luckily, the designer makes a brilliant idea to solve all three problems at once, <code>MaxLoop</code> and <code>Resize</code>.</p>\n<p>Cuckoo Filter would record the number of loop and when it comes to the <code>MaxLoop</code>, hash table would be resized and rehash (I think it needs to be allocated addtional memory to save the set of the elements and it can do rehash)</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li><a href=\"https://www.waitingforcode.com/big-data-algorithms/scalable-bloom-filter/read\">https://www.waitingforcode.com/big-data-algorithms/scalable-bloom-filter/read</a></li>\n<li>Baquero, C., &amp; Almeida, J. (2007, January). Scalable bloom filters. In European Conference on Principles of Data Mining and Knowledge Discovery (pp. 244-256). Springer, Berlin, Heidelberg.</li>\n<li><a href=\"https://www.xiemingzhao.com/posts/cuckooFilter.html\">https://www.xiemingzhao.com/posts/cuckooFilter.html</a></li>\n<li><a href=\"https://www.cs.cmu.edu/~dga/papers/cuckoo-conext2014.pdf\">https://www.cs.cmu.edu/~dga/papers/cuckoo-conext2014.pdf</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Bloom-Filter\"><a href=\"#Bloom-Filter\" class=\"headerlink\" title=\"Bloom Filter\"></a>Bloom Filter</h1><h2 id=\"What-is-Bloom-Filter\"><a href=\"#What-is-Bloom-Filter\" class=\"headerlink\" title=\"What is Bloom Filter\"></a>What is Bloom Filter</h2><p>A bloom filter is a probabilistic data structure that is based on hashing. It is extremely space efficient and is typically used to add elements to a set and test if an element is in a set. Though, the elements themselves are not added to a set. Instead a hash of the elements is added to the set.</p>\n<h2 id=\"Implementation\"><a href=\"#Implementation\" class=\"headerlink\" title=\"Implementation\"></a>Implementation</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> hashlib</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">BloomFilter</span>:</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, m, k</span>):</span><br><span class=\"line\">        self.m = m</span><br><span class=\"line\">        self.k = k</span><br><span class=\"line\">        self.data = [<span class=\"number\">0</span>]*m</span><br><span class=\"line\">        self.n = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">insert</span>(<span class=\"params\">self, element</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.k == <span class=\"number\">1</span>:</span><br><span class=\"line\">            hash1 = h1(element) % self.m</span><br><span class=\"line\">            self.data[hash1] = <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">elif</span> self.k == <span class=\"number\">2</span>:</span><br><span class=\"line\">            hash1 = h1(element) % self.m</span><br><span class=\"line\">            hash2 = h2(element) % self.m</span><br><span class=\"line\">            self.data[hash1] = <span class=\"number\">1</span></span><br><span class=\"line\">            self.data[hash2] = <span class=\"number\">1</span></span><br><span class=\"line\">        self.n += <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">search</span>(<span class=\"params\">self, element</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.k == <span class=\"number\">1</span>:</span><br><span class=\"line\">            hash1 = h1(element) % self.m</span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.data[hash1] == <span class=\"number\">0</span>:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"string\">\"Not in Bloom Filter\"</span></span><br><span class=\"line\">        <span class=\"keyword\">elif</span> self.k == <span class=\"number\">2</span>:</span><br><span class=\"line\">            hash1 = h1(element) % self.m</span><br><span class=\"line\">            hash2 = h2(element) % self.m</span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.data[hash1] == <span class=\"number\">0</span> <span class=\"keyword\">or</span> self.data[hash2] == <span class=\"number\">0</span>:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"string\">\"Not in Bloom Filter\"</span></span><br><span class=\"line\">        p = (<span class=\"number\">1.0</span> - ((<span class=\"number\">1.0</span> - <span class=\"number\">1.0</span>/self.m) * (self.k*self.n))) * self.k</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">\"Might be in Bloom Filter with false positive probability \"</span>+<span class=\"built_in\">str</span>(prob)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">h1</span>(<span class=\"params\">w</span>):</span><br><span class=\"line\">    h = hashlib.md5(w)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">hash</span>(h.digest().encode(<span class=\"string\">'base64'</span>)[:<span class=\"number\">6</span>])%<span class=\"number\">10</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">h2</span>(<span class=\"params\">w</span>):</span><br><span class=\"line\">    h = hashlib.sha256(w)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">hash</span>(h.digest().encode(<span class=\"string\">'base64'</span>)[:<span class=\"number\">6</span>])%<span class=\"number\">10</span></span><br></pre></td></tr></table></figure>\n<p>In this sample, we implement a simple Bloom Filter which has two hash function(controlled by <em>k</em>) to compute the position of input element in the bit array (named data, array size is <em>m</em>).</p>\n<p>When an element inserts into BloomFilter, two positions will be changed to 1 in the bit array computed by hash function so that if the element comes again, BloomFilter will get and check if the two positions are both 1. So when other element comes, if any position is not 1, the element doesn’t exist in the data array.</p>\n<p>It is worth noting that different elements can get same hash values computed by hash function. It means one grid in the bit array is not independently used by unique element but shared. In other words, it can be covered.</p>\n<p>So, in practical applications, how do we determine the number of hash functions to choose? How much space should be allocated for the array? What is the expected number of elements to be stored? How do we control the error? You can refer to the calculation formulas to determine how to design a bloom filter.</p>\n<p>n = ceil(m / (-k / log(1 - exp(log(p) / k))))</p>\n<p>p = pow(1 - exp(-k / (m / n)), k)</p>\n<p>m = ceil((n * log(p)) / log(1 / pow(2, log(2))));</p>\n<p>k = round((m / n) * log(2));</p>\n<h2 id=\"drawback\"><a href=\"#drawback\" class=\"headerlink\" title=\"drawback\"></a>drawback</h2><p>As mentioned above, Bloom Filter can creat false positives that Bloom Filter has a certain probability of mistakenly identifying non-existent elements.</p>\n<p>We have two choices of parameters when building a bloom filter, <code>m</code> and <code>k</code>. They should each be chosen to dampen the number of false positives as much as possible while still maintaining whatever space requirement the filter needs.</p>\n<p>If we have a bloom filter with <code>m</code> bits and <code>k</code> hash functions, the probability that a certain bit will still be zero after one insertion is</p>\n<p><mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.566ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"10.927ex\" height=\"2.497ex\" role=\"img\" focusable=\"false\" viewbox=\"0 -853.7 4829.8 1103.7\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mo\"><path data-c=\"28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\"/></g><g data-mml-node=\"mn\" transform=\"translate(389,0)\"><path data-c=\"31\" d=\"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z\"/></g><g data-mml-node=\"mo\" transform=\"translate(1111.2,0)\"><path data-c=\"2212\" d=\"M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z\"/></g><g data-mml-node=\"mn\" transform=\"translate(2111.4,0)\"><path data-c=\"31\" d=\"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z\"/></g><g data-mml-node=\"TeXAtom\" data-mjx-texclass=\"ORD\" transform=\"translate(2611.4,0)\"><g data-mml-node=\"mo\"><path data-c=\"2F\" d=\"M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z\"/></g></g><g data-mml-node=\"mi\" transform=\"translate(3111.4,0)\"><path data-c=\"1D45A\" d=\"M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z\"/></g><g data-mml-node=\"msup\" transform=\"translate(3989.4,0)\"><g data-mml-node=\"mo\"><path data-c=\"29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\"/></g><g data-mml-node=\"mi\" transform=\"translate(422,363) scale(0.707)\"><path data-c=\"1D458\" d=\"M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z\"/></g></g></g></g></svg></mjx-container></p>\n<p>Then, after <code>n</code> insertions, the probability of it still being zero after <code>n</code> insertions is</p>\n<p><mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.566ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"11.887ex\" height=\"2.497ex\" role=\"img\" focusable=\"false\" viewbox=\"0 -853.7 5254.1 1103.7\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mo\"><path data-c=\"28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\"/></g><g data-mml-node=\"mn\" transform=\"translate(389,0)\"><path data-c=\"31\" d=\"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z\"/></g><g data-mml-node=\"mo\" transform=\"translate(1111.2,0)\"><path data-c=\"2212\" d=\"M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z\"/></g><g data-mml-node=\"mn\" transform=\"translate(2111.4,0)\"><path data-c=\"31\" d=\"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z\"/></g><g data-mml-node=\"TeXAtom\" data-mjx-texclass=\"ORD\" transform=\"translate(2611.4,0)\"><g data-mml-node=\"mo\"><path data-c=\"2F\" d=\"M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z\"/></g></g><g data-mml-node=\"mi\" transform=\"translate(3111.4,0)\"><path data-c=\"1D45A\" d=\"M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z\"/></g><g data-mml-node=\"msup\" transform=\"translate(3989.4,0)\"><g data-mml-node=\"mo\"><path data-c=\"29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\"/></g><g data-mml-node=\"TeXAtom\" transform=\"translate(422,363) scale(0.707)\" data-mjx-texclass=\"ORD\"><g data-mml-node=\"mi\"><path data-c=\"1D45B\" d=\"M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z\"/></g><g data-mml-node=\"mi\" transform=\"translate(600,0)\"><path data-c=\"1D458\" d=\"M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z\"/></g></g></g></g></g></svg></mjx-container></p>\n<p>So, that means the probability of a false positive is</p>\n<p><mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.566ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"18.566ex\" height=\"2.497ex\" role=\"img\" focusable=\"false\" viewbox=\"0 -853.7 8206 1103.7\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mo\"><path data-c=\"28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\"/></g><g data-mml-node=\"mn\" transform=\"translate(389,0)\"><path data-c=\"31\" d=\"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z\"/></g><g data-mml-node=\"mo\" transform=\"translate(1111.2,0)\"><path data-c=\"2212\" d=\"M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z\"/></g><g data-mml-node=\"mo\" transform=\"translate(2111.4,0)\"><path data-c=\"28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\"/></g><g data-mml-node=\"mn\" transform=\"translate(2500.4,0)\"><path data-c=\"31\" d=\"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z\"/></g><g data-mml-node=\"mo\" transform=\"translate(3222.7,0)\"><path data-c=\"2212\" d=\"M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z\"/></g><g data-mml-node=\"mn\" transform=\"translate(4222.9,0)\"><path data-c=\"31\" d=\"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z\"/></g><g data-mml-node=\"TeXAtom\" data-mjx-texclass=\"ORD\" transform=\"translate(4722.9,0)\"><g data-mml-node=\"mo\"><path data-c=\"2F\" d=\"M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z\"/></g></g><g data-mml-node=\"mi\" transform=\"translate(5222.9,0)\"><path data-c=\"1D45A\" d=\"M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z\"/></g><g data-mml-node=\"msup\" transform=\"translate(6100.9,0)\"><g data-mml-node=\"mo\"><path data-c=\"29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\"/></g><g data-mml-node=\"TeXAtom\" transform=\"translate(422,363) scale(0.707)\" data-mjx-texclass=\"ORD\"><g data-mml-node=\"mi\"><path data-c=\"1D45B\" d=\"M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z\"/></g><g data-mml-node=\"mi\" transform=\"translate(600,0)\"><path data-c=\"1D458\" d=\"M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z\"/></g></g></g><g data-mml-node=\"msup\" transform=\"translate(7365.6,0)\"><g data-mml-node=\"mo\"><path data-c=\"29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\"/></g><g data-mml-node=\"mi\" transform=\"translate(422,363) scale(0.707)\"><path data-c=\"1D458\" d=\"M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z\"/></g></g></g></g></svg></mjx-container></p>\n<p>In each of these equations, raising the value of k (the number of hash functions) will make the probability of a false positive less likely. However, it is not computationally efficient to have an enormous value for <em>k</em>. To minimize this equation, we must choose the best <em>k</em>. We do it this way because we assume that the programmer has already chosen an <strong>m</strong> based on their space constraints and that they have some idea what their potential <em>n</em> will be. So the <em>k</em> value that minimizes that equation is</p>\n<p><mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.566ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"15.229ex\" height=\"2.262ex\" role=\"img\" focusable=\"false\" viewbox=\"0 -750 6731 1000\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><path data-c=\"1D458\" d=\"M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z\"/></g><g data-mml-node=\"mo\" transform=\"translate(798.8,0)\"><path data-c=\"3D\" d=\"M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z\"/></g><g data-mml-node=\"mi\" transform=\"translate(1854.6,0)\"><path data-c=\"1D459\" d=\"M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z\"/></g><g data-mml-node=\"mi\" transform=\"translate(2152.6,0)\"><path data-c=\"1D45B\" d=\"M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z\"/></g><g data-mml-node=\"mo\" transform=\"translate(2752.6,0)\"><path data-c=\"28\" d=\"M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z\"/></g><g data-mml-node=\"mn\" transform=\"translate(3141.6,0)\"><path data-c=\"32\" d=\"M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z\"/></g><g data-mml-node=\"mo\" transform=\"translate(3641.6,0)\"><path data-c=\"29\" d=\"M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z\"/></g><g data-mml-node=\"mo\" transform=\"translate(4252.8,0)\"><path data-c=\"22C5\" d=\"M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z\"/></g><g data-mml-node=\"mi\" transform=\"translate(4753,0)\"><path data-c=\"1D45A\" d=\"M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z\"/></g><g data-mml-node=\"TeXAtom\" data-mjx-texclass=\"ORD\" transform=\"translate(5631,0)\"><g data-mml-node=\"mo\"><path data-c=\"2F\" d=\"M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z\"/></g></g><g data-mml-node=\"mi\" transform=\"translate(6131,0)\"><path data-c=\"1D45B\" d=\"M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z\"/></g></g></g></svg></mjx-container></p>\n<h3 id=\"Hudi-Upsert\"><a href=\"#Hudi-Upsert\" class=\"headerlink\" title=\"Hudi Upsert\"></a>Hudi Upsert</h3><h3 id=\"LSM-Tree\"><a href=\"#LSM-Tree\" class=\"headerlink\" title=\"LSM-Tree\"></a>LSM-Tree</h3><p>See <a href=\"https://adooobe.github.io/2023/10/02/lsm-tree/\">LSM-Tree</a></p>\n<h3 id=\"Others\"><a href=\"#Others\" class=\"headerlink\" title=\"Others\"></a>Others</h3><ol>\n<li><a href=\"https://zhuanlan.zhihu.com/p/294069121\">White List Question</a></li>\n<li><a href=\"https://www.51cto.com/article/753025.html\">Redis Cache Breakdown</a></li>\n</ol>\n<h2 id=\"Improvement\"><a href=\"#Improvement\" class=\"headerlink\" title=\"Improvement\"></a>Improvement</h2><p>There are two limitations that have always restricted the usage of Bloom Filter, bounded source and append-only. The following are several improvement methods that revolve around addressing these two issues.</p>\n<h4 id=\"Scalable-Bloom-Filters-SBF\"><a href=\"#Scalable-Bloom-Filters-SBF\" class=\"headerlink\" title=\"Scalable Bloom Filters (SBF)\"></a>Scalable Bloom Filters (SBF)</h4><p><img src=\"/2023/10/02/bloom/scalable_bloom_filter.png#pic_center\" alt=\"scalable bloom filter structure\"></p>\n<p>when the filter reaches some fulfillness threshold, it becomes read-only and new bigger and writable filter is created in its place. If in its turn it becomes saturated, the operation is repeated. Every new filter, in order to keep the false positives rate close to the targeted one, has more hash functions than the previous filter.</p>\n<p>In Scalable Bloom filter the membership test is applied on all created (read-only + writable) filters. If the item is missing in all filters, it’s considered as not existing. In the other side, if one of the filters reports its existence, it means that the element may be in the dataset. An important point to notice here is that Scalable Bloom filter uses a variant of Bloom filters where the bit vector is divided in <em>k</em> slices where each stores <em>M/k</em> bits (<em>M</em> is the size of whole bit vector). Since the number of slices is equal to the number of hash functions, each hash function works on its own slice:</p>\n<p><img src=\"/2023/10/02/bloom/scalable_bloom_filter_slices.png#pic_center\" alt=\"scalable bloom filter slices\"></p>\n<p>Thanks to the slices each element is always described by <em>k</em> bits resulting on more robust filter without the elements more prone to the false positives than the others.</p>\n<h4 id=\"Counting-Bloom-Filter-CBF\"><a href=\"#Counting-Bloom-Filter-CBF\" class=\"headerlink\" title=\"Counting Bloom Filter (CBF)\"></a>Counting Bloom Filter (CBF)</h4><p>For the second question, CBF provides ability to delete elements in Bloom Filters. But unfortunately, the premise is that we must ensure that the deleted element is present in the Bloom filter.</p>\n<p><img src=\"/2023/10/02/bloom/counting_bloom_filter.png#pic_center\" alt=\"counting bloom filter structure\"></p>\n<h4 id=\"Cuckoo-Filter\"><a href=\"#Cuckoo-Filter\" class=\"headerlink\" title=\"Cuckoo Filter\"></a>Cuckoo Filter</h4><p>As an “improved version” of the Bloom Filter, the Cuckoo Filter uses the minimum space cost to achieve a reduction in false positives and support for reverse deletion.</p>\n<p><img src=\"/2023/10/02/bloom/cuckoo_filter.png#pic_center\" alt=\"Cuckoo Filter\"></p>\n<p>It only uses two hash functions H1 and H2 (on two hash tables/buckets T1 and T2) for Cuckoo Filter to compute the postion of input elements. If an element E1 comes into Cuckoo Filter:</p>\n<ol>\n<li>Compute the position on hashtable(or bucket) T1 with H1 and insert if the position P1 empty</li>\n<li>If the P1 has other element, compute the position P2 on hashtable T2 with H2 and try to insert again</li>\n<li>If the P2 is still not empty, kick out the element of the position on T2 then insert E1 into P2</li>\n</ol>\n<p>There are serveral issue on this design:</p>\n<ol>\n<li>kick-out loop</li>\n<li>how to solve the element kicked out</li>\n<li>size of hashtable</li>\n</ol>\n<p>Luckily, the designer makes a brilliant idea to solve all three problems at once, <code>MaxLoop</code> and <code>Resize</code>.</p>\n<p>Cuckoo Filter would record the number of loop and when it comes to the <code>MaxLoop</code>, hash table would be resized and rehash (I think it needs to be allocated addtional memory to save the set of the elements and it can do rehash)</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li><a href=\"https://www.waitingforcode.com/big-data-algorithms/scalable-bloom-filter/read\">https://www.waitingforcode.com/big-data-algorithms/scalable-bloom-filter/read</a></li>\n<li>Baquero, C., &amp; Almeida, J. (2007, January). Scalable bloom filters. In European Conference on Principles of Data Mining and Knowledge Discovery (pp. 244-256). Springer, Berlin, Heidelberg.</li>\n<li><a href=\"https://www.xiemingzhao.com/posts/cuckooFilter.html\">https://www.xiemingzhao.com/posts/cuckooFilter.html</a></li>\n<li><a href=\"https://www.cs.cmu.edu/~dga/papers/cuckoo-conext2014.pdf\">https://www.cs.cmu.edu/~dga/papers/cuckoo-conext2014.pdf</a></li>\n</ol>\n"},{"title":"the principle and application of LSM Tree","date":"2023-10-02T11:24:02.000Z","_content":"# LSM Tree\n\n> LSM-Tree is a kind of storage engine rather than storage format.\n\n## Feature\n\n* Ordered\n* Block Storage(Disk)-oriented\n* Hierarchical\n\n## Structure\n\n![LSM-Tree Structure](lsm_tree.png#pic_center)\n\n## Workflow\n\n### WAL\n\nWhen LSM Tree received a input(insert/update/delete) operation, to avoid accidental crashing or shutdown，It is neccessary to write ahead log(WAL) that saves operation records into log files.\n\n```go\ntype Wal struct {\n\tf    *os.File\n\tpath string\n\tlock sync.Locker\n}\n```\n\n### Memtable\n\nMemtable is an append-only data structure (every node inserted cannot be changed by logical delete and removing duplicates based on update time.) like Binary search tree (e.g. RedBlack tree - LevelDB) or SkipList (e.g. HBase, more popular).\n\n### Sorted String Table (SSTables)\n\nUsually, a SSTable consists of an index block and multiple data blocks. Data block structure is as follows:\n\n![Sorted Strings Table](./lsm-tree/sstable.png#pic_center)\n\nwhere data is only ordered in segment layer rather than global.\n\n### Optimization\n\n#### improve read performance\n\nFor LSM Tree, what we concerned is read performance rather than write. As data increases, some methods to improve read performance need to come into our pictures.\n\n* **Sparse Index**\n\nAs mentioned above, SStable has several segments that data are orderly stored. Without any optimization, we can also use binary search algorithm to find a certain element by scanning all the SStables and every Segments in each SStable unitl finding it. But unfortunately, for binary search algorithm, the minimum memory unit is segment for us to find the key in *O(logn)* or spent a Disk IO, both are too expensive in big data scenarios. So it is neccessary to  build a sparse index in memory to accelerate query efficiency.\n\n![sparse index](sparse_index.png#pic_center)\n\n> Sparse indexes only contain entries for documents that have the indexed field, even if the index field contains a null value. The index skips over any document that is missing the indexed field. The index is \"sparse\" because it does not include all documents of a collection.\n\n* **Bloom Filter**\n\nWhen the number of SStable increases in Disk, if some key is not present in the records, we need to scan all the SStable to find that key. Bloom filter is to overcome this issue. Unlike sparse indexes, [Bloom filters](https://adooobe.github.io/2023/10/02/bloom/) are designed to address the performance issues that arise when querying for non-existent keys.\n\nIt is worth noting that Bloom Filters should be updated as compaction operations because compaction will delete some tombstone data so that BF can't work normally.\n\n### Compaction\n\n#### questions in query\n\nLet's talk about query in LSM Tree first. There are two query methods: *point lookup query* and *range query*.\n\n* **point lookup query**: find the element what we want from new segment to old one.\n* **range query**: when a big range query is executed, data have to be found in memtable, immutable memtable and multiple SSTalbes in different levels. (Notice: range query should be **key range query** like the follow picture)\n\n![LSM Tree range query](range_query.png#pic_center)\n\nDuring range reads, the iterator will seek to the start range similar to point lookup (using Binary search with in SSTs) using`SeekTo()` call. After seeking to start range, there will be series of iterators created one for each memtable, one for each Level-0 files (because of overlapping nature of SSTs in L0) and one for each level later on. A merging iterator will collect keys from each of these iterators and gives the data in sorted order till the End range is reached.\n\n```go\nitr := txn.NewIterator(badger.DefaultIteratorOptions)   \nfor itr.Seek(\"startKey\"); itr.Valid(); itr.Next() {\n    item := itr.Item()\n    key := item.Key()\n    if bytes.Compare(key, \"endKey\") > 0 {\n      break\n    }\n    // rest of the logic.\n}\n```\n\nStep1. find *startkey* position (write as startPosition) by `seek()` and move sub iterator to `startPosition + 1`\n\nStep2. compare the sub iterators' element, return the minimal value and move the itr pointer.\n\nStep3. repeat Step2 until the returned element > endkey\n\nSo in range query, as SSTables become more and more, query execution also becomes heavier and heavier.\n\n#### Sorted Run\n\n> LSM tree organizes files into several sorted runs. A sorted run consists of one or multiple [data file](https://nightlies.apache.org/flink/flink-table-store-docs-release-0.3/docs/concepts/file-layouts/#data-files)s and each data file belongs to exactly one sorted run.\n>\n> ![sorted runs](sorted_runs.png#pic_center)\n>\n> As you can see, different sorted runs may have overlapping primary key ranges, and may even contain the same primary key. When querying the LSM tree, all sorted runs must be combined and all records with the same primary key must be merged according to the user-specified [merge engine](https://nightlies.apache.org/flink/flink-table-store-docs-release-0.3/docs/features/table-types/#merge-engines) and the timestamp of each record.\n\nIn my opinion, in LSM Tree, a single logically ordered and no-repeat structure can be regarded as a `sorted run`.\n\n#### Methods\n\n![LSM Tree merge policies](merge_policy.png#pic_center)\n\n* tiered compaction(suitable for reads scenarios)\n  ![size_tiered_compaction](size_tiered_compaction.png#pic_center)\n  * high read and space amplification\n  * in each tier, storage unit(memtable/sstable in level 0) size won't be changed unitl it is merged into next-level SSTable. The size of a single SSTable in the next level is the sum of the number of SSTables in the previous level.\n* leveled compaction(suitable for writes scenarios)\n  * high write amplification\n  * As shown in the following figure, each leveled level is an ordered run that consists of multiple sstables. These sstables also maintain an orderly relationship with each other. When the data size of each level reaches the upper limit, this level will merge with the run of the next level. This method combines multiple runs of the level to one, reducing the read amplification and space amplification. Also, the smaller sstables provide fine-grained task splitting and control. This way, controlling the task size is actually controlling the size of the temporary space. In other words, leveled merge policies will merge SSTables into next level with the same range to reduce space and read amplifications.\n\n![leveled compaction](leveled_compaction.png#pic_center)\n\n## Improvement\n\n* **bLSM**\n* **Diff-Index LSM**\n\n## Reference\n\n1. https://www.cnblogs.com/whuanle/p/16297025.html\n2. https://www.geeksforgeeks.org/introduction-to-log-structured-merge-lsm-tree/\n3. https://www.mongodb.com/docs/manual/core/index-sparse/#:~:text=Sparse%20indexes%20only%20contain%20entries,all%20documents%20of%20a%20collection.\n4. https://hzhu212.github.io/posts/2d7c5edb/\n5. https://www.alibabacloud.com/blog/an-in-depth-discussion-on-the-lsm-compaction-mechanism_596780\n6. https://zhuanlan.zhihu.com/p/380013595\n7. https://github.com/facebook/rocksdb/wiki/Iterator-Implementation\n","source":"_posts/lsm-tree.md","raw":"---\ntitle: the principle and application of LSM Tree \ndate: 2023-10-02 19:24:02\ntags: LSM Storage\n---\n# LSM Tree\n\n> LSM-Tree is a kind of storage engine rather than storage format.\n\n## Feature\n\n* Ordered\n* Block Storage(Disk)-oriented\n* Hierarchical\n\n## Structure\n\n![LSM-Tree Structure](lsm_tree.png#pic_center)\n\n## Workflow\n\n### WAL\n\nWhen LSM Tree received a input(insert/update/delete) operation, to avoid accidental crashing or shutdown，It is neccessary to write ahead log(WAL) that saves operation records into log files.\n\n```go\ntype Wal struct {\n\tf    *os.File\n\tpath string\n\tlock sync.Locker\n}\n```\n\n### Memtable\n\nMemtable is an append-only data structure (every node inserted cannot be changed by logical delete and removing duplicates based on update time.) like Binary search tree (e.g. RedBlack tree - LevelDB) or SkipList (e.g. HBase, more popular).\n\n### Sorted String Table (SSTables)\n\nUsually, a SSTable consists of an index block and multiple data blocks. Data block structure is as follows:\n\n![Sorted Strings Table](./lsm-tree/sstable.png#pic_center)\n\nwhere data is only ordered in segment layer rather than global.\n\n### Optimization\n\n#### improve read performance\n\nFor LSM Tree, what we concerned is read performance rather than write. As data increases, some methods to improve read performance need to come into our pictures.\n\n* **Sparse Index**\n\nAs mentioned above, SStable has several segments that data are orderly stored. Without any optimization, we can also use binary search algorithm to find a certain element by scanning all the SStables and every Segments in each SStable unitl finding it. But unfortunately, for binary search algorithm, the minimum memory unit is segment for us to find the key in *O(logn)* or spent a Disk IO, both are too expensive in big data scenarios. So it is neccessary to  build a sparse index in memory to accelerate query efficiency.\n\n![sparse index](sparse_index.png#pic_center)\n\n> Sparse indexes only contain entries for documents that have the indexed field, even if the index field contains a null value. The index skips over any document that is missing the indexed field. The index is \"sparse\" because it does not include all documents of a collection.\n\n* **Bloom Filter**\n\nWhen the number of SStable increases in Disk, if some key is not present in the records, we need to scan all the SStable to find that key. Bloom filter is to overcome this issue. Unlike sparse indexes, [Bloom filters](https://adooobe.github.io/2023/10/02/bloom/) are designed to address the performance issues that arise when querying for non-existent keys.\n\nIt is worth noting that Bloom Filters should be updated as compaction operations because compaction will delete some tombstone data so that BF can't work normally.\n\n### Compaction\n\n#### questions in query\n\nLet's talk about query in LSM Tree first. There are two query methods: *point lookup query* and *range query*.\n\n* **point lookup query**: find the element what we want from new segment to old one.\n* **range query**: when a big range query is executed, data have to be found in memtable, immutable memtable and multiple SSTalbes in different levels. (Notice: range query should be **key range query** like the follow picture)\n\n![LSM Tree range query](range_query.png#pic_center)\n\nDuring range reads, the iterator will seek to the start range similar to point lookup (using Binary search with in SSTs) using`SeekTo()` call. After seeking to start range, there will be series of iterators created one for each memtable, one for each Level-0 files (because of overlapping nature of SSTs in L0) and one for each level later on. A merging iterator will collect keys from each of these iterators and gives the data in sorted order till the End range is reached.\n\n```go\nitr := txn.NewIterator(badger.DefaultIteratorOptions)   \nfor itr.Seek(\"startKey\"); itr.Valid(); itr.Next() {\n    item := itr.Item()\n    key := item.Key()\n    if bytes.Compare(key, \"endKey\") > 0 {\n      break\n    }\n    // rest of the logic.\n}\n```\n\nStep1. find *startkey* position (write as startPosition) by `seek()` and move sub iterator to `startPosition + 1`\n\nStep2. compare the sub iterators' element, return the minimal value and move the itr pointer.\n\nStep3. repeat Step2 until the returned element > endkey\n\nSo in range query, as SSTables become more and more, query execution also becomes heavier and heavier.\n\n#### Sorted Run\n\n> LSM tree organizes files into several sorted runs. A sorted run consists of one or multiple [data file](https://nightlies.apache.org/flink/flink-table-store-docs-release-0.3/docs/concepts/file-layouts/#data-files)s and each data file belongs to exactly one sorted run.\n>\n> ![sorted runs](sorted_runs.png#pic_center)\n>\n> As you can see, different sorted runs may have overlapping primary key ranges, and may even contain the same primary key. When querying the LSM tree, all sorted runs must be combined and all records with the same primary key must be merged according to the user-specified [merge engine](https://nightlies.apache.org/flink/flink-table-store-docs-release-0.3/docs/features/table-types/#merge-engines) and the timestamp of each record.\n\nIn my opinion, in LSM Tree, a single logically ordered and no-repeat structure can be regarded as a `sorted run`.\n\n#### Methods\n\n![LSM Tree merge policies](merge_policy.png#pic_center)\n\n* tiered compaction(suitable for reads scenarios)\n  ![size_tiered_compaction](size_tiered_compaction.png#pic_center)\n  * high read and space amplification\n  * in each tier, storage unit(memtable/sstable in level 0) size won't be changed unitl it is merged into next-level SSTable. The size of a single SSTable in the next level is the sum of the number of SSTables in the previous level.\n* leveled compaction(suitable for writes scenarios)\n  * high write amplification\n  * As shown in the following figure, each leveled level is an ordered run that consists of multiple sstables. These sstables also maintain an orderly relationship with each other. When the data size of each level reaches the upper limit, this level will merge with the run of the next level. This method combines multiple runs of the level to one, reducing the read amplification and space amplification. Also, the smaller sstables provide fine-grained task splitting and control. This way, controlling the task size is actually controlling the size of the temporary space. In other words, leveled merge policies will merge SSTables into next level with the same range to reduce space and read amplifications.\n\n![leveled compaction](leveled_compaction.png#pic_center)\n\n## Improvement\n\n* **bLSM**\n* **Diff-Index LSM**\n\n## Reference\n\n1. https://www.cnblogs.com/whuanle/p/16297025.html\n2. https://www.geeksforgeeks.org/introduction-to-log-structured-merge-lsm-tree/\n3. https://www.mongodb.com/docs/manual/core/index-sparse/#:~:text=Sparse%20indexes%20only%20contain%20entries,all%20documents%20of%20a%20collection.\n4. https://hzhu212.github.io/posts/2d7c5edb/\n5. https://www.alibabacloud.com/blog/an-in-depth-discussion-on-the-lsm-compaction-mechanism_596780\n6. https://zhuanlan.zhihu.com/p/380013595\n7. https://github.com/facebook/rocksdb/wiki/Iterator-Implementation\n","slug":"lsm-tree","published":1,"updated":"2023-10-19T07:51:30.642Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clo80pb0y00045cc34vtph8t3","content":"<h1 id=\"LSM-Tree\"><a href=\"#LSM-Tree\" class=\"headerlink\" title=\"LSM Tree\"></a>LSM Tree</h1><blockquote>\n<p>LSM-Tree is a kind of storage engine rather than storage format.</p>\n</blockquote>\n<h2 id=\"Feature\"><a href=\"#Feature\" class=\"headerlink\" title=\"Feature\"></a>Feature</h2><ul>\n<li>Ordered</li>\n<li>Block Storage(Disk)-oriented</li>\n<li>Hierarchical</li>\n</ul>\n<h2 id=\"Structure\"><a href=\"#Structure\" class=\"headerlink\" title=\"Structure\"></a>Structure</h2><p><img src=\"/2023/10/02/lsm-tree/lsm_tree.png#pic_center\" alt=\"LSM-Tree Structure\"></p>\n<h2 id=\"Workflow\"><a href=\"#Workflow\" class=\"headerlink\" title=\"Workflow\"></a>Workflow</h2><h3 id=\"WAL\"><a href=\"#WAL\" class=\"headerlink\" title=\"WAL\"></a>WAL</h3><p>When LSM Tree received a input(insert/update/delete) operation, to avoid accidental crashing or shutdown，It is neccessary to write ahead log(WAL) that saves operation records into log files.</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">type</span> Wal <span class=\"keyword\">struct</span> &#123;</span><br><span class=\"line\">\tf    *os.File</span><br><span class=\"line\">\tpath <span class=\"type\">string</span></span><br><span class=\"line\">\tlock sync.Locker</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"Memtable\"><a href=\"#Memtable\" class=\"headerlink\" title=\"Memtable\"></a>Memtable</h3><p>Memtable is an append-only data structure (every node inserted cannot be changed by logical delete and removing duplicates based on update time.) like Binary search tree (e.g. RedBlack tree - LevelDB) or SkipList (e.g. HBase, more popular).</p>\n<h3 id=\"Sorted-String-Table-SSTables\"><a href=\"#Sorted-String-Table-SSTables\" class=\"headerlink\" title=\"Sorted String Table (SSTables)\"></a>Sorted String Table (SSTables)</h3><p>Usually, a SSTable consists of an index block and multiple data blocks. Data block structure is as follows:</p>\n<p><img src=\"/2023/10/02/lsm-tree/sstable.png#pic_center\" alt=\"Sorted Strings Table\"></p>\n<p>where data is only ordered in segment layer rather than global.</p>\n<h3 id=\"Optimization\"><a href=\"#Optimization\" class=\"headerlink\" title=\"Optimization\"></a>Optimization</h3><h4 id=\"improve-read-performance\"><a href=\"#improve-read-performance\" class=\"headerlink\" title=\"improve read performance\"></a>improve read performance</h4><p>For LSM Tree, what we concerned is read performance rather than write. As data increases, some methods to improve read performance need to come into our pictures.</p>\n<ul>\n<li><strong>Sparse Index</strong></li>\n</ul>\n<p>As mentioned above, SStable has several segments that data are orderly stored. Without any optimization, we can also use binary search algorithm to find a certain element by scanning all the SStables and every Segments in each SStable unitl finding it. But unfortunately, for binary search algorithm, the minimum memory unit is segment for us to find the key in <em>O(logn)</em> or spent a Disk IO, both are too expensive in big data scenarios. So it is neccessary to  build a sparse index in memory to accelerate query efficiency.</p>\n<p><img src=\"/2023/10/02/lsm-tree/sparse_index.png#pic_center\" alt=\"sparse index\"></p>\n<blockquote>\n<p>Sparse indexes only contain entries for documents that have the indexed field, even if the index field contains a null value. The index skips over any document that is missing the indexed field. The index is “sparse” because it does not include all documents of a collection.</p>\n</blockquote>\n<ul>\n<li><strong>Bloom Filter</strong></li>\n</ul>\n<p>When the number of SStable increases in Disk, if some key is not present in the records, we need to scan all the SStable to find that key. Bloom filter is to overcome this issue. Unlike sparse indexes, <a href=\"https://adooobe.github.io/2023/10/02/bloom/\">Bloom filters</a> are designed to address the performance issues that arise when querying for non-existent keys.</p>\n<p>It is worth noting that Bloom Filters should be updated as compaction operations because compaction will delete some tombstone data so that BF can’t work normally.</p>\n<h3 id=\"Compaction\"><a href=\"#Compaction\" class=\"headerlink\" title=\"Compaction\"></a>Compaction</h3><h4 id=\"questions-in-query\"><a href=\"#questions-in-query\" class=\"headerlink\" title=\"questions in query\"></a>questions in query</h4><p>Let’s talk about query in LSM Tree first. There are two query methods: <em>point lookup query</em> and <em>range query</em>.</p>\n<ul>\n<li><strong>point lookup query</strong>: find the element what we want from new segment to old one.</li>\n<li><strong>range query</strong>: when a big range query is executed, data have to be found in memtable, immutable memtable and multiple SSTalbes in different levels. (Notice: range query should be <strong>key range query</strong> like the follow picture)</li>\n</ul>\n<p><img src=\"/2023/10/02/lsm-tree/range_query.png#pic_center\" alt=\"LSM Tree range query\"></p>\n<p>During range reads, the iterator will seek to the start range similar to point lookup (using Binary search with in SSTs) using<code>SeekTo()</code> call. After seeking to start range, there will be series of iterators created one for each memtable, one for each Level-0 files (because of overlapping nature of SSTs in L0) and one for each level later on. A merging iterator will collect keys from each of these iterators and gives the data in sorted order till the End range is reached.</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">itr := txn.NewIterator(badger.DefaultIteratorOptions)   </span><br><span class=\"line\"><span class=\"keyword\">for</span> itr.Seek(<span class=\"string\">&quot;startKey&quot;</span>); itr.Valid(); itr.Next() &#123;</span><br><span class=\"line\">    item := itr.Item()</span><br><span class=\"line\">    key := item.Key()</span><br><span class=\"line\">    <span class=\"keyword\">if</span> bytes.Compare(key, <span class=\"string\">&quot;endKey&quot;</span>) &gt; <span class=\"number\">0</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">break</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// rest of the logic.</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>Step1. find <em>startkey</em> position (write as startPosition) by <code>seek()</code> and move sub iterator to <code>startPosition + 1</code></p>\n<p>Step2. compare the sub iterators’ element, return the minimal value and move the itr pointer.</p>\n<p>Step3. repeat Step2 until the returned element &gt; endkey</p>\n<p>So in range query, as SSTables become more and more, query execution also becomes heavier and heavier.</p>\n<h4 id=\"Sorted-Run\"><a href=\"#Sorted-Run\" class=\"headerlink\" title=\"Sorted Run\"></a>Sorted Run</h4><blockquote>\n<p>LSM tree organizes files into several sorted runs. A sorted run consists of one or multiple <a href=\"https://nightlies.apache.org/flink/flink-table-store-docs-release-0.3/docs/concepts/file-layouts/#data-files\">data file</a>s and each data file belongs to exactly one sorted run.</p>\n<p><img src=\"/2023/10/02/lsm-tree/sorted_runs.png#pic_center\" alt=\"sorted runs\"></p>\n<p>As you can see, different sorted runs may have overlapping primary key ranges, and may even contain the same primary key. When querying the LSM tree, all sorted runs must be combined and all records with the same primary key must be merged according to the user-specified <a href=\"https://nightlies.apache.org/flink/flink-table-store-docs-release-0.3/docs/features/table-types/#merge-engines\">merge engine</a> and the timestamp of each record.</p>\n</blockquote>\n<p>In my opinion, in LSM Tree, a single logically ordered and no-repeat structure can be regarded as a <code>sorted run</code>.</p>\n<h4 id=\"Methods\"><a href=\"#Methods\" class=\"headerlink\" title=\"Methods\"></a>Methods</h4><p><img src=\"/2023/10/02/lsm-tree/merge_policy.png#pic_center\" alt=\"LSM Tree merge policies\"></p>\n<ul>\n<li>tiered compaction(suitable for reads scenarios)<br><img src=\"/2023/10/02/lsm-tree/size_tiered_compaction.png#pic_center\" alt=\"size_tiered_compaction\"><ul>\n<li>high read and space amplification</li>\n<li>in each tier, storage unit(memtable/sstable in level 0) size won’t be changed unitl it is merged into next-level SSTable. The size of a single SSTable in the next level is the sum of the number of SSTables in the previous level.</li>\n</ul>\n</li>\n<li>leveled compaction(suitable for writes scenarios)<ul>\n<li>high write amplification</li>\n<li>As shown in the following figure, each leveled level is an ordered run that consists of multiple sstables. These sstables also maintain an orderly relationship with each other. When the data size of each level reaches the upper limit, this level will merge with the run of the next level. This method combines multiple runs of the level to one, reducing the read amplification and space amplification. Also, the smaller sstables provide fine-grained task splitting and control. This way, controlling the task size is actually controlling the size of the temporary space. In other words, leveled merge policies will merge SSTables into next level with the same range to reduce space and read amplifications.</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"/2023/10/02/lsm-tree/leveled_compaction.png#pic_center\" alt=\"leveled compaction\"></p>\n<h2 id=\"Improvement\"><a href=\"#Improvement\" class=\"headerlink\" title=\"Improvement\"></a>Improvement</h2><ul>\n<li><strong>bLSM</strong></li>\n<li><strong>Diff-Index LSM</strong></li>\n</ul>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li><a href=\"https://www.cnblogs.com/whuanle/p/16297025.html\">https://www.cnblogs.com/whuanle/p/16297025.html</a></li>\n<li><a href=\"https://www.geeksforgeeks.org/introduction-to-log-structured-merge-lsm-tree/\">https://www.geeksforgeeks.org/introduction-to-log-structured-merge-lsm-tree/</a></li>\n<li><a href=\"https://www.mongodb.com/docs/manual/core/index-sparse/#:~:text=Sparse%20indexes%20only%20contain%20entries,all%20documents%20of%20a%20collection\">https://www.mongodb.com/docs/manual/core/index-sparse/#:~:text=Sparse%20indexes%20only%20contain%20entries,all%20documents%20of%20a%20collection</a>.</li>\n<li><a href=\"https://hzhu212.github.io/posts/2d7c5edb/\">https://hzhu212.github.io/posts/2d7c5edb/</a></li>\n<li><a href=\"https://www.alibabacloud.com/blog/an-in-depth-discussion-on-the-lsm-compaction-mechanism_596780\">https://www.alibabacloud.com/blog/an-in-depth-discussion-on-the-lsm-compaction-mechanism_596780</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/380013595\">https://zhuanlan.zhihu.com/p/380013595</a></li>\n<li><a href=\"https://github.com/facebook/rocksdb/wiki/Iterator-Implementation\">https://github.com/facebook/rocksdb/wiki/Iterator-Implementation</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"LSM-Tree\"><a href=\"#LSM-Tree\" class=\"headerlink\" title=\"LSM Tree\"></a>LSM Tree</h1><blockquote>\n<p>LSM-Tree is a kind of storage engine rather than storage format.</p>\n</blockquote>\n<h2 id=\"Feature\"><a href=\"#Feature\" class=\"headerlink\" title=\"Feature\"></a>Feature</h2><ul>\n<li>Ordered</li>\n<li>Block Storage(Disk)-oriented</li>\n<li>Hierarchical</li>\n</ul>\n<h2 id=\"Structure\"><a href=\"#Structure\" class=\"headerlink\" title=\"Structure\"></a>Structure</h2><p><img src=\"/2023/10/02/lsm-tree/lsm_tree.png#pic_center\" alt=\"LSM-Tree Structure\"></p>\n<h2 id=\"Workflow\"><a href=\"#Workflow\" class=\"headerlink\" title=\"Workflow\"></a>Workflow</h2><h3 id=\"WAL\"><a href=\"#WAL\" class=\"headerlink\" title=\"WAL\"></a>WAL</h3><p>When LSM Tree received a input(insert/update/delete) operation, to avoid accidental crashing or shutdown，It is neccessary to write ahead log(WAL) that saves operation records into log files.</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">type</span> Wal <span class=\"keyword\">struct</span> &#123;</span><br><span class=\"line\">\tf    *os.File</span><br><span class=\"line\">\tpath <span class=\"type\">string</span></span><br><span class=\"line\">\tlock sync.Locker</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"Memtable\"><a href=\"#Memtable\" class=\"headerlink\" title=\"Memtable\"></a>Memtable</h3><p>Memtable is an append-only data structure (every node inserted cannot be changed by logical delete and removing duplicates based on update time.) like Binary search tree (e.g. RedBlack tree - LevelDB) or SkipList (e.g. HBase, more popular).</p>\n<h3 id=\"Sorted-String-Table-SSTables\"><a href=\"#Sorted-String-Table-SSTables\" class=\"headerlink\" title=\"Sorted String Table (SSTables)\"></a>Sorted String Table (SSTables)</h3><p>Usually, a SSTable consists of an index block and multiple data blocks. Data block structure is as follows:</p>\n<p><img src=\"/2023/10/02/lsm-tree/sstable.png#pic_center\" alt=\"Sorted Strings Table\"></p>\n<p>where data is only ordered in segment layer rather than global.</p>\n<h3 id=\"Optimization\"><a href=\"#Optimization\" class=\"headerlink\" title=\"Optimization\"></a>Optimization</h3><h4 id=\"improve-read-performance\"><a href=\"#improve-read-performance\" class=\"headerlink\" title=\"improve read performance\"></a>improve read performance</h4><p>For LSM Tree, what we concerned is read performance rather than write. As data increases, some methods to improve read performance need to come into our pictures.</p>\n<ul>\n<li><strong>Sparse Index</strong></li>\n</ul>\n<p>As mentioned above, SStable has several segments that data are orderly stored. Without any optimization, we can also use binary search algorithm to find a certain element by scanning all the SStables and every Segments in each SStable unitl finding it. But unfortunately, for binary search algorithm, the minimum memory unit is segment for us to find the key in <em>O(logn)</em> or spent a Disk IO, both are too expensive in big data scenarios. So it is neccessary to  build a sparse index in memory to accelerate query efficiency.</p>\n<p><img src=\"/2023/10/02/lsm-tree/sparse_index.png#pic_center\" alt=\"sparse index\"></p>\n<blockquote>\n<p>Sparse indexes only contain entries for documents that have the indexed field, even if the index field contains a null value. The index skips over any document that is missing the indexed field. The index is “sparse” because it does not include all documents of a collection.</p>\n</blockquote>\n<ul>\n<li><strong>Bloom Filter</strong></li>\n</ul>\n<p>When the number of SStable increases in Disk, if some key is not present in the records, we need to scan all the SStable to find that key. Bloom filter is to overcome this issue. Unlike sparse indexes, <a href=\"https://adooobe.github.io/2023/10/02/bloom/\">Bloom filters</a> are designed to address the performance issues that arise when querying for non-existent keys.</p>\n<p>It is worth noting that Bloom Filters should be updated as compaction operations because compaction will delete some tombstone data so that BF can’t work normally.</p>\n<h3 id=\"Compaction\"><a href=\"#Compaction\" class=\"headerlink\" title=\"Compaction\"></a>Compaction</h3><h4 id=\"questions-in-query\"><a href=\"#questions-in-query\" class=\"headerlink\" title=\"questions in query\"></a>questions in query</h4><p>Let’s talk about query in LSM Tree first. There are two query methods: <em>point lookup query</em> and <em>range query</em>.</p>\n<ul>\n<li><strong>point lookup query</strong>: find the element what we want from new segment to old one.</li>\n<li><strong>range query</strong>: when a big range query is executed, data have to be found in memtable, immutable memtable and multiple SSTalbes in different levels. (Notice: range query should be <strong>key range query</strong> like the follow picture)</li>\n</ul>\n<p><img src=\"/2023/10/02/lsm-tree/range_query.png#pic_center\" alt=\"LSM Tree range query\"></p>\n<p>During range reads, the iterator will seek to the start range similar to point lookup (using Binary search with in SSTs) using<code>SeekTo()</code> call. After seeking to start range, there will be series of iterators created one for each memtable, one for each Level-0 files (because of overlapping nature of SSTs in L0) and one for each level later on. A merging iterator will collect keys from each of these iterators and gives the data in sorted order till the End range is reached.</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">itr := txn.NewIterator(badger.DefaultIteratorOptions)   </span><br><span class=\"line\"><span class=\"keyword\">for</span> itr.Seek(<span class=\"string\">&quot;startKey&quot;</span>); itr.Valid(); itr.Next() &#123;</span><br><span class=\"line\">    item := itr.Item()</span><br><span class=\"line\">    key := item.Key()</span><br><span class=\"line\">    <span class=\"keyword\">if</span> bytes.Compare(key, <span class=\"string\">&quot;endKey&quot;</span>) &gt; <span class=\"number\">0</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">break</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// rest of the logic.</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>Step1. find <em>startkey</em> position (write as startPosition) by <code>seek()</code> and move sub iterator to <code>startPosition + 1</code></p>\n<p>Step2. compare the sub iterators’ element, return the minimal value and move the itr pointer.</p>\n<p>Step3. repeat Step2 until the returned element &gt; endkey</p>\n<p>So in range query, as SSTables become more and more, query execution also becomes heavier and heavier.</p>\n<h4 id=\"Sorted-Run\"><a href=\"#Sorted-Run\" class=\"headerlink\" title=\"Sorted Run\"></a>Sorted Run</h4><blockquote>\n<p>LSM tree organizes files into several sorted runs. A sorted run consists of one or multiple <a href=\"https://nightlies.apache.org/flink/flink-table-store-docs-release-0.3/docs/concepts/file-layouts/#data-files\">data file</a>s and each data file belongs to exactly one sorted run.</p>\n<p><img src=\"/2023/10/02/lsm-tree/sorted_runs.png#pic_center\" alt=\"sorted runs\"></p>\n<p>As you can see, different sorted runs may have overlapping primary key ranges, and may even contain the same primary key. When querying the LSM tree, all sorted runs must be combined and all records with the same primary key must be merged according to the user-specified <a href=\"https://nightlies.apache.org/flink/flink-table-store-docs-release-0.3/docs/features/table-types/#merge-engines\">merge engine</a> and the timestamp of each record.</p>\n</blockquote>\n<p>In my opinion, in LSM Tree, a single logically ordered and no-repeat structure can be regarded as a <code>sorted run</code>.</p>\n<h4 id=\"Methods\"><a href=\"#Methods\" class=\"headerlink\" title=\"Methods\"></a>Methods</h4><p><img src=\"/2023/10/02/lsm-tree/merge_policy.png#pic_center\" alt=\"LSM Tree merge policies\"></p>\n<ul>\n<li>tiered compaction(suitable for reads scenarios)<br><img src=\"/2023/10/02/lsm-tree/size_tiered_compaction.png#pic_center\" alt=\"size_tiered_compaction\"><ul>\n<li>high read and space amplification</li>\n<li>in each tier, storage unit(memtable/sstable in level 0) size won’t be changed unitl it is merged into next-level SSTable. The size of a single SSTable in the next level is the sum of the number of SSTables in the previous level.</li>\n</ul>\n</li>\n<li>leveled compaction(suitable for writes scenarios)<ul>\n<li>high write amplification</li>\n<li>As shown in the following figure, each leveled level is an ordered run that consists of multiple sstables. These sstables also maintain an orderly relationship with each other. When the data size of each level reaches the upper limit, this level will merge with the run of the next level. This method combines multiple runs of the level to one, reducing the read amplification and space amplification. Also, the smaller sstables provide fine-grained task splitting and control. This way, controlling the task size is actually controlling the size of the temporary space. In other words, leveled merge policies will merge SSTables into next level with the same range to reduce space and read amplifications.</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"/2023/10/02/lsm-tree/leveled_compaction.png#pic_center\" alt=\"leveled compaction\"></p>\n<h2 id=\"Improvement\"><a href=\"#Improvement\" class=\"headerlink\" title=\"Improvement\"></a>Improvement</h2><ul>\n<li><strong>bLSM</strong></li>\n<li><strong>Diff-Index LSM</strong></li>\n</ul>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li><a href=\"https://www.cnblogs.com/whuanle/p/16297025.html\">https://www.cnblogs.com/whuanle/p/16297025.html</a></li>\n<li><a href=\"https://www.geeksforgeeks.org/introduction-to-log-structured-merge-lsm-tree/\">https://www.geeksforgeeks.org/introduction-to-log-structured-merge-lsm-tree/</a></li>\n<li><a href=\"https://www.mongodb.com/docs/manual/core/index-sparse/#:~:text=Sparse%20indexes%20only%20contain%20entries,all%20documents%20of%20a%20collection\">https://www.mongodb.com/docs/manual/core/index-sparse/#:~:text=Sparse%20indexes%20only%20contain%20entries,all%20documents%20of%20a%20collection</a>.</li>\n<li><a href=\"https://hzhu212.github.io/posts/2d7c5edb/\">https://hzhu212.github.io/posts/2d7c5edb/</a></li>\n<li><a href=\"https://www.alibabacloud.com/blog/an-in-depth-discussion-on-the-lsm-compaction-mechanism_596780\">https://www.alibabacloud.com/blog/an-in-depth-discussion-on-the-lsm-compaction-mechanism_596780</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/380013595\">https://zhuanlan.zhihu.com/p/380013595</a></li>\n<li><a href=\"https://github.com/facebook/rocksdb/wiki/Iterator-Implementation\">https://github.com/facebook/rocksdb/wiki/Iterator-Implementation</a></li>\n</ol>\n"},{"title":"z-order","date":"2023-10-19T05:33:09.000Z","_content":"\n## What is Z-order & Why Z-order\n\nZ-order is a concept of sorting data. Specifically，to quote Wikipedia, `“Z-order maps multidimensional data to one dimension while preserving locality of the data points.”` In the field of big data, Columns in table are regarded as dimensions to some extension. In other words, Z-order brings us a better way to organize data.\n\nFirstly, let's think about two different ordered data, Z-order data and Linear-order data.\n\n![z-order data](z-order.png#pic_center)\n\n![linear-order data](linear-order.png#pic_center)\n\nIt is difficult for us to determine which one performs better immediately so there is a simple example to examine why z-order data may be the better one.\n\n\n| x | y | z |\n| :-: | :-: | :-: |\n| 0 | 0 | 1 |\n| 0 | 3 | 1 |\n| 1 | 0 | 1 |\n| 1 | 2 | 1 |\n| 2 | 1 | 1 |\n| 2 | 3 | 1 |\n| 3 | 0 | 1 |\n| 3 | 3 | 1 |\n\ndata in z-order:\n\n\n| x | y | z |\n| :-: | :-: | :-: |\n| 0 | 0 | 1 |\n| 1 | 0 | 1 |\n| 3 | 0 | 1 |\n| 2 | 1 | 1 |\n| 1 | 2 | 1 |\n| 0 | 3 | 1 |\n| 2 | 3 | 1 |\n| 3 | 3 | 1 |\n\nNow, think about executing a query as follows:\n\n1. `select sum(z) from test where x = 2`\n2. `select sum(z) from test where y = 2`\n\nHow do we know the query behavior of this SQL? Let's figure out it in clickhouse.\n\ndefault minmax index:\n\n\n| granule | x_min | x_max | y_min | y_max |\n| --------- | ------- | ------- | ------- | ------- |\n| 0       | 0     | 0     | 0     | 3     |\n| 1       | 1     | 1     | 0     | 2     |\n| 2       | 2     | 2     | 1     | 3     |\n| 3       | 3     | 3     | 0     | 3     |\n\nminmax index with z-order:\n\n\n| granule | x_min | x_max | y_min | y_max |\n| --------- | ------- | ------- | ------- | ------- |\n| 0       | 0     | 1     | 0     | 0     |\n| 1       | 2     | 3     | 0     | 1     |\n| 2       | 0     | 1     | 2     | 3     |\n| 3       | 2     | 3     | 3     | 3     |\n\nFor default minmax index, we filter only one granule when filtering `x=2` and  4 granules when filtering `y=2`. For z-order data, the results are\n2 and 2. It seems that z-order data may be litter better than default one but what if we have more than 3 dimensions filter conditions? Therefore, arranging data according to z-order can help us filter out data does not need to be read, achieving better dataskipping.\n\n## Z-order in OLTP & OLAP\n\nIn conclusion, Z-order in OLAP can be more popular than OLTP. There are two reasons as follows.\n\nOne is due to the order of physical data that data can only be ordered as one rule in disk. If we put data into disk ordered by primary key, it can't be ordered by another key. It means we can't get satisfying performance when filtering some other column as same as filtering sprimary key columns. Another one is we can create serveral B-Tree index in OLTP to replace the capabilities of z-order but OLAP not (because index in memory is too expensive for OLAP).\n\n## Extension: GeoHash VS Morton Code\n\n1. https://zhuanlan.zhihu.com/p/35940647\n2. https://zhuanlan.zhihu.com/p/468542418 (z-order code, but I don't understand:( )\n\n## Reference\n\n1. https://blog.cloudera.com/speeding-up-queries-with-z-order/\n2. https://izualzhy.cn/lakehouse-zorder\n3. https://juejin.cn/post/7118344872947515428\n4. https://zhuanlan.zhihu.com/p/491256487\n","source":"_posts/z-order.md","raw":"title: z-order\ndate: 2023-10-19 13:33:09\ntags: big data, index\n---------------------\n\n## What is Z-order & Why Z-order\n\nZ-order is a concept of sorting data. Specifically，to quote Wikipedia, `“Z-order maps multidimensional data to one dimension while preserving locality of the data points.”` In the field of big data, Columns in table are regarded as dimensions to some extension. In other words, Z-order brings us a better way to organize data.\n\nFirstly, let's think about two different ordered data, Z-order data and Linear-order data.\n\n![z-order data](z-order.png#pic_center)\n\n![linear-order data](linear-order.png#pic_center)\n\nIt is difficult for us to determine which one performs better immediately so there is a simple example to examine why z-order data may be the better one.\n\n\n| x | y | z |\n| :-: | :-: | :-: |\n| 0 | 0 | 1 |\n| 0 | 3 | 1 |\n| 1 | 0 | 1 |\n| 1 | 2 | 1 |\n| 2 | 1 | 1 |\n| 2 | 3 | 1 |\n| 3 | 0 | 1 |\n| 3 | 3 | 1 |\n\ndata in z-order:\n\n\n| x | y | z |\n| :-: | :-: | :-: |\n| 0 | 0 | 1 |\n| 1 | 0 | 1 |\n| 3 | 0 | 1 |\n| 2 | 1 | 1 |\n| 1 | 2 | 1 |\n| 0 | 3 | 1 |\n| 2 | 3 | 1 |\n| 3 | 3 | 1 |\n\nNow, think about executing a query as follows:\n\n1. `select sum(z) from test where x = 2`\n2. `select sum(z) from test where y = 2`\n\nHow do we know the query behavior of this SQL? Let's figure out it in clickhouse.\n\ndefault minmax index:\n\n\n| granule | x_min | x_max | y_min | y_max |\n| --------- | ------- | ------- | ------- | ------- |\n| 0       | 0     | 0     | 0     | 3     |\n| 1       | 1     | 1     | 0     | 2     |\n| 2       | 2     | 2     | 1     | 3     |\n| 3       | 3     | 3     | 0     | 3     |\n\nminmax index with z-order:\n\n\n| granule | x_min | x_max | y_min | y_max |\n| --------- | ------- | ------- | ------- | ------- |\n| 0       | 0     | 1     | 0     | 0     |\n| 1       | 2     | 3     | 0     | 1     |\n| 2       | 0     | 1     | 2     | 3     |\n| 3       | 2     | 3     | 3     | 3     |\n\nFor default minmax index, we filter only one granule when filtering `x=2` and  4 granules when filtering `y=2`. For z-order data, the results are\n2 and 2. It seems that z-order data may be litter better than default one but what if we have more than 3 dimensions filter conditions? Therefore, arranging data according to z-order can help us filter out data does not need to be read, achieving better dataskipping.\n\n## Z-order in OLTP & OLAP\n\nIn conclusion, Z-order in OLAP can be more popular than OLTP. There are two reasons as follows.\n\nOne is due to the order of physical data that data can only be ordered as one rule in disk. If we put data into disk ordered by primary key, it can't be ordered by another key. It means we can't get satisfying performance when filtering some other column as same as filtering sprimary key columns. Another one is we can create serveral B-Tree index in OLTP to replace the capabilities of z-order but OLAP not (because index in memory is too expensive for OLAP).\n\n## Extension: GeoHash VS Morton Code\n\n1. https://zhuanlan.zhihu.com/p/35940647\n2. https://zhuanlan.zhihu.com/p/468542418 (z-order code, but I don't understand:( )\n\n## Reference\n\n1. https://blog.cloudera.com/speeding-up-queries-with-z-order/\n2. https://izualzhy.cn/lakehouse-zorder\n3. https://juejin.cn/post/7118344872947515428\n4. https://zhuanlan.zhihu.com/p/491256487\n","slug":"z-order","published":1,"updated":"2023-10-23T01:58:15.691Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clo80pb0y00055cc3cdo58nh7","content":"<h2 id=\"What-is-Z-order-amp-Why-Z-order\"><a href=\"#What-is-Z-order-amp-Why-Z-order\" class=\"headerlink\" title=\"What is Z-order &amp; Why Z-order\"></a>What is Z-order &amp; Why Z-order</h2><p>Z-order is a concept of sorting data. Specifically，to quote Wikipedia, <code>“Z-order maps multidimensional data to one dimension while preserving locality of the data points.”</code> In the field of big data, Columns in table are regarded as dimensions to some extension. In other words, Z-order brings us a better way to organize data.</p>\n<p>Firstly, let’s think about two different ordered data, Z-order data and Linear-order data.</p>\n<p><img src=\"/2023/10/19/z-order/z-order.png#pic_center\" alt=\"z-order data\"></p>\n<p><img src=\"/2023/10/19/z-order/linear-order.png#pic_center\" alt=\"linear-order data\"></p>\n<p>It is difficult for us to determine which one performs better immediately so there is a simple example to examine why z-order data may be the better one.</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">x</th>\n<th style=\"text-align:center\">y</th>\n<th style=\"text-align:center\">z</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>data in z-order:</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">x</th>\n<th style=\"text-align:center\">y</th>\n<th style=\"text-align:center\">z</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>Now, think about executing a query as follows:</p>\n<ol>\n<li><code>select sum(z) from test where x = 2</code></li>\n<li><code>select sum(z) from test where y = 2</code></li>\n</ol>\n<p>How do we know the query behavior of this SQL? Let’s figure out it in clickhouse.</p>\n<p>default minmax index:</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>granule</th>\n<th>x_min</th>\n<th>x_max</th>\n<th>y_min</th>\n<th>y_max</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>3</td>\n</tr>\n<tr>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>0</td>\n<td>2</td>\n</tr>\n<tr>\n<td>2</td>\n<td>2</td>\n<td>2</td>\n<td>1</td>\n<td>3</td>\n</tr>\n<tr>\n<td>3</td>\n<td>3</td>\n<td>3</td>\n<td>0</td>\n<td>3</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>minmax index with z-order:</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>granule</th>\n<th>x_min</th>\n<th>x_max</th>\n<th>y_min</th>\n<th>y_max</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n<td>0</td>\n<td>0</td>\n</tr>\n<tr>\n<td>1</td>\n<td>2</td>\n<td>3</td>\n<td>0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>2</td>\n<td>0</td>\n<td>1</td>\n<td>2</td>\n<td>3</td>\n</tr>\n<tr>\n<td>3</td>\n<td>2</td>\n<td>3</td>\n<td>3</td>\n<td>3</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>For default minmax index, we filter only one granule when filtering <code>x=2</code> and  4 granules when filtering <code>y=2</code>. For z-order data, the results are<br>2 and 2. It seems that z-order data may be litter better than default one but what if we have more than 3 dimensions filter conditions? Therefore, arranging data according to z-order can help us filter out data does not need to be read, achieving better dataskipping.</p>\n<h2 id=\"Z-order-in-OLTP-amp-OLAP\"><a href=\"#Z-order-in-OLTP-amp-OLAP\" class=\"headerlink\" title=\"Z-order in OLTP &amp; OLAP\"></a>Z-order in OLTP &amp; OLAP</h2><p>In conclusion, Z-order in OLAP can be more popular than OLTP. There are two reasons as follows.</p>\n<p>One is due to the order of physical data that data can only be ordered as one rule in disk. If we put data into disk ordered by primary key, it can’t be ordered by another key. It means we can’t get satisfying performance when filtering some other column as same as filtering sprimary key columns. Another one is we can create serveral B-Tree index in OLTP to replace the capabilities of z-order but OLAP not (because index in memory is too expensive for OLAP).</p>\n<h2 id=\"Extension-GeoHash-VS-Morton-Code\"><a href=\"#Extension-GeoHash-VS-Morton-Code\" class=\"headerlink\" title=\"Extension: GeoHash VS Morton Code\"></a>Extension: GeoHash VS Morton Code</h2><ol>\n<li><a href=\"https://zhuanlan.zhihu.com/p/35940647\">https://zhuanlan.zhihu.com/p/35940647</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/468542418\">https://zhuanlan.zhihu.com/p/468542418</a> (z-order code, but I don’t understand:( )</li>\n</ol>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li><a href=\"https://blog.cloudera.com/speeding-up-queries-with-z-order/\">https://blog.cloudera.com/speeding-up-queries-with-z-order/</a></li>\n<li><a href=\"https://izualzhy.cn/lakehouse-zorder\">https://izualzhy.cn/lakehouse-zorder</a></li>\n<li><a href=\"https://juejin.cn/post/7118344872947515428\">https://juejin.cn/post/7118344872947515428</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/491256487\">https://zhuanlan.zhihu.com/p/491256487</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"What-is-Z-order-amp-Why-Z-order\"><a href=\"#What-is-Z-order-amp-Why-Z-order\" class=\"headerlink\" title=\"What is Z-order &amp; Why Z-order\"></a>What is Z-order &amp; Why Z-order</h2><p>Z-order is a concept of sorting data. Specifically，to quote Wikipedia, <code>“Z-order maps multidimensional data to one dimension while preserving locality of the data points.”</code> In the field of big data, Columns in table are regarded as dimensions to some extension. In other words, Z-order brings us a better way to organize data.</p>\n<p>Firstly, let’s think about two different ordered data, Z-order data and Linear-order data.</p>\n<p><img src=\"/2023/10/19/z-order/z-order.png#pic_center\" alt=\"z-order data\"></p>\n<p><img src=\"/2023/10/19/z-order/linear-order.png#pic_center\" alt=\"linear-order data\"></p>\n<p>It is difficult for us to determine which one performs better immediately so there is a simple example to examine why z-order data may be the better one.</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">x</th>\n<th style=\"text-align:center\">y</th>\n<th style=\"text-align:center\">z</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>data in z-order:</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">x</th>\n<th style=\"text-align:center\">y</th>\n<th style=\"text-align:center\">z</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">1</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>Now, think about executing a query as follows:</p>\n<ol>\n<li><code>select sum(z) from test where x = 2</code></li>\n<li><code>select sum(z) from test where y = 2</code></li>\n</ol>\n<p>How do we know the query behavior of this SQL? Let’s figure out it in clickhouse.</p>\n<p>default minmax index:</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>granule</th>\n<th>x_min</th>\n<th>x_max</th>\n<th>y_min</th>\n<th>y_max</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>3</td>\n</tr>\n<tr>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>0</td>\n<td>2</td>\n</tr>\n<tr>\n<td>2</td>\n<td>2</td>\n<td>2</td>\n<td>1</td>\n<td>3</td>\n</tr>\n<tr>\n<td>3</td>\n<td>3</td>\n<td>3</td>\n<td>0</td>\n<td>3</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>minmax index with z-order:</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>granule</th>\n<th>x_min</th>\n<th>x_max</th>\n<th>y_min</th>\n<th>y_max</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n<td>0</td>\n<td>0</td>\n</tr>\n<tr>\n<td>1</td>\n<td>2</td>\n<td>3</td>\n<td>0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>2</td>\n<td>0</td>\n<td>1</td>\n<td>2</td>\n<td>3</td>\n</tr>\n<tr>\n<td>3</td>\n<td>2</td>\n<td>3</td>\n<td>3</td>\n<td>3</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>For default minmax index, we filter only one granule when filtering <code>x=2</code> and  4 granules when filtering <code>y=2</code>. For z-order data, the results are<br>2 and 2. It seems that z-order data may be litter better than default one but what if we have more than 3 dimensions filter conditions? Therefore, arranging data according to z-order can help us filter out data does not need to be read, achieving better dataskipping.</p>\n<h2 id=\"Z-order-in-OLTP-amp-OLAP\"><a href=\"#Z-order-in-OLTP-amp-OLAP\" class=\"headerlink\" title=\"Z-order in OLTP &amp; OLAP\"></a>Z-order in OLTP &amp; OLAP</h2><p>In conclusion, Z-order in OLAP can be more popular than OLTP. There are two reasons as follows.</p>\n<p>One is due to the order of physical data that data can only be ordered as one rule in disk. If we put data into disk ordered by primary key, it can’t be ordered by another key. It means we can’t get satisfying performance when filtering some other column as same as filtering sprimary key columns. Another one is we can create serveral B-Tree index in OLTP to replace the capabilities of z-order but OLAP not (because index in memory is too expensive for OLAP).</p>\n<h2 id=\"Extension-GeoHash-VS-Morton-Code\"><a href=\"#Extension-GeoHash-VS-Morton-Code\" class=\"headerlink\" title=\"Extension: GeoHash VS Morton Code\"></a>Extension: GeoHash VS Morton Code</h2><ol>\n<li><a href=\"https://zhuanlan.zhihu.com/p/35940647\">https://zhuanlan.zhihu.com/p/35940647</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/468542418\">https://zhuanlan.zhihu.com/p/468542418</a> (z-order code, but I don’t understand:( )</li>\n</ol>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li><a href=\"https://blog.cloudera.com/speeding-up-queries-with-z-order/\">https://blog.cloudera.com/speeding-up-queries-with-z-order/</a></li>\n<li><a href=\"https://izualzhy.cn/lakehouse-zorder\">https://izualzhy.cn/lakehouse-zorder</a></li>\n<li><a href=\"https://juejin.cn/post/7118344872947515428\">https://juejin.cn/post/7118344872947515428</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/491256487\">https://zhuanlan.zhihu.com/p/491256487</a></li>\n</ol>\n"},{"title":"Snowflake CDC VS Hudi CDC","date":"2023-10-30T01:55:24.000Z","_content":"## Batch vs. Streaming\n\nBatch and streaming are different ways to treat data transformation. We usually talk about `Flow-Batch integration` artitechture but I have not seen any applications which can do self-adaption batch or streaming operation in the same semantic and I totally agree the point of view proposed by [this article](https://dl.acm.org/doi/pdf/10.1145/3589776).\n\n> batch systems process input datasets in their entirety to produce new output datasets in their entirety.\n>\n> streaming systems process the changes to input datasets over time to incrementally evolve their corresponding outputs over time.\n\nIn my opinion, streaming systems combine the data and time semantic but batch systems only concerns about data. It leads to a new physical concept: `changes`. In Snowflake, `Streams` are based on `changes` which capture the changes to a dataset over time.\n\n## MySQL redo log\n\nFirstly, let us talk about what is redo log and why we need redo log.\n\nIn mySQL design, to avoid disk operation each DML and load data from disk each query statement, there is a `buffer pool` to reduce disk I/O and mySQL will query the adjacent data of aiming data, then put them into buffer pool as one page.\n\nHowever, when we make changes to the data in the buffer pool, the buffer pool does not immediately flush data into disk, resulting in inconsistency between the data in the buffer pool and the data on the disk. It is called a dirty page. If MySQL crashed when dirty page happened, ACID was destroyed.\n\n![buffer pool](buffer_pool.png#pic_center)\n\n## Implementation\n\nd\n\n## Reference\n\n1. https://dl.acm.org/doi/pdf/10.1145/3589776\n2. https://15445.courses.cs.cmu.edu/fall2021/notes/05-bufferpool.pdf\n","source":"_posts/snowflake-stream.md","raw":"---\ntitle: Snowflake CDC VS Hudi CDC\ndate: 2023-10-30 09:55:24\ntags: stream cdc snowflake\n---\n## Batch vs. Streaming\n\nBatch and streaming are different ways to treat data transformation. We usually talk about `Flow-Batch integration` artitechture but I have not seen any applications which can do self-adaption batch or streaming operation in the same semantic and I totally agree the point of view proposed by [this article](https://dl.acm.org/doi/pdf/10.1145/3589776).\n\n> batch systems process input datasets in their entirety to produce new output datasets in their entirety.\n>\n> streaming systems process the changes to input datasets over time to incrementally evolve their corresponding outputs over time.\n\nIn my opinion, streaming systems combine the data and time semantic but batch systems only concerns about data. It leads to a new physical concept: `changes`. In Snowflake, `Streams` are based on `changes` which capture the changes to a dataset over time.\n\n## MySQL redo log\n\nFirstly, let us talk about what is redo log and why we need redo log.\n\nIn mySQL design, to avoid disk operation each DML and load data from disk each query statement, there is a `buffer pool` to reduce disk I/O and mySQL will query the adjacent data of aiming data, then put them into buffer pool as one page.\n\nHowever, when we make changes to the data in the buffer pool, the buffer pool does not immediately flush data into disk, resulting in inconsistency between the data in the buffer pool and the data on the disk. It is called a dirty page. If MySQL crashed when dirty page happened, ACID was destroyed.\n\n![buffer pool](buffer_pool.png#pic_center)\n\n## Implementation\n\nd\n\n## Reference\n\n1. https://dl.acm.org/doi/pdf/10.1145/3589776\n2. https://15445.courses.cs.cmu.edu/fall2021/notes/05-bufferpool.pdf\n","slug":"snowflake-stream","published":1,"updated":"2023-12-29T07:10:04.831Z","_id":"clqfx3ahf0000b7c37bxb0miw","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"Batch-vs-Streaming\"><a href=\"#Batch-vs-Streaming\" class=\"headerlink\" title=\"Batch vs. Streaming\"></a>Batch vs. Streaming</h2><p>Batch and streaming are different ways to treat data transformation. We usually talk about <code>Flow-Batch integration</code> artitechture but I have not seen any applications which can do self-adaption batch or streaming operation in the same semantic and I totally agree the point of view proposed by <a href=\"https://dl.acm.org/doi/pdf/10.1145/3589776\">this article</a>.</p>\n<blockquote>\n<p>batch systems process input datasets in their entirety to produce new output datasets in their entirety.</p>\n<p>streaming systems process the changes to input datasets over time to incrementally evolve their corresponding outputs over time.</p>\n</blockquote>\n<p>In my opinion, streaming systems combine the data and time semantic but batch systems only concerns about data. It leads to a new physical concept: <code>changes</code>. In Snowflake, <code>Streams</code> are based on <code>changes</code> which capture the changes to a dataset over time.</p>\n<h2 id=\"MySQL-redo-log\"><a href=\"#MySQL-redo-log\" class=\"headerlink\" title=\"MySQL redo log\"></a>MySQL redo log</h2><p>Firstly, let us talk about what is redo log and why we need redo log.</p>\n<p>In mySQL design, to avoid disk operation each DML and load data from disk each query statement, there is a <code>buffer pool</code> to reduce disk I/O and mySQL will query the adjacent data of aiming data, then put them into buffer pool as one page.</p>\n<p>However, when we make changes to the data in the buffer pool, the buffer pool does not immediately flush data into disk, resulting in inconsistency between the data in the buffer pool and the data on the disk. It is called a dirty page. If MySQL crashed when dirty page happened, ACID was destroyed.</p>\n<p><img src=\"/2023/10/30/snowflake-stream/buffer_pool.png#pic_center\" alt=\"buffer pool\"></p>\n<h2 id=\"Implementation\"><a href=\"#Implementation\" class=\"headerlink\" title=\"Implementation\"></a>Implementation</h2><p>d</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li><a href=\"https://dl.acm.org/doi/pdf/10.1145/3589776\">https://dl.acm.org/doi/pdf/10.1145/3589776</a></li>\n<li><a href=\"https://15445.courses.cs.cmu.edu/fall2021/notes/05-bufferpool.pdf\">https://15445.courses.cs.cmu.edu/fall2021/notes/05-bufferpool.pdf</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Batch-vs-Streaming\"><a href=\"#Batch-vs-Streaming\" class=\"headerlink\" title=\"Batch vs. Streaming\"></a>Batch vs. Streaming</h2><p>Batch and streaming are different ways to treat data transformation. We usually talk about <code>Flow-Batch integration</code> artitechture but I have not seen any applications which can do self-adaption batch or streaming operation in the same semantic and I totally agree the point of view proposed by <a href=\"https://dl.acm.org/doi/pdf/10.1145/3589776\">this article</a>.</p>\n<blockquote>\n<p>batch systems process input datasets in their entirety to produce new output datasets in their entirety.</p>\n<p>streaming systems process the changes to input datasets over time to incrementally evolve their corresponding outputs over time.</p>\n</blockquote>\n<p>In my opinion, streaming systems combine the data and time semantic but batch systems only concerns about data. It leads to a new physical concept: <code>changes</code>. In Snowflake, <code>Streams</code> are based on <code>changes</code> which capture the changes to a dataset over time.</p>\n<h2 id=\"MySQL-redo-log\"><a href=\"#MySQL-redo-log\" class=\"headerlink\" title=\"MySQL redo log\"></a>MySQL redo log</h2><p>Firstly, let us talk about what is redo log and why we need redo log.</p>\n<p>In mySQL design, to avoid disk operation each DML and load data from disk each query statement, there is a <code>buffer pool</code> to reduce disk I/O and mySQL will query the adjacent data of aiming data, then put them into buffer pool as one page.</p>\n<p>However, when we make changes to the data in the buffer pool, the buffer pool does not immediately flush data into disk, resulting in inconsistency between the data in the buffer pool and the data on the disk. It is called a dirty page. If MySQL crashed when dirty page happened, ACID was destroyed.</p>\n<p><img src=\"/2023/10/30/snowflake-stream/buffer_pool.png#pic_center\" alt=\"buffer pool\"></p>\n<h2 id=\"Implementation\"><a href=\"#Implementation\" class=\"headerlink\" title=\"Implementation\"></a>Implementation</h2><p>d</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li><a href=\"https://dl.acm.org/doi/pdf/10.1145/3589776\">https://dl.acm.org/doi/pdf/10.1145/3589776</a></li>\n<li><a href=\"https://15445.courses.cs.cmu.edu/fall2021/notes/05-bufferpool.pdf\">https://15445.courses.cs.cmu.edu/fall2021/notes/05-bufferpool.pdf</a></li>\n</ol>\n"},{"title":"mysql-cdc","date":"2023-12-21T07:38:37.000Z","_content":"## CDC Process\n\nA common binlog stream reader contains the parts as follows. However, in many open source library or project, some important steps are boxed as internal processes. Frequently, user should only concern about the connection to the server and how to handle the BinlogEvents. I will use [rust-mysql-cdc](https://github.com/rusuly/mysql_cdc) to introduce the whole process of mysql cdc.\n\n### Connect to MySQL Server & Authentication\n\n```rust\npub fn connect(&self) -> Result<(PacketChannel, DatabaseProvider), Error> {\n        let mut channel = PacketChannel::new(&self.options)?;\n        let (packet, seq_num) = channel.read_packet()?;\n        check_error_packet(&packet, \"Initial handshake error.\")?;\n        let handshake = HandshakePacket::parse(&packet)?;\n        let auth_plugin = self.get_auth_plugin(&handshake.auth_plugin_name)?;\n        self.authenticate(&mut channel, &handshake, auth_plugin, seq_num + 1)?;\n        Ok((channel, DatabaseProvider::from(&handshake.server_version)))\n    }\n\n```\n\n1. User provides the connection info and get one PacketChannel(TCPStream).\n2. Get authentication method and authenticate\n\n### Register as a slave\n\n[RegisterSlave](https://mariadb.com/kb/en/com_register_slave/)\n\nIn Rust, we need to construct a message and send to the PacketChannel as above which includes\n\n* COM_REGISTER_SLAVE command\n* server_id\n* **Empty** host user password port rank masterid\n\n```rust\nimpl RegisterSlaveCommand {\n    pub fn new(server_id: u32) -> Self {\n        Self { server_id }\n    }\n\n    pub fn serialize(&self) -> Result<Vec<u8>, io::Error> {\n        let mut vec = Vec::new();\n        let mut cursor = Cursor::new(&mut vec);\n\n        cursor.write_u8(CommandType::RegisterSlave as u8)?;\n        cursor.write_u32::<LittleEndian>(self.server_id)?;\n\n        //Empty host, user, password, port, rank, masterid\n        cursor.write_u8(0)?;\n        cursor.write_u8(0)?;\n        cursor.write_u8(0)?;\n        cursor.write_u16::<LittleEndian>(0)?;\n        cursor.write_u32::<LittleEndian>(0)?;\n        cursor.write_u32::<LittleEndian>(0)?;\n\n        Ok(vec)\n    }\n}\n```\n\n### Binlog Dump\n\nSame as RegisterSlave, DumpBinogCommand needs:\n\n* server_id\n* binlog_filename\n* binlog_position\n\n```rust\npub struct DumpBinlogCommand {\n    pub server_id: u32,\n    pub binlog_filename: String,\n    pub binlog_position: u32,\n    pub flags: u16,\n}\n\nimpl DumpBinlogCommand {\n    pub fn new(server_id: u32, binlog_filename: String, binlog_position: u32) -> Self {\n        Self {\n            server_id,\n            binlog_filename,\n            binlog_position,\n            flags: 0,\n        }\n    }\n\n    pub fn serialize(&self) -> Result<Vec<u8>, io::Error> {\n        let mut vec = Vec::new();\n        let mut cursor = Cursor::new(&mut vec);\n\n        cursor.write_u8(CommandType::BinlogDump as u8)?;\n        cursor.write_u32::<LittleEndian>(self.binlog_position)?;\n        cursor.write_u16::<LittleEndian>(self.flags)?;\n        cursor.write_u32::<LittleEndian>(self.server_id)?;\n        cursor.write(self.binlog_filename.as_bytes())?;\n\n        Ok(vec)\n    }\n}\n```\n\n### Handle Binlog Events\n\nAs mentioned above, usually, we let users handle these events but firstly, we need to parse them.\n\n### HeartBeats\n\n```rust\n    pub fn set_master_heartbeat(&mut self, channel: &mut PacketChannel) -> Result<(), Error> {\n        let milliseconds = self.options.heartbeat_interval.as_millis();\n        let nanoseconds = milliseconds * 1000 * 1000;\n        let query = format!(\"set @master_heartbeat_period={}\", nanoseconds);\n        let command = QueryCommand::new(query.to_string());\n        channel.write_packet(&command.serialize()?, 0)?;\n        let (packet, _) = channel.read_packet()?;\n        check_error_packet(&packet, \"Setting master heartbeat error.\")?;\n        Ok(())\n    }\n```\n\n### Get Better Performance\n\nAs we know, it is impossible for a CDC program to pull and parse binlog events infinitely as a slave. Also, as a developer, you may be not able to adjust the MySQL server parameters to improve the CDC performance. There are some suggestions for you if you are trouble with any relationed issues.\n\n#### Batch-Processing\n\nFor the only slave, you can use batch processing anywhere except pulling binlog events like **parse events**, **handle events** or **sink the events.** There is an example that I used in parsing binlog events.\n\n1. Parser for parsing a single binlog events\n\n```rust\npub fn read_data(&self) -> io::Result<Option<EventData<'_>>> {\n        use EventType::*;\n\n        let event_type = match self.header.event_type.get() {\n            Ok(event_type) => event_type,\n            _ => return Ok(None),\n        };\n\n        let event_data = match event_type {\n            ENUM_END_EVENT | UNKNOWN_EVENT => EventData::UnknownEvent,\n            START_EVENT_V3 => EventData::StartEventV3(Cow::Borrowed(&*self.data)),\n            QUERY_EVENT => EventData::QueryEvent(self.read_event()?),\n            STOP_EVENT => EventData::StopEvent,\n            ROTATE_EVENT => EventData::RotateEvent(self.read_event()?),\n            INTVAR_EVENT => EventData::IntvarEvent(self.read_event()?),\n            LOAD_EVENT => EventData::LoadEvent(Cow::Borrowed(&*self.data)),\n            SLAVE_EVENT => EventData::SlaveEvent,\n            CREATE_FILE_EVENT => EventData::CreateFileEvent(Cow::Borrowed(&*self.data)),\n            APPEND_BLOCK_EVENT => EventData::AppendBlockEvent(Cow::Borrowed(&*self.data)),\n            EXEC_LOAD_EVENT => EventData::ExecLoadEvent(Cow::Borrowed(&*self.data)),\n            DELETE_FILE_EVENT => EventData::DeleteFileEvent(Cow::Borrowed(&*self.data)),\n            NEW_LOAD_EVENT => EventData::NewLoadEvent(Cow::Borrowed(&*self.data)),\n            RAND_EVENT => EventData::RandEvent(self.read_event()?),\n            USER_VAR_EVENT => EventData::UserVarEvent(self.read_event()?),\n            FORMAT_DESCRIPTION_EVENT => {\n                let fde = self\n                    .read_event::<FormatDescriptionEvent>()?\n                    .with_footer(self.footer);\n                EventData::FormatDescriptionEvent(fde)\n            }\n            XID_EVENT => EventData::XidEvent(self.read_event()?),\n            BEGIN_LOAD_QUERY_EVENT => EventData::BeginLoadQueryEvent(self.read_event()?),\n            EXECUTE_LOAD_QUERY_EVENT => EventData::ExecuteLoadQueryEvent(self.read_event()?),\n            TABLE_MAP_EVENT => EventData::TableMapEvent(self.read_event()?),\n            PRE_GA_WRITE_ROWS_EVENT => EventData::PreGaWriteRowsEvent(Cow::Borrowed(&*self.data)),\n            PRE_GA_UPDATE_ROWS_EVENT => EventData::PreGaUpdateRowsEvent(Cow::Borrowed(&*self.data)),\n            PRE_GA_DELETE_ROWS_EVENT => EventData::PreGaDeleteRowsEvent(Cow::Borrowed(&*self.data)),\n            WRITE_ROWS_EVENT_V1 => {\n                EventData::RowsEvent(RowsEventData::WriteRowsEventV1(self.read_event()?))\n            }\n            UPDATE_ROWS_EVENT_V1 => {\n                EventData::RowsEvent(RowsEventData::UpdateRowsEventV1(self.read_event()?))\n            }\n            DELETE_ROWS_EVENT_V1 => {\n                EventData::RowsEvent(RowsEventData::DeleteRowsEventV1(self.read_event()?))\n            }\n            INCIDENT_EVENT => EventData::IncidentEvent(self.read_event()?),\n            HEARTBEAT_EVENT => EventData::HeartbeatEvent,\n            IGNORABLE_EVENT => EventData::IgnorableEvent(Cow::Borrowed(&*self.data)),\n            ROWS_QUERY_EVENT => EventData::RowsQueryEvent(self.read_event()?),\n            WRITE_ROWS_EVENT => {\n                EventData::RowsEvent(RowsEventData::WriteRowsEvent(self.read_event()?))\n            }\n            UPDATE_ROWS_EVENT => {\n                EventData::RowsEvent(RowsEventData::UpdateRowsEvent(self.read_event()?))\n            }\n            DELETE_ROWS_EVENT => {\n                EventData::RowsEvent(RowsEventData::DeleteRowsEvent(self.read_event()?))\n            }\n            GTID_EVENT => EventData::GtidEvent(self.read_event()?),\n            ANONYMOUS_GTID_EVENT => EventData::AnonymousGtidEvent(self.read_event()?),\n            PREVIOUS_GTIDS_EVENT => EventData::PreviousGtidsEvent(Cow::Borrowed(&*self.data)),\n            TRANSACTION_CONTEXT_EVENT => {\n                EventData::TransactionContextEvent(Cow::Borrowed(&*self.data))\n            }\n            VIEW_CHANGE_EVENT => EventData::ViewChangeEvent(Cow::Borrowed(&*self.data)),\n            XA_PREPARE_LOG_EVENT => EventData::XaPrepareLogEvent(Cow::Borrowed(&*self.data)),\n            PARTIAL_UPDATE_ROWS_EVENT => {\n                EventData::RowsEvent(RowsEventData::PartialUpdateRowsEvent(self.read_event()?))\n            }\n            TRANSACTION_PAYLOAD_EVENT => EventData::TransactionPayloadEvent(self.read_event()?),\n        };\n\n        Ok(Some(event_data))\n    }\n```\n\n2. Use the Rayon framework to enable concurrency\n\nMany operations in Rayon, such as map(), filter_map(), and others, do not inherently guarantee that the output order will be the same as the input order. However, when used in combination with collect(), they generally ensure the results are in order. So, if the order of data is needed, please write a test case to test your code.\n\n```rust\nfor result in client.replicate()? {\n        let (header, event) = result?;\n        event_queue.enqueue((header, event));\n        event_count += 1;\n        if event_count.borrow() % 100000 == 0 {\n            let mut event_array = event_queue.drain_all();\n            let new_arr: Vec<(EventHeader, Result<BinlogEvent, Error>)> = event_array.into_par_iter().map(|(header, payload)| {\n                let mut c_table_map = table_map.clone();\n                let parsed_event = parse_event(&header, &payload, &mut c_table_map);\n                (header, parsed_event)\n            }).collect();\n            let dur = start_time.elapsed();\n            println!(\"Process Time Duration: {:?}\", dur);\n            println!(\"Current Binlog Events: {:?}\", event_count);\n            println!(\"Result Length: {:?}\", new_arr.len());\n        }\n    }\n```\n\n5. Choose the needed events\n\nNormally, in CDC program, several binlog event types are as follows:\n\n```rust\n#[allow(non_camel_case_types)]\n#[repr(u8)]\n#[derive(Debug, Clone, Copy, Eq, PartialEq, Hash)]\npub enum EventType {\n\n    /// A `QUERY_EVENT` is created for each query that modifies the database,\n    /// unless the query is logged row-based.\n    QUERY_EVENT = 0x02,\n    /// to tell the reader what binlog to request next.\n    ROTATE_EVENT = 0x04,\n\n    /// if any transaction commit info needed\n    XID_EVENT = 0x10,\n  \n    TABLE_MAP_EVENT = 0x13,\n   \n    WRITE_ROWS_EVENT_V1 = 0x17,\n    UPDATE_ROWS_EVENT_V1 = 0x18,\n    DELETE_ROWS_EVENT_V1 = 0x19,\n  \n    WRITE_ROWS_EVENT = 0x1e,\n    UPDATE_ROWS_EVENT = 0x1f,\n    DELETE_ROWS_EVENT = 0x20,\n    GTID_EVENT = 0x21,\n}\n```\n\n6. Add iterator for your binlog stream reader\n\nIn my code, `client.replicate()` will return `Result<BinlogEvents, Error>`. So we need to implement Iterator trait for BinlogEvents.\n\n```rust\nimpl Iterator for BinlogEvents {\n    type Item = Result<(EventHeader, Vec<u8>), Error>;\n\n    /// Reads binlog event packets from network stream.\n    /// <a href=\"https://mariadb.com/kb/en/3-binlog-network-stream/\">See more</a>\n    fn next(&mut self) -> Option<Self::Item> {\n        let (packet, _) = match self.channel.read_packet() {\n            Ok(x) => x,\n            Err(e) => return Some(Err(Error::IoError(e))),\n        };\n        match packet[0] {\n            ResponseType::OK => Some(self.read_event(&packet)),\n            ResponseType::ERROR => Some(self.read_error(&packet)),\n            ResponseType::END_OF_FILE => {\n                let _ = EndOfFilePacket::parse(&packet[1..]);\n                None\n            }\n            _ => Some(Err(Error::String(\n                \"Unknown network stream status\".to_string(),\n            ))),\n        }\n    }\n}\n```\n\nIn another case, the binlog stream reader implements the Stream trait like `while let Ok(Some(c)) = cdc_stream.try_next().await`.\n\n```rust\nimpl futures_core::stream::Stream for BinlogStream {\n    type Item = Result<ChgcapEvent>;\n\n    fn poll_next(\n        self: Pin<&mut Self>,\n        cx: &mut std::task::Context<'_>,\n    ) -> Poll<Option<Self::Item>> {\n        let this = self.get_mut();\n        // TODO: Support rate limiting.\n        loop {\n            let binlog_stream = Pin::new(&mut this.binlog_stream);\n            return match binlog_stream.poll_next(cx) {\n                Poll::Ready(t) => match t {\n                    Some(event_result) => match event_result {\n                        Ok(event) => match this.handle_event(event) { // here\n                            Ok(change) => match change {\n                                Some(c) => Poll::Ready(Some(Ok(c))),\n                                None => continue, // Skip this event.\n                            },\n                            Err(e) => Poll::Ready(Some(Err(e))),\n                        },\n                        Err(err) => Poll::Ready(Some(Err(anyhow!(err)))),\n                    },\n                    None => Poll::Ready(None), // Completed.\n                },\n                Poll::Pending => Poll::Pending,\n            };\n        }\n    }\n}\n\n```\n\n7. Accumulate unprocessed binlog events into a batch like Step 2\n\n#### Multiple Slave\n\nIn **Batch-Processing** section, we can boost the performance of CDC from tens of thousands of events to over a hundred thousand events per second. But however, if you need higher performance, single-instance batching may not meet your needs.\n\nLuckily, there is an another road to make it. Before introducing this way, let us take eyes on **table_id**.\n\n* table_id is 6 bytes, need pad little-endian number.\n* table_id is a temporary value. it would be changed when:\n  * drop and replace table\n  * rename table\n  * alter table\n  * MySQL Engine upgrade\n* Before every data change events(WriteRowsEvent, UpdateRowsEvent, DeleteRowsEvent), there would be a TableMapEvent including table name, table id and table schema.\n\nNext, you should know how to get table metadata from TableMapEvent.\n\n![TableMapEvent](./mysql-cdc/table_map_event.png)\n\n```python\nclass TableMapEvent(BinLogEvent):\n    \"\"\"This event describes the structure of a table.\n    It's sent before a change happens on a table.\n    An end user of the lib should have no usage of this\n    \"\"\"\n\n    def __init__(self, from_packet, event_size, table_map, ctl_connection, **kwargs):\n        super(TableMapEvent, self).__init__(from_packet, event_size,\n                                            table_map, ctl_connection, **kwargs)\n        self.__only_tables = kwargs[\"only_tables\"]\n        self.__ignored_tables = kwargs[\"ignored_tables\"]\n        self.__only_schemas = kwargs[\"only_schemas\"]\n        self.__ignored_schemas = kwargs[\"ignored_schemas\"]\n        self.__freeze_schema = kwargs[\"freeze_schema\"]\n\n        # Post-Header\n        self.table_id = self._read_table_id()\n\n        if self.table_id in table_map and self.__freeze_schema:\n            self._processed = False\n            return\n\n        self.flags = struct.unpack('<H', self.packet.read(2))[0]\n\n        # Payload\n        self.schema_length = struct.unpack(\"!B\", self.packet.read(1))[0]\n        self.schema = self.packet.read(self.schema_length).decode()\n        self.packet.advance(1)\n        self.table_length = struct.unpack(\"!B\", self.packet.read(1))[0]\n        self.table = self.packet.read(self.table_length).decode()\n\n        if self.__only_tables is not None and self.table not in self.__only_tables:\n            self._processed = False\n            return\n        elif self.__ignored_tables is not None and self.table in self.__ignored_tables:\n            self._processed = False\n            return\n\n        if self.__only_schemas is not None and self.schema not in self.__only_schemas:\n            self._processed = False\n            return\n        elif self.__ignored_schemas is not None and self.schema in self.__ignored_schemas:\n            self._processed = False\n            return\n\n        self.packet.advance(1)\n        self.column_count = self.packet.read_length_coded_binary()\n\n        self.columns = []\n\n        if self.table_id in table_map:\n            self.column_schemas = table_map[self.table_id].column_schemas\n        else:\n            self.column_schemas = self._ctl_connection._get_table_information(self.schema, self.table)\n\n        ordinal_pos_loc = 0\n\n        if len(self.column_schemas) != 0:\n            # Read columns meta data\n            column_types = bytearray(self.packet.read(self.column_count))\n            self.packet.read_length_coded_binary()\n            for i in range(0, len(column_types)):\n                column_type = column_types[i]\n                try:\n                    column_schema = self.column_schemas[ordinal_pos_loc]\n\n                    # only acknowledge the column definition if the iteration matches with ordinal position of\n                    # the column. this helps in maintaining support for restricted columnar access\n                    if i != (column_schema['ORDINAL_POSITION'] - 1):\n                        # raise IndexError to follow the workflow of dropping columns which are not matching the\n                        # underlying table schema\n                        raise IndexError\n\n                    ordinal_pos_loc += 1\n                except IndexError:\n                    # this a dirty hack to prevent row events containing columns which have been dropped prior\n                    # to pymysqlreplication start, but replayed from binlog from blowing up the service.\n                    # TODO: this does not address the issue if the column other than the last one is dropped\n                    column_schema = {\n                        'COLUMN_NAME': '__dropped_col_{i}__'.format(i=i),\n                        'COLLATION_NAME': None,\n                        'CHARACTER_SET_NAME': None,\n                        'COLUMN_COMMENT': None,\n                        'COLUMN_TYPE': 'BLOB',  # we don't know what it is, so let's not do anything with it.\n                        'COLUMN_KEY': '',\n                    }\n                col = Column(column_type, column_schema, from_packet)\n                self.columns.append(col)\n\n        self.table_obj = Table(self.column_schemas, self.table_id, self.schema,\n                               self.table, self.columns)\n\n        # ith column is nullable if (i - 1)th bit is set to True, not nullable otherwise\n        ## Refer to definition of and call to row.event._is_null() to interpret bitmap corresponding to columns\n        self.null_bitmask = self.packet.read((self.column_count + 7) / 8)\n```\n\nCongratulations! You have known how to divide your binlog stream reader to different database(schema) / table.\n\n![RowEvents](./mysql-cdc/row_events.png)\n\nFor TableMapEvent, we can store the table_id for the RowEvents. When any RowEvents parse the table_id and it is not in the table map cache, the event should be ignore.\n\n## Reference\n\n1. https://arxiv.org/pdf/2010.12597.pdf\n2. https://github.com/blackbeam/mysql_async\n3. https://github.com/neverchanje/chgcap-rs\n","source":"_posts/mysql-cdc.md","raw":"---\ntitle: mysql-cdc\ndate: 2023-12-21 15:38:37\ntags: cdc mysql binlog\n---\n## CDC Process\n\nA common binlog stream reader contains the parts as follows. However, in many open source library or project, some important steps are boxed as internal processes. Frequently, user should only concern about the connection to the server and how to handle the BinlogEvents. I will use [rust-mysql-cdc](https://github.com/rusuly/mysql_cdc) to introduce the whole process of mysql cdc.\n\n### Connect to MySQL Server & Authentication\n\n```rust\npub fn connect(&self) -> Result<(PacketChannel, DatabaseProvider), Error> {\n        let mut channel = PacketChannel::new(&self.options)?;\n        let (packet, seq_num) = channel.read_packet()?;\n        check_error_packet(&packet, \"Initial handshake error.\")?;\n        let handshake = HandshakePacket::parse(&packet)?;\n        let auth_plugin = self.get_auth_plugin(&handshake.auth_plugin_name)?;\n        self.authenticate(&mut channel, &handshake, auth_plugin, seq_num + 1)?;\n        Ok((channel, DatabaseProvider::from(&handshake.server_version)))\n    }\n\n```\n\n1. User provides the connection info and get one PacketChannel(TCPStream).\n2. Get authentication method and authenticate\n\n### Register as a slave\n\n[RegisterSlave](https://mariadb.com/kb/en/com_register_slave/)\n\nIn Rust, we need to construct a message and send to the PacketChannel as above which includes\n\n* COM_REGISTER_SLAVE command\n* server_id\n* **Empty** host user password port rank masterid\n\n```rust\nimpl RegisterSlaveCommand {\n    pub fn new(server_id: u32) -> Self {\n        Self { server_id }\n    }\n\n    pub fn serialize(&self) -> Result<Vec<u8>, io::Error> {\n        let mut vec = Vec::new();\n        let mut cursor = Cursor::new(&mut vec);\n\n        cursor.write_u8(CommandType::RegisterSlave as u8)?;\n        cursor.write_u32::<LittleEndian>(self.server_id)?;\n\n        //Empty host, user, password, port, rank, masterid\n        cursor.write_u8(0)?;\n        cursor.write_u8(0)?;\n        cursor.write_u8(0)?;\n        cursor.write_u16::<LittleEndian>(0)?;\n        cursor.write_u32::<LittleEndian>(0)?;\n        cursor.write_u32::<LittleEndian>(0)?;\n\n        Ok(vec)\n    }\n}\n```\n\n### Binlog Dump\n\nSame as RegisterSlave, DumpBinogCommand needs:\n\n* server_id\n* binlog_filename\n* binlog_position\n\n```rust\npub struct DumpBinlogCommand {\n    pub server_id: u32,\n    pub binlog_filename: String,\n    pub binlog_position: u32,\n    pub flags: u16,\n}\n\nimpl DumpBinlogCommand {\n    pub fn new(server_id: u32, binlog_filename: String, binlog_position: u32) -> Self {\n        Self {\n            server_id,\n            binlog_filename,\n            binlog_position,\n            flags: 0,\n        }\n    }\n\n    pub fn serialize(&self) -> Result<Vec<u8>, io::Error> {\n        let mut vec = Vec::new();\n        let mut cursor = Cursor::new(&mut vec);\n\n        cursor.write_u8(CommandType::BinlogDump as u8)?;\n        cursor.write_u32::<LittleEndian>(self.binlog_position)?;\n        cursor.write_u16::<LittleEndian>(self.flags)?;\n        cursor.write_u32::<LittleEndian>(self.server_id)?;\n        cursor.write(self.binlog_filename.as_bytes())?;\n\n        Ok(vec)\n    }\n}\n```\n\n### Handle Binlog Events\n\nAs mentioned above, usually, we let users handle these events but firstly, we need to parse them.\n\n### HeartBeats\n\n```rust\n    pub fn set_master_heartbeat(&mut self, channel: &mut PacketChannel) -> Result<(), Error> {\n        let milliseconds = self.options.heartbeat_interval.as_millis();\n        let nanoseconds = milliseconds * 1000 * 1000;\n        let query = format!(\"set @master_heartbeat_period={}\", nanoseconds);\n        let command = QueryCommand::new(query.to_string());\n        channel.write_packet(&command.serialize()?, 0)?;\n        let (packet, _) = channel.read_packet()?;\n        check_error_packet(&packet, \"Setting master heartbeat error.\")?;\n        Ok(())\n    }\n```\n\n### Get Better Performance\n\nAs we know, it is impossible for a CDC program to pull and parse binlog events infinitely as a slave. Also, as a developer, you may be not able to adjust the MySQL server parameters to improve the CDC performance. There are some suggestions for you if you are trouble with any relationed issues.\n\n#### Batch-Processing\n\nFor the only slave, you can use batch processing anywhere except pulling binlog events like **parse events**, **handle events** or **sink the events.** There is an example that I used in parsing binlog events.\n\n1. Parser for parsing a single binlog events\n\n```rust\npub fn read_data(&self) -> io::Result<Option<EventData<'_>>> {\n        use EventType::*;\n\n        let event_type = match self.header.event_type.get() {\n            Ok(event_type) => event_type,\n            _ => return Ok(None),\n        };\n\n        let event_data = match event_type {\n            ENUM_END_EVENT | UNKNOWN_EVENT => EventData::UnknownEvent,\n            START_EVENT_V3 => EventData::StartEventV3(Cow::Borrowed(&*self.data)),\n            QUERY_EVENT => EventData::QueryEvent(self.read_event()?),\n            STOP_EVENT => EventData::StopEvent,\n            ROTATE_EVENT => EventData::RotateEvent(self.read_event()?),\n            INTVAR_EVENT => EventData::IntvarEvent(self.read_event()?),\n            LOAD_EVENT => EventData::LoadEvent(Cow::Borrowed(&*self.data)),\n            SLAVE_EVENT => EventData::SlaveEvent,\n            CREATE_FILE_EVENT => EventData::CreateFileEvent(Cow::Borrowed(&*self.data)),\n            APPEND_BLOCK_EVENT => EventData::AppendBlockEvent(Cow::Borrowed(&*self.data)),\n            EXEC_LOAD_EVENT => EventData::ExecLoadEvent(Cow::Borrowed(&*self.data)),\n            DELETE_FILE_EVENT => EventData::DeleteFileEvent(Cow::Borrowed(&*self.data)),\n            NEW_LOAD_EVENT => EventData::NewLoadEvent(Cow::Borrowed(&*self.data)),\n            RAND_EVENT => EventData::RandEvent(self.read_event()?),\n            USER_VAR_EVENT => EventData::UserVarEvent(self.read_event()?),\n            FORMAT_DESCRIPTION_EVENT => {\n                let fde = self\n                    .read_event::<FormatDescriptionEvent>()?\n                    .with_footer(self.footer);\n                EventData::FormatDescriptionEvent(fde)\n            }\n            XID_EVENT => EventData::XidEvent(self.read_event()?),\n            BEGIN_LOAD_QUERY_EVENT => EventData::BeginLoadQueryEvent(self.read_event()?),\n            EXECUTE_LOAD_QUERY_EVENT => EventData::ExecuteLoadQueryEvent(self.read_event()?),\n            TABLE_MAP_EVENT => EventData::TableMapEvent(self.read_event()?),\n            PRE_GA_WRITE_ROWS_EVENT => EventData::PreGaWriteRowsEvent(Cow::Borrowed(&*self.data)),\n            PRE_GA_UPDATE_ROWS_EVENT => EventData::PreGaUpdateRowsEvent(Cow::Borrowed(&*self.data)),\n            PRE_GA_DELETE_ROWS_EVENT => EventData::PreGaDeleteRowsEvent(Cow::Borrowed(&*self.data)),\n            WRITE_ROWS_EVENT_V1 => {\n                EventData::RowsEvent(RowsEventData::WriteRowsEventV1(self.read_event()?))\n            }\n            UPDATE_ROWS_EVENT_V1 => {\n                EventData::RowsEvent(RowsEventData::UpdateRowsEventV1(self.read_event()?))\n            }\n            DELETE_ROWS_EVENT_V1 => {\n                EventData::RowsEvent(RowsEventData::DeleteRowsEventV1(self.read_event()?))\n            }\n            INCIDENT_EVENT => EventData::IncidentEvent(self.read_event()?),\n            HEARTBEAT_EVENT => EventData::HeartbeatEvent,\n            IGNORABLE_EVENT => EventData::IgnorableEvent(Cow::Borrowed(&*self.data)),\n            ROWS_QUERY_EVENT => EventData::RowsQueryEvent(self.read_event()?),\n            WRITE_ROWS_EVENT => {\n                EventData::RowsEvent(RowsEventData::WriteRowsEvent(self.read_event()?))\n            }\n            UPDATE_ROWS_EVENT => {\n                EventData::RowsEvent(RowsEventData::UpdateRowsEvent(self.read_event()?))\n            }\n            DELETE_ROWS_EVENT => {\n                EventData::RowsEvent(RowsEventData::DeleteRowsEvent(self.read_event()?))\n            }\n            GTID_EVENT => EventData::GtidEvent(self.read_event()?),\n            ANONYMOUS_GTID_EVENT => EventData::AnonymousGtidEvent(self.read_event()?),\n            PREVIOUS_GTIDS_EVENT => EventData::PreviousGtidsEvent(Cow::Borrowed(&*self.data)),\n            TRANSACTION_CONTEXT_EVENT => {\n                EventData::TransactionContextEvent(Cow::Borrowed(&*self.data))\n            }\n            VIEW_CHANGE_EVENT => EventData::ViewChangeEvent(Cow::Borrowed(&*self.data)),\n            XA_PREPARE_LOG_EVENT => EventData::XaPrepareLogEvent(Cow::Borrowed(&*self.data)),\n            PARTIAL_UPDATE_ROWS_EVENT => {\n                EventData::RowsEvent(RowsEventData::PartialUpdateRowsEvent(self.read_event()?))\n            }\n            TRANSACTION_PAYLOAD_EVENT => EventData::TransactionPayloadEvent(self.read_event()?),\n        };\n\n        Ok(Some(event_data))\n    }\n```\n\n2. Use the Rayon framework to enable concurrency\n\nMany operations in Rayon, such as map(), filter_map(), and others, do not inherently guarantee that the output order will be the same as the input order. However, when used in combination with collect(), they generally ensure the results are in order. So, if the order of data is needed, please write a test case to test your code.\n\n```rust\nfor result in client.replicate()? {\n        let (header, event) = result?;\n        event_queue.enqueue((header, event));\n        event_count += 1;\n        if event_count.borrow() % 100000 == 0 {\n            let mut event_array = event_queue.drain_all();\n            let new_arr: Vec<(EventHeader, Result<BinlogEvent, Error>)> = event_array.into_par_iter().map(|(header, payload)| {\n                let mut c_table_map = table_map.clone();\n                let parsed_event = parse_event(&header, &payload, &mut c_table_map);\n                (header, parsed_event)\n            }).collect();\n            let dur = start_time.elapsed();\n            println!(\"Process Time Duration: {:?}\", dur);\n            println!(\"Current Binlog Events: {:?}\", event_count);\n            println!(\"Result Length: {:?}\", new_arr.len());\n        }\n    }\n```\n\n5. Choose the needed events\n\nNormally, in CDC program, several binlog event types are as follows:\n\n```rust\n#[allow(non_camel_case_types)]\n#[repr(u8)]\n#[derive(Debug, Clone, Copy, Eq, PartialEq, Hash)]\npub enum EventType {\n\n    /// A `QUERY_EVENT` is created for each query that modifies the database,\n    /// unless the query is logged row-based.\n    QUERY_EVENT = 0x02,\n    /// to tell the reader what binlog to request next.\n    ROTATE_EVENT = 0x04,\n\n    /// if any transaction commit info needed\n    XID_EVENT = 0x10,\n  \n    TABLE_MAP_EVENT = 0x13,\n   \n    WRITE_ROWS_EVENT_V1 = 0x17,\n    UPDATE_ROWS_EVENT_V1 = 0x18,\n    DELETE_ROWS_EVENT_V1 = 0x19,\n  \n    WRITE_ROWS_EVENT = 0x1e,\n    UPDATE_ROWS_EVENT = 0x1f,\n    DELETE_ROWS_EVENT = 0x20,\n    GTID_EVENT = 0x21,\n}\n```\n\n6. Add iterator for your binlog stream reader\n\nIn my code, `client.replicate()` will return `Result<BinlogEvents, Error>`. So we need to implement Iterator trait for BinlogEvents.\n\n```rust\nimpl Iterator for BinlogEvents {\n    type Item = Result<(EventHeader, Vec<u8>), Error>;\n\n    /// Reads binlog event packets from network stream.\n    /// <a href=\"https://mariadb.com/kb/en/3-binlog-network-stream/\">See more</a>\n    fn next(&mut self) -> Option<Self::Item> {\n        let (packet, _) = match self.channel.read_packet() {\n            Ok(x) => x,\n            Err(e) => return Some(Err(Error::IoError(e))),\n        };\n        match packet[0] {\n            ResponseType::OK => Some(self.read_event(&packet)),\n            ResponseType::ERROR => Some(self.read_error(&packet)),\n            ResponseType::END_OF_FILE => {\n                let _ = EndOfFilePacket::parse(&packet[1..]);\n                None\n            }\n            _ => Some(Err(Error::String(\n                \"Unknown network stream status\".to_string(),\n            ))),\n        }\n    }\n}\n```\n\nIn another case, the binlog stream reader implements the Stream trait like `while let Ok(Some(c)) = cdc_stream.try_next().await`.\n\n```rust\nimpl futures_core::stream::Stream for BinlogStream {\n    type Item = Result<ChgcapEvent>;\n\n    fn poll_next(\n        self: Pin<&mut Self>,\n        cx: &mut std::task::Context<'_>,\n    ) -> Poll<Option<Self::Item>> {\n        let this = self.get_mut();\n        // TODO: Support rate limiting.\n        loop {\n            let binlog_stream = Pin::new(&mut this.binlog_stream);\n            return match binlog_stream.poll_next(cx) {\n                Poll::Ready(t) => match t {\n                    Some(event_result) => match event_result {\n                        Ok(event) => match this.handle_event(event) { // here\n                            Ok(change) => match change {\n                                Some(c) => Poll::Ready(Some(Ok(c))),\n                                None => continue, // Skip this event.\n                            },\n                            Err(e) => Poll::Ready(Some(Err(e))),\n                        },\n                        Err(err) => Poll::Ready(Some(Err(anyhow!(err)))),\n                    },\n                    None => Poll::Ready(None), // Completed.\n                },\n                Poll::Pending => Poll::Pending,\n            };\n        }\n    }\n}\n\n```\n\n7. Accumulate unprocessed binlog events into a batch like Step 2\n\n#### Multiple Slave\n\nIn **Batch-Processing** section, we can boost the performance of CDC from tens of thousands of events to over a hundred thousand events per second. But however, if you need higher performance, single-instance batching may not meet your needs.\n\nLuckily, there is an another road to make it. Before introducing this way, let us take eyes on **table_id**.\n\n* table_id is 6 bytes, need pad little-endian number.\n* table_id is a temporary value. it would be changed when:\n  * drop and replace table\n  * rename table\n  * alter table\n  * MySQL Engine upgrade\n* Before every data change events(WriteRowsEvent, UpdateRowsEvent, DeleteRowsEvent), there would be a TableMapEvent including table name, table id and table schema.\n\nNext, you should know how to get table metadata from TableMapEvent.\n\n![TableMapEvent](./mysql-cdc/table_map_event.png)\n\n```python\nclass TableMapEvent(BinLogEvent):\n    \"\"\"This event describes the structure of a table.\n    It's sent before a change happens on a table.\n    An end user of the lib should have no usage of this\n    \"\"\"\n\n    def __init__(self, from_packet, event_size, table_map, ctl_connection, **kwargs):\n        super(TableMapEvent, self).__init__(from_packet, event_size,\n                                            table_map, ctl_connection, **kwargs)\n        self.__only_tables = kwargs[\"only_tables\"]\n        self.__ignored_tables = kwargs[\"ignored_tables\"]\n        self.__only_schemas = kwargs[\"only_schemas\"]\n        self.__ignored_schemas = kwargs[\"ignored_schemas\"]\n        self.__freeze_schema = kwargs[\"freeze_schema\"]\n\n        # Post-Header\n        self.table_id = self._read_table_id()\n\n        if self.table_id in table_map and self.__freeze_schema:\n            self._processed = False\n            return\n\n        self.flags = struct.unpack('<H', self.packet.read(2))[0]\n\n        # Payload\n        self.schema_length = struct.unpack(\"!B\", self.packet.read(1))[0]\n        self.schema = self.packet.read(self.schema_length).decode()\n        self.packet.advance(1)\n        self.table_length = struct.unpack(\"!B\", self.packet.read(1))[0]\n        self.table = self.packet.read(self.table_length).decode()\n\n        if self.__only_tables is not None and self.table not in self.__only_tables:\n            self._processed = False\n            return\n        elif self.__ignored_tables is not None and self.table in self.__ignored_tables:\n            self._processed = False\n            return\n\n        if self.__only_schemas is not None and self.schema not in self.__only_schemas:\n            self._processed = False\n            return\n        elif self.__ignored_schemas is not None and self.schema in self.__ignored_schemas:\n            self._processed = False\n            return\n\n        self.packet.advance(1)\n        self.column_count = self.packet.read_length_coded_binary()\n\n        self.columns = []\n\n        if self.table_id in table_map:\n            self.column_schemas = table_map[self.table_id].column_schemas\n        else:\n            self.column_schemas = self._ctl_connection._get_table_information(self.schema, self.table)\n\n        ordinal_pos_loc = 0\n\n        if len(self.column_schemas) != 0:\n            # Read columns meta data\n            column_types = bytearray(self.packet.read(self.column_count))\n            self.packet.read_length_coded_binary()\n            for i in range(0, len(column_types)):\n                column_type = column_types[i]\n                try:\n                    column_schema = self.column_schemas[ordinal_pos_loc]\n\n                    # only acknowledge the column definition if the iteration matches with ordinal position of\n                    # the column. this helps in maintaining support for restricted columnar access\n                    if i != (column_schema['ORDINAL_POSITION'] - 1):\n                        # raise IndexError to follow the workflow of dropping columns which are not matching the\n                        # underlying table schema\n                        raise IndexError\n\n                    ordinal_pos_loc += 1\n                except IndexError:\n                    # this a dirty hack to prevent row events containing columns which have been dropped prior\n                    # to pymysqlreplication start, but replayed from binlog from blowing up the service.\n                    # TODO: this does not address the issue if the column other than the last one is dropped\n                    column_schema = {\n                        'COLUMN_NAME': '__dropped_col_{i}__'.format(i=i),\n                        'COLLATION_NAME': None,\n                        'CHARACTER_SET_NAME': None,\n                        'COLUMN_COMMENT': None,\n                        'COLUMN_TYPE': 'BLOB',  # we don't know what it is, so let's not do anything with it.\n                        'COLUMN_KEY': '',\n                    }\n                col = Column(column_type, column_schema, from_packet)\n                self.columns.append(col)\n\n        self.table_obj = Table(self.column_schemas, self.table_id, self.schema,\n                               self.table, self.columns)\n\n        # ith column is nullable if (i - 1)th bit is set to True, not nullable otherwise\n        ## Refer to definition of and call to row.event._is_null() to interpret bitmap corresponding to columns\n        self.null_bitmask = self.packet.read((self.column_count + 7) / 8)\n```\n\nCongratulations! You have known how to divide your binlog stream reader to different database(schema) / table.\n\n![RowEvents](./mysql-cdc/row_events.png)\n\nFor TableMapEvent, we can store the table_id for the RowEvents. When any RowEvents parse the table_id and it is not in the table map cache, the event should be ignore.\n\n## Reference\n\n1. https://arxiv.org/pdf/2010.12597.pdf\n2. https://github.com/blackbeam/mysql_async\n3. https://github.com/neverchanje/chgcap-rs\n","slug":"mysql-cdc","published":1,"updated":"2024-04-07T06:53:00.908Z","_id":"clqfx3ahg0001b7c3hbtlfxzp","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"CDC-Process\"><a href=\"#CDC-Process\" class=\"headerlink\" title=\"CDC Process\"></a>CDC Process</h2><p>A common binlog stream reader contains the parts as follows. However, in many open source library or project, some important steps are boxed as internal processes. Frequently, user should only concern about the connection to the server and how to handle the BinlogEvents. I will use <a href=\"https://github.com/rusuly/mysql_cdc\">rust-mysql-cdc</a> to introduce the whole process of mysql cdc.</p>\n<h3 id=\"Connect-to-MySQL-Server-amp-Authentication\"><a href=\"#Connect-to-MySQL-Server-amp-Authentication\" class=\"headerlink\" title=\"Connect to MySQL Server &amp; Authentication\"></a>Connect to MySQL Server &amp; Authentication</h3><figure class=\"highlight rust\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">pub</span> <span class=\"keyword\">fn</span> <span class=\"title function_\">connect</span>(&amp;<span class=\"keyword\">self</span>) <span class=\"punctuation\">-&gt;</span> <span class=\"type\">Result</span>&lt;(PacketChannel, DatabaseProvider), Error&gt; &#123;</span><br><span class=\"line\">        <span class=\"keyword\">let</span> <span class=\"keyword\">mut </span><span class=\"variable\">channel</span> = PacketChannel::<span class=\"title function_ invoke__\">new</span>(&amp;<span class=\"keyword\">self</span>.options)?;</span><br><span class=\"line\">        <span class=\"keyword\">let</span> (packet, seq_num) = channel.<span class=\"title function_ invoke__\">read_packet</span>()?;</span><br><span class=\"line\">        <span class=\"title function_ invoke__\">check_error_packet</span>(&amp;packet, <span class=\"string\">&quot;Initial handshake error.&quot;</span>)?;</span><br><span class=\"line\">        <span class=\"keyword\">let</span> <span class=\"variable\">handshake</span> = HandshakePacket::<span class=\"title function_ invoke__\">parse</span>(&amp;packet)?;</span><br><span class=\"line\">        <span class=\"keyword\">let</span> <span class=\"variable\">auth_plugin</span> = <span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">get_auth_plugin</span>(&amp;handshake.auth_plugin_name)?;</span><br><span class=\"line\">        <span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">authenticate</span>(&amp;<span class=\"keyword\">mut</span> channel, &amp;handshake, auth_plugin, seq_num + <span class=\"number\">1</span>)?;</span><br><span class=\"line\">        <span class=\"title function_ invoke__\">Ok</span>((channel, DatabaseProvider::<span class=\"title function_ invoke__\">from</span>(&amp;handshake.server_version)))</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<ol>\n<li>User provides the connection info and get one PacketChannel(TCPStream).</li>\n<li>Get authentication method and authenticate</li>\n</ol>\n<h3 id=\"Register-as-a-slave\"><a href=\"#Register-as-a-slave\" class=\"headerlink\" title=\"Register as a slave\"></a>Register as a slave</h3><p><a href=\"https://mariadb.com/kb/en/com_register_slave/\">RegisterSlave</a></p>\n<p>In Rust, we need to construct a message and send to the PacketChannel as above which includes</p>\n<ul>\n<li>COM_REGISTER_SLAVE command</li>\n<li>server_id</li>\n<li><strong>Empty</strong> host user password port rank masterid</li>\n</ul>\n<figure class=\"highlight rust\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">impl</span> <span class=\"title class_\">RegisterSlaveCommand</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">pub</span> <span class=\"keyword\">fn</span> <span class=\"title function_\">new</span>(server_id: <span class=\"type\">u32</span>) <span class=\"punctuation\">-&gt;</span> <span class=\"keyword\">Self</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">Self</span> &#123; server_id &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">pub</span> <span class=\"keyword\">fn</span> <span class=\"title function_\">serialize</span>(&amp;<span class=\"keyword\">self</span>) <span class=\"punctuation\">-&gt;</span> <span class=\"type\">Result</span>&lt;<span class=\"type\">Vec</span>&lt;<span class=\"type\">u8</span>&gt;, io::Error&gt; &#123;</span><br><span class=\"line\">        <span class=\"keyword\">let</span> <span class=\"keyword\">mut </span><span class=\"variable\">vec</span> = <span class=\"type\">Vec</span>::<span class=\"title function_ invoke__\">new</span>();</span><br><span class=\"line\">        <span class=\"keyword\">let</span> <span class=\"keyword\">mut </span><span class=\"variable\">cursor</span> = Cursor::<span class=\"title function_ invoke__\">new</span>(&amp;<span class=\"keyword\">mut</span> vec);</span><br><span class=\"line\"></span><br><span class=\"line\">        cursor.<span class=\"title function_ invoke__\">write_u8</span>(CommandType::RegisterSlave <span class=\"keyword\">as</span> <span class=\"type\">u8</span>)?;</span><br><span class=\"line\">        cursor.write_u32::&lt;LittleEndian&gt;(<span class=\"keyword\">self</span>.server_id)?;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//Empty host, user, password, port, rank, masterid</span></span><br><span class=\"line\">        cursor.<span class=\"title function_ invoke__\">write_u8</span>(<span class=\"number\">0</span>)?;</span><br><span class=\"line\">        cursor.<span class=\"title function_ invoke__\">write_u8</span>(<span class=\"number\">0</span>)?;</span><br><span class=\"line\">        cursor.<span class=\"title function_ invoke__\">write_u8</span>(<span class=\"number\">0</span>)?;</span><br><span class=\"line\">        cursor.write_u16::&lt;LittleEndian&gt;(<span class=\"number\">0</span>)?;</span><br><span class=\"line\">        cursor.write_u32::&lt;LittleEndian&gt;(<span class=\"number\">0</span>)?;</span><br><span class=\"line\">        cursor.write_u32::&lt;LittleEndian&gt;(<span class=\"number\">0</span>)?;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"title function_ invoke__\">Ok</span>(vec)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"Binlog-Dump\"><a href=\"#Binlog-Dump\" class=\"headerlink\" title=\"Binlog Dump\"></a>Binlog Dump</h3><p>Same as RegisterSlave, DumpBinogCommand needs:</p>\n<ul>\n<li>server_id</li>\n<li>binlog_filename</li>\n<li>binlog_position</li>\n</ul>\n<figure class=\"highlight rust\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">pub</span> <span class=\"keyword\">struct</span> <span class=\"title class_\">DumpBinlogCommand</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">pub</span> server_id: <span class=\"type\">u32</span>,</span><br><span class=\"line\">    <span class=\"keyword\">pub</span> binlog_filename: <span class=\"type\">String</span>,</span><br><span class=\"line\">    <span class=\"keyword\">pub</span> binlog_position: <span class=\"type\">u32</span>,</span><br><span class=\"line\">    <span class=\"keyword\">pub</span> flags: <span class=\"type\">u16</span>,</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">impl</span> <span class=\"title class_\">DumpBinlogCommand</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">pub</span> <span class=\"keyword\">fn</span> <span class=\"title function_\">new</span>(server_id: <span class=\"type\">u32</span>, binlog_filename: <span class=\"type\">String</span>, binlog_position: <span class=\"type\">u32</span>) <span class=\"punctuation\">-&gt;</span> <span class=\"keyword\">Self</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">Self</span> &#123;</span><br><span class=\"line\">            server_id,</span><br><span class=\"line\">            binlog_filename,</span><br><span class=\"line\">            binlog_position,</span><br><span class=\"line\">            flags: <span class=\"number\">0</span>,</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">pub</span> <span class=\"keyword\">fn</span> <span class=\"title function_\">serialize</span>(&amp;<span class=\"keyword\">self</span>) <span class=\"punctuation\">-&gt;</span> <span class=\"type\">Result</span>&lt;<span class=\"type\">Vec</span>&lt;<span class=\"type\">u8</span>&gt;, io::Error&gt; &#123;</span><br><span class=\"line\">        <span class=\"keyword\">let</span> <span class=\"keyword\">mut </span><span class=\"variable\">vec</span> = <span class=\"type\">Vec</span>::<span class=\"title function_ invoke__\">new</span>();</span><br><span class=\"line\">        <span class=\"keyword\">let</span> <span class=\"keyword\">mut </span><span class=\"variable\">cursor</span> = Cursor::<span class=\"title function_ invoke__\">new</span>(&amp;<span class=\"keyword\">mut</span> vec);</span><br><span class=\"line\"></span><br><span class=\"line\">        cursor.<span class=\"title function_ invoke__\">write_u8</span>(CommandType::BinlogDump <span class=\"keyword\">as</span> <span class=\"type\">u8</span>)?;</span><br><span class=\"line\">        cursor.write_u32::&lt;LittleEndian&gt;(<span class=\"keyword\">self</span>.binlog_position)?;</span><br><span class=\"line\">        cursor.write_u16::&lt;LittleEndian&gt;(<span class=\"keyword\">self</span>.flags)?;</span><br><span class=\"line\">        cursor.write_u32::&lt;LittleEndian&gt;(<span class=\"keyword\">self</span>.server_id)?;</span><br><span class=\"line\">        cursor.<span class=\"title function_ invoke__\">write</span>(<span class=\"keyword\">self</span>.binlog_filename.<span class=\"title function_ invoke__\">as_bytes</span>())?;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"title function_ invoke__\">Ok</span>(vec)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"Handle-Binlog-Events\"><a href=\"#Handle-Binlog-Events\" class=\"headerlink\" title=\"Handle Binlog Events\"></a>Handle Binlog Events</h3><p>As mentioned above, usually, we let users handle these events but firstly, we need to parse them.</p>\n<h3 id=\"HeartBeats\"><a href=\"#HeartBeats\" class=\"headerlink\" title=\"HeartBeats\"></a>HeartBeats</h3><figure class=\"highlight rust\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">pub</span> <span class=\"keyword\">fn</span> <span class=\"title function_\">set_master_heartbeat</span>(&amp;<span class=\"keyword\">mut</span> <span class=\"keyword\">self</span>, channel: &amp;<span class=\"keyword\">mut</span> PacketChannel) <span class=\"punctuation\">-&gt;</span> <span class=\"type\">Result</span>&lt;(), Error&gt; &#123;</span><br><span class=\"line\">    <span class=\"keyword\">let</span> <span class=\"variable\">milliseconds</span> = <span class=\"keyword\">self</span>.options.heartbeat_interval.<span class=\"title function_ invoke__\">as_millis</span>();</span><br><span class=\"line\">    <span class=\"keyword\">let</span> <span class=\"variable\">nanoseconds</span> = milliseconds * <span class=\"number\">1000</span> * <span class=\"number\">1000</span>;</span><br><span class=\"line\">    <span class=\"keyword\">let</span> <span class=\"variable\">query</span> = <span class=\"built_in\">format!</span>(<span class=\"string\">&quot;set @master_heartbeat_period=&#123;&#125;&quot;</span>, nanoseconds);</span><br><span class=\"line\">    <span class=\"keyword\">let</span> <span class=\"variable\">command</span> = QueryCommand::<span class=\"title function_ invoke__\">new</span>(query.<span class=\"title function_ invoke__\">to_string</span>());</span><br><span class=\"line\">    channel.<span class=\"title function_ invoke__\">write_packet</span>(&amp;command.<span class=\"title function_ invoke__\">serialize</span>()?, <span class=\"number\">0</span>)?;</span><br><span class=\"line\">    <span class=\"keyword\">let</span> (packet, _) = channel.<span class=\"title function_ invoke__\">read_packet</span>()?;</span><br><span class=\"line\">    <span class=\"title function_ invoke__\">check_error_packet</span>(&amp;packet, <span class=\"string\">&quot;Setting master heartbeat error.&quot;</span>)?;</span><br><span class=\"line\">    <span class=\"title function_ invoke__\">Ok</span>(())</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"Get-Better-Performance\"><a href=\"#Get-Better-Performance\" class=\"headerlink\" title=\"Get Better Performance\"></a>Get Better Performance</h3><p>As we know, it is impossible for a CDC program to pull and parse binlog events infinitely as a slave. Also, as a developer, you may be not able to adjust the MySQL server parameters to improve the CDC performance. There are some suggestions for you if you are trouble with any relationed issues.</p>\n<h4 id=\"Batch-Processing\"><a href=\"#Batch-Processing\" class=\"headerlink\" title=\"Batch-Processing\"></a>Batch-Processing</h4><p>For the only slave, you can use batch processing anywhere except pulling binlog events like <strong>parse events</strong>, <strong>handle events</strong> or <strong>sink the events.</strong> There is an example that I used in parsing binlog events.</p>\n<ol>\n<li>Parser for parsing a single binlog events</li>\n</ol>\n<figure class=\"highlight rust\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">pub</span> <span class=\"keyword\">fn</span> <span class=\"title function_\">read_data</span>(&amp;<span class=\"keyword\">self</span>) <span class=\"punctuation\">-&gt;</span> io::<span class=\"type\">Result</span>&lt;<span class=\"type\">Option</span>&lt;EventData&lt;<span class=\"symbol\">&#x27;_</span>&gt;&gt;&gt; &#123;</span><br><span class=\"line\">        <span class=\"keyword\">use</span> EventType::*;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">let</span> <span class=\"variable\">event_type</span> = <span class=\"keyword\">match</span> <span class=\"keyword\">self</span>.header.event_type.<span class=\"title function_ invoke__\">get</span>() &#123;</span><br><span class=\"line\">            <span class=\"title function_ invoke__\">Ok</span>(event_type) =&gt; event_type,</span><br><span class=\"line\">            _ =&gt; <span class=\"keyword\">return</span> <span class=\"title function_ invoke__\">Ok</span>(<span class=\"literal\">None</span>),</span><br><span class=\"line\">        &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">let</span> <span class=\"variable\">event_data</span> = <span class=\"keyword\">match</span> event_type &#123;</span><br><span class=\"line\">            ENUM_END_EVENT | UNKNOWN_EVENT =&gt; EventData::UnknownEvent,</span><br><span class=\"line\">            START_EVENT_V3 =&gt; EventData::<span class=\"title function_ invoke__\">StartEventV3</span>(Cow::<span class=\"title function_ invoke__\">Borrowed</span>(&amp;*<span class=\"keyword\">self</span>.data)),</span><br><span class=\"line\">            QUERY_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">QueryEvent</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?),</span><br><span class=\"line\">            STOP_EVENT =&gt; EventData::StopEvent,</span><br><span class=\"line\">            ROTATE_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">RotateEvent</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?),</span><br><span class=\"line\">            INTVAR_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">IntvarEvent</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?),</span><br><span class=\"line\">            LOAD_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">LoadEvent</span>(Cow::<span class=\"title function_ invoke__\">Borrowed</span>(&amp;*<span class=\"keyword\">self</span>.data)),</span><br><span class=\"line\">            SLAVE_EVENT =&gt; EventData::SlaveEvent,</span><br><span class=\"line\">            CREATE_FILE_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">CreateFileEvent</span>(Cow::<span class=\"title function_ invoke__\">Borrowed</span>(&amp;*<span class=\"keyword\">self</span>.data)),</span><br><span class=\"line\">            APPEND_BLOCK_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">AppendBlockEvent</span>(Cow::<span class=\"title function_ invoke__\">Borrowed</span>(&amp;*<span class=\"keyword\">self</span>.data)),</span><br><span class=\"line\">            EXEC_LOAD_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">ExecLoadEvent</span>(Cow::<span class=\"title function_ invoke__\">Borrowed</span>(&amp;*<span class=\"keyword\">self</span>.data)),</span><br><span class=\"line\">            DELETE_FILE_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">DeleteFileEvent</span>(Cow::<span class=\"title function_ invoke__\">Borrowed</span>(&amp;*<span class=\"keyword\">self</span>.data)),</span><br><span class=\"line\">            NEW_LOAD_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">NewLoadEvent</span>(Cow::<span class=\"title function_ invoke__\">Borrowed</span>(&amp;*<span class=\"keyword\">self</span>.data)),</span><br><span class=\"line\">            RAND_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">RandEvent</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?),</span><br><span class=\"line\">            USER_VAR_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">UserVarEvent</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?),</span><br><span class=\"line\">            FORMAT_DESCRIPTION_EVENT =&gt; &#123;</span><br><span class=\"line\">                <span class=\"keyword\">let</span> <span class=\"variable\">fde</span> = <span class=\"keyword\">self</span></span><br><span class=\"line\">                    .read_event::&lt;FormatDescriptionEvent&gt;()?</span><br><span class=\"line\">                    .<span class=\"title function_ invoke__\">with_footer</span>(<span class=\"keyword\">self</span>.footer);</span><br><span class=\"line\">                EventData::<span class=\"title function_ invoke__\">FormatDescriptionEvent</span>(fde)</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            XID_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">XidEvent</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?),</span><br><span class=\"line\">            BEGIN_LOAD_QUERY_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">BeginLoadQueryEvent</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?),</span><br><span class=\"line\">            EXECUTE_LOAD_QUERY_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">ExecuteLoadQueryEvent</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?),</span><br><span class=\"line\">            TABLE_MAP_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">TableMapEvent</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?),</span><br><span class=\"line\">            PRE_GA_WRITE_ROWS_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">PreGaWriteRowsEvent</span>(Cow::<span class=\"title function_ invoke__\">Borrowed</span>(&amp;*<span class=\"keyword\">self</span>.data)),</span><br><span class=\"line\">            PRE_GA_UPDATE_ROWS_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">PreGaUpdateRowsEvent</span>(Cow::<span class=\"title function_ invoke__\">Borrowed</span>(&amp;*<span class=\"keyword\">self</span>.data)),</span><br><span class=\"line\">            PRE_GA_DELETE_ROWS_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">PreGaDeleteRowsEvent</span>(Cow::<span class=\"title function_ invoke__\">Borrowed</span>(&amp;*<span class=\"keyword\">self</span>.data)),</span><br><span class=\"line\">            WRITE_ROWS_EVENT_V1 =&gt; &#123;</span><br><span class=\"line\">                EventData::<span class=\"title function_ invoke__\">RowsEvent</span>(RowsEventData::<span class=\"title function_ invoke__\">WriteRowsEventV1</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?))</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            UPDATE_ROWS_EVENT_V1 =&gt; &#123;</span><br><span class=\"line\">                EventData::<span class=\"title function_ invoke__\">RowsEvent</span>(RowsEventData::<span class=\"title function_ invoke__\">UpdateRowsEventV1</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?))</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            DELETE_ROWS_EVENT_V1 =&gt; &#123;</span><br><span class=\"line\">                EventData::<span class=\"title function_ invoke__\">RowsEvent</span>(RowsEventData::<span class=\"title function_ invoke__\">DeleteRowsEventV1</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?))</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            INCIDENT_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">IncidentEvent</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?),</span><br><span class=\"line\">            HEARTBEAT_EVENT =&gt; EventData::HeartbeatEvent,</span><br><span class=\"line\">            IGNORABLE_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">IgnorableEvent</span>(Cow::<span class=\"title function_ invoke__\">Borrowed</span>(&amp;*<span class=\"keyword\">self</span>.data)),</span><br><span class=\"line\">            ROWS_QUERY_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">RowsQueryEvent</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?),</span><br><span class=\"line\">            WRITE_ROWS_EVENT =&gt; &#123;</span><br><span class=\"line\">                EventData::<span class=\"title function_ invoke__\">RowsEvent</span>(RowsEventData::<span class=\"title function_ invoke__\">WriteRowsEvent</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?))</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            UPDATE_ROWS_EVENT =&gt; &#123;</span><br><span class=\"line\">                EventData::<span class=\"title function_ invoke__\">RowsEvent</span>(RowsEventData::<span class=\"title function_ invoke__\">UpdateRowsEvent</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?))</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            DELETE_ROWS_EVENT =&gt; &#123;</span><br><span class=\"line\">                EventData::<span class=\"title function_ invoke__\">RowsEvent</span>(RowsEventData::<span class=\"title function_ invoke__\">DeleteRowsEvent</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?))</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            GTID_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">GtidEvent</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?),</span><br><span class=\"line\">            ANONYMOUS_GTID_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">AnonymousGtidEvent</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?),</span><br><span class=\"line\">            PREVIOUS_GTIDS_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">PreviousGtidsEvent</span>(Cow::<span class=\"title function_ invoke__\">Borrowed</span>(&amp;*<span class=\"keyword\">self</span>.data)),</span><br><span class=\"line\">            TRANSACTION_CONTEXT_EVENT =&gt; &#123;</span><br><span class=\"line\">                EventData::<span class=\"title function_ invoke__\">TransactionContextEvent</span>(Cow::<span class=\"title function_ invoke__\">Borrowed</span>(&amp;*<span class=\"keyword\">self</span>.data))</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            VIEW_CHANGE_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">ViewChangeEvent</span>(Cow::<span class=\"title function_ invoke__\">Borrowed</span>(&amp;*<span class=\"keyword\">self</span>.data)),</span><br><span class=\"line\">            XA_PREPARE_LOG_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">XaPrepareLogEvent</span>(Cow::<span class=\"title function_ invoke__\">Borrowed</span>(&amp;*<span class=\"keyword\">self</span>.data)),</span><br><span class=\"line\">            PARTIAL_UPDATE_ROWS_EVENT =&gt; &#123;</span><br><span class=\"line\">                EventData::<span class=\"title function_ invoke__\">RowsEvent</span>(RowsEventData::<span class=\"title function_ invoke__\">PartialUpdateRowsEvent</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?))</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            TRANSACTION_PAYLOAD_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">TransactionPayloadEvent</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?),</span><br><span class=\"line\">        &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"title function_ invoke__\">Ok</span>(<span class=\"title function_ invoke__\">Some</span>(event_data))</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<ol>\n<li>Use the Rayon framework to enable concurrency</li>\n</ol>\n<p>Many operations in Rayon, such as map(), filter_map(), and others, do not inherently guarantee that the output order will be the same as the input order. However, when used in combination with collect(), they generally ensure the results are in order. So, if the order of data is needed, please write a test case to test your code.</p>\n<figure class=\"highlight rust\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> <span class=\"variable\">result</span> <span class=\"keyword\">in</span> client.<span class=\"title function_ invoke__\">replicate</span>()? &#123;</span><br><span class=\"line\">        <span class=\"keyword\">let</span> (header, event) = result?;</span><br><span class=\"line\">        event_queue.<span class=\"title function_ invoke__\">enqueue</span>((header, event));</span><br><span class=\"line\">        event_count += <span class=\"number\">1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> event_count.<span class=\"title function_ invoke__\">borrow</span>() % <span class=\"number\">100000</span> == <span class=\"number\">0</span> &#123;</span><br><span class=\"line\">            <span class=\"keyword\">let</span> <span class=\"keyword\">mut </span><span class=\"variable\">event_array</span> = event_queue.<span class=\"title function_ invoke__\">drain_all</span>();</span><br><span class=\"line\">            <span class=\"keyword\">let</span> <span class=\"variable\">new_arr</span>: <span class=\"type\">Vec</span>&lt;(EventHeader, <span class=\"type\">Result</span>&lt;BinlogEvent, Error&gt;)&gt; = event_array.<span class=\"title function_ invoke__\">into_par_iter</span>().<span class=\"title function_ invoke__\">map</span>(|(header, payload)| &#123;</span><br><span class=\"line\">                <span class=\"keyword\">let</span> <span class=\"keyword\">mut </span><span class=\"variable\">c_table_map</span> = table_map.<span class=\"title function_ invoke__\">clone</span>();</span><br><span class=\"line\">                <span class=\"keyword\">let</span> <span class=\"variable\">parsed_event</span> = <span class=\"title function_ invoke__\">parse_event</span>(&amp;header, &amp;payload, &amp;<span class=\"keyword\">mut</span> c_table_map);</span><br><span class=\"line\">                (header, parsed_event)</span><br><span class=\"line\">            &#125;).<span class=\"title function_ invoke__\">collect</span>();</span><br><span class=\"line\">            <span class=\"keyword\">let</span> <span class=\"variable\">dur</span> = start_time.<span class=\"title function_ invoke__\">elapsed</span>();</span><br><span class=\"line\">            <span class=\"built_in\">println!</span>(<span class=\"string\">&quot;Process Time Duration: &#123;:?&#125;&quot;</span>, dur);</span><br><span class=\"line\">            <span class=\"built_in\">println!</span>(<span class=\"string\">&quot;Current Binlog Events: &#123;:?&#125;&quot;</span>, event_count);</span><br><span class=\"line\">            <span class=\"built_in\">println!</span>(<span class=\"string\">&quot;Result Length: &#123;:?&#125;&quot;</span>, new_arr.<span class=\"title function_ invoke__\">len</span>());</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<ol>\n<li>Choose the needed events</li>\n</ol>\n<p>Normally, in CDC program, several binlog event types are as follows:</p>\n<figure class=\"highlight rust\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#[allow(non_camel_case_types)]</span></span><br><span class=\"line\"><span class=\"meta\">#[repr(u8)]</span></span><br><span class=\"line\"><span class=\"meta\">#[derive(Debug, Clone, Copy, Eq, PartialEq, Hash)]</span></span><br><span class=\"line\"><span class=\"keyword\">pub</span> <span class=\"keyword\">enum</span> <span class=\"title class_\">EventType</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/// A `QUERY_EVENT` is created for each query that modifies the database,</span></span><br><span class=\"line\">    <span class=\"comment\">/// unless the query is logged row-based.</span></span><br><span class=\"line\">    QUERY_EVENT = <span class=\"number\">0x02</span>,</span><br><span class=\"line\">    <span class=\"comment\">/// to tell the reader what binlog to request next.</span></span><br><span class=\"line\">    ROTATE_EVENT = <span class=\"number\">0x04</span>,</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/// if any transaction commit info needed</span></span><br><span class=\"line\">    XID_EVENT = <span class=\"number\">0x10</span>,</span><br><span class=\"line\">  </span><br><span class=\"line\">    TABLE_MAP_EVENT = <span class=\"number\">0x13</span>,</span><br><span class=\"line\">   </span><br><span class=\"line\">    WRITE_ROWS_EVENT_V1 = <span class=\"number\">0x17</span>,</span><br><span class=\"line\">    UPDATE_ROWS_EVENT_V1 = <span class=\"number\">0x18</span>,</span><br><span class=\"line\">    DELETE_ROWS_EVENT_V1 = <span class=\"number\">0x19</span>,</span><br><span class=\"line\">  </span><br><span class=\"line\">    WRITE_ROWS_EVENT = <span class=\"number\">0x1e</span>,</span><br><span class=\"line\">    UPDATE_ROWS_EVENT = <span class=\"number\">0x1f</span>,</span><br><span class=\"line\">    DELETE_ROWS_EVENT = <span class=\"number\">0x20</span>,</span><br><span class=\"line\">    GTID_EVENT = <span class=\"number\">0x21</span>,</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ol>\n<li>Add iterator for your binlog stream reader</li>\n</ol>\n<p>In my code, <code>client.replicate()</code> will return <code>Result&lt;BinlogEvents, Error&gt;</code>. So we need to implement Iterator trait for BinlogEvents.</p>\n<figure class=\"highlight rust\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">impl</span> <span class=\"title class_\">Iterator</span> <span class=\"keyword\">for</span> <span class=\"title class_\">BinlogEvents</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">type</span> <span class=\"title class_\">Item</span> = <span class=\"type\">Result</span>&lt;(EventHeader, <span class=\"type\">Vec</span>&lt;<span class=\"type\">u8</span>&gt;), Error&gt;;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/// Reads binlog event packets from network stream.</span></span><br><span class=\"line\">    <span class=\"comment\">/// &lt;a href=&quot;https://mariadb.com/kb/en/3-binlog-network-stream/&quot;&gt;See more&lt;/a&gt;</span></span><br><span class=\"line\">    <span class=\"keyword\">fn</span> <span class=\"title function_\">next</span>(&amp;<span class=\"keyword\">mut</span> <span class=\"keyword\">self</span>) <span class=\"punctuation\">-&gt;</span> <span class=\"type\">Option</span>&lt;<span class=\"keyword\">Self</span>::Item&gt; &#123;</span><br><span class=\"line\">        <span class=\"keyword\">let</span> (packet, _) = <span class=\"keyword\">match</span> <span class=\"keyword\">self</span>.channel.<span class=\"title function_ invoke__\">read_packet</span>() &#123;</span><br><span class=\"line\">            <span class=\"title function_ invoke__\">Ok</span>(x) =&gt; x,</span><br><span class=\"line\">            <span class=\"title function_ invoke__\">Err</span>(e) =&gt; <span class=\"keyword\">return</span> <span class=\"title function_ invoke__\">Some</span>(<span class=\"title function_ invoke__\">Err</span>(Error::<span class=\"title function_ invoke__\">IoError</span>(e))),</span><br><span class=\"line\">        &#125;;</span><br><span class=\"line\">        <span class=\"keyword\">match</span> packet[<span class=\"number\">0</span>] &#123;</span><br><span class=\"line\">            ResponseType::OK =&gt; <span class=\"title function_ invoke__\">Some</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>(&amp;packet)),</span><br><span class=\"line\">            ResponseType::ERROR =&gt; <span class=\"title function_ invoke__\">Some</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_error</span>(&amp;packet)),</span><br><span class=\"line\">            ResponseType::END_OF_FILE =&gt; &#123;</span><br><span class=\"line\">                <span class=\"keyword\">let</span> <span class=\"variable\">_</span> = EndOfFilePacket::<span class=\"title function_ invoke__\">parse</span>(&amp;packet[<span class=\"number\">1</span>..]);</span><br><span class=\"line\">                <span class=\"literal\">None</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            _ =&gt; <span class=\"title function_ invoke__\">Some</span>(<span class=\"title function_ invoke__\">Err</span>(Error::<span class=\"title function_ invoke__\">String</span>(</span><br><span class=\"line\">                <span class=\"string\">&quot;Unknown network stream status&quot;</span>.<span class=\"title function_ invoke__\">to_string</span>(),</span><br><span class=\"line\">            ))),</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>In another case, the binlog stream reader implements the Stream trait like <code>while let Ok(Some(c)) = cdc_stream.try_next().await</code>.</p>\n<figure class=\"highlight rust\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">impl</span> <span class=\"title class_\">futures_core</span>::stream::Stream <span class=\"keyword\">for</span> <span class=\"title class_\">BinlogStream</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">type</span> <span class=\"title class_\">Item</span> = <span class=\"type\">Result</span>&lt;ChgcapEvent&gt;;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">fn</span> <span class=\"title function_\">poll_next</span>(</span><br><span class=\"line\">        <span class=\"keyword\">self</span>: Pin&lt;&amp;<span class=\"keyword\">mut</span> <span class=\"keyword\">Self</span>&gt;,</span><br><span class=\"line\">        cx: &amp;<span class=\"keyword\">mut</span> std::task::Context&lt;<span class=\"symbol\">&#x27;_</span>&gt;,</span><br><span class=\"line\">    ) <span class=\"punctuation\">-&gt;</span> Poll&lt;<span class=\"type\">Option</span>&lt;<span class=\"keyword\">Self</span>::Item&gt;&gt; &#123;</span><br><span class=\"line\">        <span class=\"keyword\">let</span> <span class=\"variable\">this</span> = <span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">get_mut</span>();</span><br><span class=\"line\">        <span class=\"comment\">// <span class=\"doctag\">TODO:</span> Support rate limiting.</span></span><br><span class=\"line\">        <span class=\"keyword\">loop</span> &#123;</span><br><span class=\"line\">            <span class=\"keyword\">let</span> <span class=\"variable\">binlog_stream</span> = Pin::<span class=\"title function_ invoke__\">new</span>(&amp;<span class=\"keyword\">mut</span> this.binlog_stream);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">match</span> binlog_stream.<span class=\"title function_ invoke__\">poll_next</span>(cx) &#123;</span><br><span class=\"line\">                Poll::<span class=\"title function_ invoke__\">Ready</span>(t) =&gt; <span class=\"keyword\">match</span> t &#123;</span><br><span class=\"line\">                    <span class=\"title function_ invoke__\">Some</span>(event_result) =&gt; <span class=\"keyword\">match</span> event_result &#123;</span><br><span class=\"line\">                        <span class=\"title function_ invoke__\">Ok</span>(event) =&gt; <span class=\"keyword\">match</span> this.<span class=\"title function_ invoke__\">handle_event</span>(event) &#123; <span class=\"comment\">// here</span></span><br><span class=\"line\">                            <span class=\"title function_ invoke__\">Ok</span>(change) =&gt; <span class=\"keyword\">match</span> change &#123;</span><br><span class=\"line\">                                <span class=\"title function_ invoke__\">Some</span>(c) =&gt; Poll::<span class=\"title function_ invoke__\">Ready</span>(<span class=\"title function_ invoke__\">Some</span>(<span class=\"title function_ invoke__\">Ok</span>(c))),</span><br><span class=\"line\">                                <span class=\"literal\">None</span> =&gt; <span class=\"keyword\">continue</span>, <span class=\"comment\">// Skip this event.</span></span><br><span class=\"line\">                            &#125;,</span><br><span class=\"line\">                            <span class=\"title function_ invoke__\">Err</span>(e) =&gt; Poll::<span class=\"title function_ invoke__\">Ready</span>(<span class=\"title function_ invoke__\">Some</span>(<span class=\"title function_ invoke__\">Err</span>(e))),</span><br><span class=\"line\">                        &#125;,</span><br><span class=\"line\">                        <span class=\"title function_ invoke__\">Err</span>(err) =&gt; Poll::<span class=\"title function_ invoke__\">Ready</span>(<span class=\"title function_ invoke__\">Some</span>(<span class=\"title function_ invoke__\">Err</span>(anyhow!(err)))),</span><br><span class=\"line\">                    &#125;,</span><br><span class=\"line\">                    <span class=\"literal\">None</span> =&gt; Poll::<span class=\"title function_ invoke__\">Ready</span>(<span class=\"literal\">None</span>), <span class=\"comment\">// Completed.</span></span><br><span class=\"line\">                &#125;,</span><br><span class=\"line\">                Poll::Pending =&gt; Poll::Pending,</span><br><span class=\"line\">            &#125;;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<ol>\n<li>Accumulate unprocessed binlog events into a batch like Step 2</li>\n</ol>\n<h4 id=\"Multiple-Slave\"><a href=\"#Multiple-Slave\" class=\"headerlink\" title=\"Multiple Slave\"></a>Multiple Slave</h4><p>In <strong>Batch-Processing</strong> section, we can boost the performance of CDC from tens of thousands of events to over a hundred thousand events per second. But however, if you need higher performance, single-instance batching may not meet your needs.</p>\n<p>Luckily, there is an another road to make it. Before introducing this way, let us take eyes on <strong>table_id</strong>.</p>\n<ul>\n<li>table_id is 6 bytes, need pad little-endian number.</li>\n<li>table_id is a temporary value. it would be changed when:<ul>\n<li>drop and replace table</li>\n<li>rename table</li>\n<li>alter table</li>\n<li>MySQL Engine upgrade</li>\n</ul>\n</li>\n<li>Before every data change events(WriteRowsEvent, UpdateRowsEvent, DeleteRowsEvent), there would be a TableMapEvent including table name, table id and table schema.</li>\n</ul>\n<p>Next, you should know how to get table metadata from TableMapEvent.</p>\n<p><img src=\"/2023/12/21/mysql-cdc/table_map_event.png\" alt=\"TableMapEvent\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">TableMapEvent</span>(<span class=\"title class_ inherited__\">BinLogEvent</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;This event describes the structure of a table.</span></span><br><span class=\"line\"><span class=\"string\">    It&#x27;s sent before a change happens on a table.</span></span><br><span class=\"line\"><span class=\"string\">    An end user of the lib should have no usage of this</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, from_packet, event_size, table_map, ctl_connection, **kwargs</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(TableMapEvent, self).__init__(from_packet, event_size,</span><br><span class=\"line\">                                            table_map, ctl_connection, **kwargs)</span><br><span class=\"line\">        self.__only_tables = kwargs[<span class=\"string\">&quot;only_tables&quot;</span>]</span><br><span class=\"line\">        self.__ignored_tables = kwargs[<span class=\"string\">&quot;ignored_tables&quot;</span>]</span><br><span class=\"line\">        self.__only_schemas = kwargs[<span class=\"string\">&quot;only_schemas&quot;</span>]</span><br><span class=\"line\">        self.__ignored_schemas = kwargs[<span class=\"string\">&quot;ignored_schemas&quot;</span>]</span><br><span class=\"line\">        self.__freeze_schema = kwargs[<span class=\"string\">&quot;freeze_schema&quot;</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Post-Header</span></span><br><span class=\"line\">        self.table_id = self._read_table_id()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.table_id <span class=\"keyword\">in</span> table_map <span class=\"keyword\">and</span> self.__freeze_schema:</span><br><span class=\"line\">            self._processed = <span class=\"literal\">False</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span></span><br><span class=\"line\"></span><br><span class=\"line\">        self.flags = struct.unpack(<span class=\"string\">&#x27;&lt;H&#x27;</span>, self.packet.read(<span class=\"number\">2</span>))[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Payload</span></span><br><span class=\"line\">        self.schema_length = struct.unpack(<span class=\"string\">&quot;!B&quot;</span>, self.packet.read(<span class=\"number\">1</span>))[<span class=\"number\">0</span>]</span><br><span class=\"line\">        self.schema = self.packet.read(self.schema_length).decode()</span><br><span class=\"line\">        self.packet.advance(<span class=\"number\">1</span>)</span><br><span class=\"line\">        self.table_length = struct.unpack(<span class=\"string\">&quot;!B&quot;</span>, self.packet.read(<span class=\"number\">1</span>))[<span class=\"number\">0</span>]</span><br><span class=\"line\">        self.table = self.packet.read(self.table_length).decode()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.__only_tables <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span> <span class=\"keyword\">and</span> self.table <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> self.__only_tables:</span><br><span class=\"line\">            self._processed = <span class=\"literal\">False</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span></span><br><span class=\"line\">        <span class=\"keyword\">elif</span> self.__ignored_tables <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span> <span class=\"keyword\">and</span> self.table <span class=\"keyword\">in</span> self.__ignored_tables:</span><br><span class=\"line\">            self._processed = <span class=\"literal\">False</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.__only_schemas <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span> <span class=\"keyword\">and</span> self.schema <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> self.__only_schemas:</span><br><span class=\"line\">            self._processed = <span class=\"literal\">False</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span></span><br><span class=\"line\">        <span class=\"keyword\">elif</span> self.__ignored_schemas <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span> <span class=\"keyword\">and</span> self.schema <span class=\"keyword\">in</span> self.__ignored_schemas:</span><br><span class=\"line\">            self._processed = <span class=\"literal\">False</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span></span><br><span class=\"line\"></span><br><span class=\"line\">        self.packet.advance(<span class=\"number\">1</span>)</span><br><span class=\"line\">        self.column_count = self.packet.read_length_coded_binary()</span><br><span class=\"line\"></span><br><span class=\"line\">        self.columns = []</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.table_id <span class=\"keyword\">in</span> table_map:</span><br><span class=\"line\">            self.column_schemas = table_map[self.table_id].column_schemas</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            self.column_schemas = self._ctl_connection._get_table_information(self.schema, self.table)</span><br><span class=\"line\"></span><br><span class=\"line\">        ordinal_pos_loc = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(self.column_schemas) != <span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"comment\"># Read columns meta data</span></span><br><span class=\"line\">            column_types = <span class=\"built_in\">bytearray</span>(self.packet.read(self.column_count))</span><br><span class=\"line\">            self.packet.read_length_coded_binary()</span><br><span class=\"line\">            <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">0</span>, <span class=\"built_in\">len</span>(column_types)):</span><br><span class=\"line\">                column_type = column_types[i]</span><br><span class=\"line\">                <span class=\"keyword\">try</span>:</span><br><span class=\"line\">                    column_schema = self.column_schemas[ordinal_pos_loc]</span><br><span class=\"line\"></span><br><span class=\"line\">                    <span class=\"comment\"># only acknowledge the column definition if the iteration matches with ordinal position of</span></span><br><span class=\"line\">                    <span class=\"comment\"># the column. this helps in maintaining support for restricted columnar access</span></span><br><span class=\"line\">                    <span class=\"keyword\">if</span> i != (column_schema[<span class=\"string\">&#x27;ORDINAL_POSITION&#x27;</span>] - <span class=\"number\">1</span>):</span><br><span class=\"line\">                        <span class=\"comment\"># raise IndexError to follow the workflow of dropping columns which are not matching the</span></span><br><span class=\"line\">                        <span class=\"comment\"># underlying table schema</span></span><br><span class=\"line\">                        <span class=\"keyword\">raise</span> IndexError</span><br><span class=\"line\"></span><br><span class=\"line\">                    ordinal_pos_loc += <span class=\"number\">1</span></span><br><span class=\"line\">                <span class=\"keyword\">except</span> IndexError:</span><br><span class=\"line\">                    <span class=\"comment\"># this a dirty hack to prevent row events containing columns which have been dropped prior</span></span><br><span class=\"line\">                    <span class=\"comment\"># to pymysqlreplication start, but replayed from binlog from blowing up the service.</span></span><br><span class=\"line\">                    <span class=\"comment\"># <span class=\"doctag\">TODO:</span> this does not address the issue if the column other than the last one is dropped</span></span><br><span class=\"line\">                    column_schema = &#123;</span><br><span class=\"line\">                        <span class=\"string\">&#x27;COLUMN_NAME&#x27;</span>: <span class=\"string\">&#x27;__dropped_col_&#123;i&#125;__&#x27;</span>.<span class=\"built_in\">format</span>(i=i),</span><br><span class=\"line\">                        <span class=\"string\">&#x27;COLLATION_NAME&#x27;</span>: <span class=\"literal\">None</span>,</span><br><span class=\"line\">                        <span class=\"string\">&#x27;CHARACTER_SET_NAME&#x27;</span>: <span class=\"literal\">None</span>,</span><br><span class=\"line\">                        <span class=\"string\">&#x27;COLUMN_COMMENT&#x27;</span>: <span class=\"literal\">None</span>,</span><br><span class=\"line\">                        <span class=\"string\">&#x27;COLUMN_TYPE&#x27;</span>: <span class=\"string\">&#x27;BLOB&#x27;</span>,  <span class=\"comment\"># we don&#x27;t know what it is, so let&#x27;s not do anything with it.</span></span><br><span class=\"line\">                        <span class=\"string\">&#x27;COLUMN_KEY&#x27;</span>: <span class=\"string\">&#x27;&#x27;</span>,</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                col = Column(column_type, column_schema, from_packet)</span><br><span class=\"line\">                self.columns.append(col)</span><br><span class=\"line\"></span><br><span class=\"line\">        self.table_obj = Table(self.column_schemas, self.table_id, self.schema,</span><br><span class=\"line\">                               self.table, self.columns)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># ith column is nullable if (i - 1)th bit is set to True, not nullable otherwise</span></span><br><span class=\"line\">        <span class=\"comment\">## Refer to definition of and call to row.event._is_null() to interpret bitmap corresponding to columns</span></span><br><span class=\"line\">        self.null_bitmask = self.packet.read((self.column_count + <span class=\"number\">7</span>) / <span class=\"number\">8</span>)</span><br></pre></td></tr></table></figure>\n<p>Congratulations! You have known how to divide your binlog stream reader to different database(schema) / table.</p>\n<p><img src=\"/2023/12/21/mysql-cdc/row_events.png\" alt=\"RowEvents\"></p>\n<p>For TableMapEvent, we can store the table_id for the RowEvents. When any RowEvents parse the table_id and it is not in the table map cache, the event should be ignore.</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li><a href=\"https://arxiv.org/pdf/2010.12597.pdf\">https://arxiv.org/pdf/2010.12597.pdf</a></li>\n<li><a href=\"https://github.com/blackbeam/mysql_async\">https://github.com/blackbeam/mysql_async</a></li>\n<li><a href=\"https://github.com/neverchanje/chgcap-rs\">https://github.com/neverchanje/chgcap-rs</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"CDC-Process\"><a href=\"#CDC-Process\" class=\"headerlink\" title=\"CDC Process\"></a>CDC Process</h2><p>A common binlog stream reader contains the parts as follows. However, in many open source library or project, some important steps are boxed as internal processes. Frequently, user should only concern about the connection to the server and how to handle the BinlogEvents. I will use <a href=\"https://github.com/rusuly/mysql_cdc\">rust-mysql-cdc</a> to introduce the whole process of mysql cdc.</p>\n<h3 id=\"Connect-to-MySQL-Server-amp-Authentication\"><a href=\"#Connect-to-MySQL-Server-amp-Authentication\" class=\"headerlink\" title=\"Connect to MySQL Server &amp; Authentication\"></a>Connect to MySQL Server &amp; Authentication</h3><figure class=\"highlight rust\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">pub</span> <span class=\"keyword\">fn</span> <span class=\"title function_\">connect</span>(&amp;<span class=\"keyword\">self</span>) <span class=\"punctuation\">-&gt;</span> <span class=\"type\">Result</span>&lt;(PacketChannel, DatabaseProvider), Error&gt; &#123;</span><br><span class=\"line\">        <span class=\"keyword\">let</span> <span class=\"keyword\">mut </span><span class=\"variable\">channel</span> = PacketChannel::<span class=\"title function_ invoke__\">new</span>(&amp;<span class=\"keyword\">self</span>.options)?;</span><br><span class=\"line\">        <span class=\"keyword\">let</span> (packet, seq_num) = channel.<span class=\"title function_ invoke__\">read_packet</span>()?;</span><br><span class=\"line\">        <span class=\"title function_ invoke__\">check_error_packet</span>(&amp;packet, <span class=\"string\">&quot;Initial handshake error.&quot;</span>)?;</span><br><span class=\"line\">        <span class=\"keyword\">let</span> <span class=\"variable\">handshake</span> = HandshakePacket::<span class=\"title function_ invoke__\">parse</span>(&amp;packet)?;</span><br><span class=\"line\">        <span class=\"keyword\">let</span> <span class=\"variable\">auth_plugin</span> = <span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">get_auth_plugin</span>(&amp;handshake.auth_plugin_name)?;</span><br><span class=\"line\">        <span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">authenticate</span>(&amp;<span class=\"keyword\">mut</span> channel, &amp;handshake, auth_plugin, seq_num + <span class=\"number\">1</span>)?;</span><br><span class=\"line\">        <span class=\"title function_ invoke__\">Ok</span>((channel, DatabaseProvider::<span class=\"title function_ invoke__\">from</span>(&amp;handshake.server_version)))</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<ol>\n<li>User provides the connection info and get one PacketChannel(TCPStream).</li>\n<li>Get authentication method and authenticate</li>\n</ol>\n<h3 id=\"Register-as-a-slave\"><a href=\"#Register-as-a-slave\" class=\"headerlink\" title=\"Register as a slave\"></a>Register as a slave</h3><p><a href=\"https://mariadb.com/kb/en/com_register_slave/\">RegisterSlave</a></p>\n<p>In Rust, we need to construct a message and send to the PacketChannel as above which includes</p>\n<ul>\n<li>COM_REGISTER_SLAVE command</li>\n<li>server_id</li>\n<li><strong>Empty</strong> host user password port rank masterid</li>\n</ul>\n<figure class=\"highlight rust\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">impl</span> <span class=\"title class_\">RegisterSlaveCommand</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">pub</span> <span class=\"keyword\">fn</span> <span class=\"title function_\">new</span>(server_id: <span class=\"type\">u32</span>) <span class=\"punctuation\">-&gt;</span> <span class=\"keyword\">Self</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">Self</span> &#123; server_id &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">pub</span> <span class=\"keyword\">fn</span> <span class=\"title function_\">serialize</span>(&amp;<span class=\"keyword\">self</span>) <span class=\"punctuation\">-&gt;</span> <span class=\"type\">Result</span>&lt;<span class=\"type\">Vec</span>&lt;<span class=\"type\">u8</span>&gt;, io::Error&gt; &#123;</span><br><span class=\"line\">        <span class=\"keyword\">let</span> <span class=\"keyword\">mut </span><span class=\"variable\">vec</span> = <span class=\"type\">Vec</span>::<span class=\"title function_ invoke__\">new</span>();</span><br><span class=\"line\">        <span class=\"keyword\">let</span> <span class=\"keyword\">mut </span><span class=\"variable\">cursor</span> = Cursor::<span class=\"title function_ invoke__\">new</span>(&amp;<span class=\"keyword\">mut</span> vec);</span><br><span class=\"line\"></span><br><span class=\"line\">        cursor.<span class=\"title function_ invoke__\">write_u8</span>(CommandType::RegisterSlave <span class=\"keyword\">as</span> <span class=\"type\">u8</span>)?;</span><br><span class=\"line\">        cursor.write_u32::&lt;LittleEndian&gt;(<span class=\"keyword\">self</span>.server_id)?;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">//Empty host, user, password, port, rank, masterid</span></span><br><span class=\"line\">        cursor.<span class=\"title function_ invoke__\">write_u8</span>(<span class=\"number\">0</span>)?;</span><br><span class=\"line\">        cursor.<span class=\"title function_ invoke__\">write_u8</span>(<span class=\"number\">0</span>)?;</span><br><span class=\"line\">        cursor.<span class=\"title function_ invoke__\">write_u8</span>(<span class=\"number\">0</span>)?;</span><br><span class=\"line\">        cursor.write_u16::&lt;LittleEndian&gt;(<span class=\"number\">0</span>)?;</span><br><span class=\"line\">        cursor.write_u32::&lt;LittleEndian&gt;(<span class=\"number\">0</span>)?;</span><br><span class=\"line\">        cursor.write_u32::&lt;LittleEndian&gt;(<span class=\"number\">0</span>)?;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"title function_ invoke__\">Ok</span>(vec)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"Binlog-Dump\"><a href=\"#Binlog-Dump\" class=\"headerlink\" title=\"Binlog Dump\"></a>Binlog Dump</h3><p>Same as RegisterSlave, DumpBinogCommand needs:</p>\n<ul>\n<li>server_id</li>\n<li>binlog_filename</li>\n<li>binlog_position</li>\n</ul>\n<figure class=\"highlight rust\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">pub</span> <span class=\"keyword\">struct</span> <span class=\"title class_\">DumpBinlogCommand</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">pub</span> server_id: <span class=\"type\">u32</span>,</span><br><span class=\"line\">    <span class=\"keyword\">pub</span> binlog_filename: <span class=\"type\">String</span>,</span><br><span class=\"line\">    <span class=\"keyword\">pub</span> binlog_position: <span class=\"type\">u32</span>,</span><br><span class=\"line\">    <span class=\"keyword\">pub</span> flags: <span class=\"type\">u16</span>,</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">impl</span> <span class=\"title class_\">DumpBinlogCommand</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">pub</span> <span class=\"keyword\">fn</span> <span class=\"title function_\">new</span>(server_id: <span class=\"type\">u32</span>, binlog_filename: <span class=\"type\">String</span>, binlog_position: <span class=\"type\">u32</span>) <span class=\"punctuation\">-&gt;</span> <span class=\"keyword\">Self</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">Self</span> &#123;</span><br><span class=\"line\">            server_id,</span><br><span class=\"line\">            binlog_filename,</span><br><span class=\"line\">            binlog_position,</span><br><span class=\"line\">            flags: <span class=\"number\">0</span>,</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">pub</span> <span class=\"keyword\">fn</span> <span class=\"title function_\">serialize</span>(&amp;<span class=\"keyword\">self</span>) <span class=\"punctuation\">-&gt;</span> <span class=\"type\">Result</span>&lt;<span class=\"type\">Vec</span>&lt;<span class=\"type\">u8</span>&gt;, io::Error&gt; &#123;</span><br><span class=\"line\">        <span class=\"keyword\">let</span> <span class=\"keyword\">mut </span><span class=\"variable\">vec</span> = <span class=\"type\">Vec</span>::<span class=\"title function_ invoke__\">new</span>();</span><br><span class=\"line\">        <span class=\"keyword\">let</span> <span class=\"keyword\">mut </span><span class=\"variable\">cursor</span> = Cursor::<span class=\"title function_ invoke__\">new</span>(&amp;<span class=\"keyword\">mut</span> vec);</span><br><span class=\"line\"></span><br><span class=\"line\">        cursor.<span class=\"title function_ invoke__\">write_u8</span>(CommandType::BinlogDump <span class=\"keyword\">as</span> <span class=\"type\">u8</span>)?;</span><br><span class=\"line\">        cursor.write_u32::&lt;LittleEndian&gt;(<span class=\"keyword\">self</span>.binlog_position)?;</span><br><span class=\"line\">        cursor.write_u16::&lt;LittleEndian&gt;(<span class=\"keyword\">self</span>.flags)?;</span><br><span class=\"line\">        cursor.write_u32::&lt;LittleEndian&gt;(<span class=\"keyword\">self</span>.server_id)?;</span><br><span class=\"line\">        cursor.<span class=\"title function_ invoke__\">write</span>(<span class=\"keyword\">self</span>.binlog_filename.<span class=\"title function_ invoke__\">as_bytes</span>())?;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"title function_ invoke__\">Ok</span>(vec)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"Handle-Binlog-Events\"><a href=\"#Handle-Binlog-Events\" class=\"headerlink\" title=\"Handle Binlog Events\"></a>Handle Binlog Events</h3><p>As mentioned above, usually, we let users handle these events but firstly, we need to parse them.</p>\n<h3 id=\"HeartBeats\"><a href=\"#HeartBeats\" class=\"headerlink\" title=\"HeartBeats\"></a>HeartBeats</h3><figure class=\"highlight rust\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">pub</span> <span class=\"keyword\">fn</span> <span class=\"title function_\">set_master_heartbeat</span>(&amp;<span class=\"keyword\">mut</span> <span class=\"keyword\">self</span>, channel: &amp;<span class=\"keyword\">mut</span> PacketChannel) <span class=\"punctuation\">-&gt;</span> <span class=\"type\">Result</span>&lt;(), Error&gt; &#123;</span><br><span class=\"line\">    <span class=\"keyword\">let</span> <span class=\"variable\">milliseconds</span> = <span class=\"keyword\">self</span>.options.heartbeat_interval.<span class=\"title function_ invoke__\">as_millis</span>();</span><br><span class=\"line\">    <span class=\"keyword\">let</span> <span class=\"variable\">nanoseconds</span> = milliseconds * <span class=\"number\">1000</span> * <span class=\"number\">1000</span>;</span><br><span class=\"line\">    <span class=\"keyword\">let</span> <span class=\"variable\">query</span> = <span class=\"built_in\">format!</span>(<span class=\"string\">&quot;set @master_heartbeat_period=&#123;&#125;&quot;</span>, nanoseconds);</span><br><span class=\"line\">    <span class=\"keyword\">let</span> <span class=\"variable\">command</span> = QueryCommand::<span class=\"title function_ invoke__\">new</span>(query.<span class=\"title function_ invoke__\">to_string</span>());</span><br><span class=\"line\">    channel.<span class=\"title function_ invoke__\">write_packet</span>(&amp;command.<span class=\"title function_ invoke__\">serialize</span>()?, <span class=\"number\">0</span>)?;</span><br><span class=\"line\">    <span class=\"keyword\">let</span> (packet, _) = channel.<span class=\"title function_ invoke__\">read_packet</span>()?;</span><br><span class=\"line\">    <span class=\"title function_ invoke__\">check_error_packet</span>(&amp;packet, <span class=\"string\">&quot;Setting master heartbeat error.&quot;</span>)?;</span><br><span class=\"line\">    <span class=\"title function_ invoke__\">Ok</span>(())</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"Get-Better-Performance\"><a href=\"#Get-Better-Performance\" class=\"headerlink\" title=\"Get Better Performance\"></a>Get Better Performance</h3><p>As we know, it is impossible for a CDC program to pull and parse binlog events infinitely as a slave. Also, as a developer, you may be not able to adjust the MySQL server parameters to improve the CDC performance. There are some suggestions for you if you are trouble with any relationed issues.</p>\n<h4 id=\"Batch-Processing\"><a href=\"#Batch-Processing\" class=\"headerlink\" title=\"Batch-Processing\"></a>Batch-Processing</h4><p>For the only slave, you can use batch processing anywhere except pulling binlog events like <strong>parse events</strong>, <strong>handle events</strong> or <strong>sink the events.</strong> There is an example that I used in parsing binlog events.</p>\n<ol>\n<li>Parser for parsing a single binlog events</li>\n</ol>\n<figure class=\"highlight rust\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">pub</span> <span class=\"keyword\">fn</span> <span class=\"title function_\">read_data</span>(&amp;<span class=\"keyword\">self</span>) <span class=\"punctuation\">-&gt;</span> io::<span class=\"type\">Result</span>&lt;<span class=\"type\">Option</span>&lt;EventData&lt;<span class=\"symbol\">&#x27;_</span>&gt;&gt;&gt; &#123;</span><br><span class=\"line\">        <span class=\"keyword\">use</span> EventType::*;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">let</span> <span class=\"variable\">event_type</span> = <span class=\"keyword\">match</span> <span class=\"keyword\">self</span>.header.event_type.<span class=\"title function_ invoke__\">get</span>() &#123;</span><br><span class=\"line\">            <span class=\"title function_ invoke__\">Ok</span>(event_type) =&gt; event_type,</span><br><span class=\"line\">            _ =&gt; <span class=\"keyword\">return</span> <span class=\"title function_ invoke__\">Ok</span>(<span class=\"literal\">None</span>),</span><br><span class=\"line\">        &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">let</span> <span class=\"variable\">event_data</span> = <span class=\"keyword\">match</span> event_type &#123;</span><br><span class=\"line\">            ENUM_END_EVENT | UNKNOWN_EVENT =&gt; EventData::UnknownEvent,</span><br><span class=\"line\">            START_EVENT_V3 =&gt; EventData::<span class=\"title function_ invoke__\">StartEventV3</span>(Cow::<span class=\"title function_ invoke__\">Borrowed</span>(&amp;*<span class=\"keyword\">self</span>.data)),</span><br><span class=\"line\">            QUERY_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">QueryEvent</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?),</span><br><span class=\"line\">            STOP_EVENT =&gt; EventData::StopEvent,</span><br><span class=\"line\">            ROTATE_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">RotateEvent</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?),</span><br><span class=\"line\">            INTVAR_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">IntvarEvent</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?),</span><br><span class=\"line\">            LOAD_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">LoadEvent</span>(Cow::<span class=\"title function_ invoke__\">Borrowed</span>(&amp;*<span class=\"keyword\">self</span>.data)),</span><br><span class=\"line\">            SLAVE_EVENT =&gt; EventData::SlaveEvent,</span><br><span class=\"line\">            CREATE_FILE_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">CreateFileEvent</span>(Cow::<span class=\"title function_ invoke__\">Borrowed</span>(&amp;*<span class=\"keyword\">self</span>.data)),</span><br><span class=\"line\">            APPEND_BLOCK_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">AppendBlockEvent</span>(Cow::<span class=\"title function_ invoke__\">Borrowed</span>(&amp;*<span class=\"keyword\">self</span>.data)),</span><br><span class=\"line\">            EXEC_LOAD_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">ExecLoadEvent</span>(Cow::<span class=\"title function_ invoke__\">Borrowed</span>(&amp;*<span class=\"keyword\">self</span>.data)),</span><br><span class=\"line\">            DELETE_FILE_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">DeleteFileEvent</span>(Cow::<span class=\"title function_ invoke__\">Borrowed</span>(&amp;*<span class=\"keyword\">self</span>.data)),</span><br><span class=\"line\">            NEW_LOAD_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">NewLoadEvent</span>(Cow::<span class=\"title function_ invoke__\">Borrowed</span>(&amp;*<span class=\"keyword\">self</span>.data)),</span><br><span class=\"line\">            RAND_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">RandEvent</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?),</span><br><span class=\"line\">            USER_VAR_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">UserVarEvent</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?),</span><br><span class=\"line\">            FORMAT_DESCRIPTION_EVENT =&gt; &#123;</span><br><span class=\"line\">                <span class=\"keyword\">let</span> <span class=\"variable\">fde</span> = <span class=\"keyword\">self</span></span><br><span class=\"line\">                    .read_event::&lt;FormatDescriptionEvent&gt;()?</span><br><span class=\"line\">                    .<span class=\"title function_ invoke__\">with_footer</span>(<span class=\"keyword\">self</span>.footer);</span><br><span class=\"line\">                EventData::<span class=\"title function_ invoke__\">FormatDescriptionEvent</span>(fde)</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            XID_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">XidEvent</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?),</span><br><span class=\"line\">            BEGIN_LOAD_QUERY_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">BeginLoadQueryEvent</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?),</span><br><span class=\"line\">            EXECUTE_LOAD_QUERY_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">ExecuteLoadQueryEvent</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?),</span><br><span class=\"line\">            TABLE_MAP_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">TableMapEvent</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?),</span><br><span class=\"line\">            PRE_GA_WRITE_ROWS_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">PreGaWriteRowsEvent</span>(Cow::<span class=\"title function_ invoke__\">Borrowed</span>(&amp;*<span class=\"keyword\">self</span>.data)),</span><br><span class=\"line\">            PRE_GA_UPDATE_ROWS_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">PreGaUpdateRowsEvent</span>(Cow::<span class=\"title function_ invoke__\">Borrowed</span>(&amp;*<span class=\"keyword\">self</span>.data)),</span><br><span class=\"line\">            PRE_GA_DELETE_ROWS_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">PreGaDeleteRowsEvent</span>(Cow::<span class=\"title function_ invoke__\">Borrowed</span>(&amp;*<span class=\"keyword\">self</span>.data)),</span><br><span class=\"line\">            WRITE_ROWS_EVENT_V1 =&gt; &#123;</span><br><span class=\"line\">                EventData::<span class=\"title function_ invoke__\">RowsEvent</span>(RowsEventData::<span class=\"title function_ invoke__\">WriteRowsEventV1</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?))</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            UPDATE_ROWS_EVENT_V1 =&gt; &#123;</span><br><span class=\"line\">                EventData::<span class=\"title function_ invoke__\">RowsEvent</span>(RowsEventData::<span class=\"title function_ invoke__\">UpdateRowsEventV1</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?))</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            DELETE_ROWS_EVENT_V1 =&gt; &#123;</span><br><span class=\"line\">                EventData::<span class=\"title function_ invoke__\">RowsEvent</span>(RowsEventData::<span class=\"title function_ invoke__\">DeleteRowsEventV1</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?))</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            INCIDENT_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">IncidentEvent</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?),</span><br><span class=\"line\">            HEARTBEAT_EVENT =&gt; EventData::HeartbeatEvent,</span><br><span class=\"line\">            IGNORABLE_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">IgnorableEvent</span>(Cow::<span class=\"title function_ invoke__\">Borrowed</span>(&amp;*<span class=\"keyword\">self</span>.data)),</span><br><span class=\"line\">            ROWS_QUERY_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">RowsQueryEvent</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?),</span><br><span class=\"line\">            WRITE_ROWS_EVENT =&gt; &#123;</span><br><span class=\"line\">                EventData::<span class=\"title function_ invoke__\">RowsEvent</span>(RowsEventData::<span class=\"title function_ invoke__\">WriteRowsEvent</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?))</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            UPDATE_ROWS_EVENT =&gt; &#123;</span><br><span class=\"line\">                EventData::<span class=\"title function_ invoke__\">RowsEvent</span>(RowsEventData::<span class=\"title function_ invoke__\">UpdateRowsEvent</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?))</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            DELETE_ROWS_EVENT =&gt; &#123;</span><br><span class=\"line\">                EventData::<span class=\"title function_ invoke__\">RowsEvent</span>(RowsEventData::<span class=\"title function_ invoke__\">DeleteRowsEvent</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?))</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            GTID_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">GtidEvent</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?),</span><br><span class=\"line\">            ANONYMOUS_GTID_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">AnonymousGtidEvent</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?),</span><br><span class=\"line\">            PREVIOUS_GTIDS_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">PreviousGtidsEvent</span>(Cow::<span class=\"title function_ invoke__\">Borrowed</span>(&amp;*<span class=\"keyword\">self</span>.data)),</span><br><span class=\"line\">            TRANSACTION_CONTEXT_EVENT =&gt; &#123;</span><br><span class=\"line\">                EventData::<span class=\"title function_ invoke__\">TransactionContextEvent</span>(Cow::<span class=\"title function_ invoke__\">Borrowed</span>(&amp;*<span class=\"keyword\">self</span>.data))</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            VIEW_CHANGE_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">ViewChangeEvent</span>(Cow::<span class=\"title function_ invoke__\">Borrowed</span>(&amp;*<span class=\"keyword\">self</span>.data)),</span><br><span class=\"line\">            XA_PREPARE_LOG_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">XaPrepareLogEvent</span>(Cow::<span class=\"title function_ invoke__\">Borrowed</span>(&amp;*<span class=\"keyword\">self</span>.data)),</span><br><span class=\"line\">            PARTIAL_UPDATE_ROWS_EVENT =&gt; &#123;</span><br><span class=\"line\">                EventData::<span class=\"title function_ invoke__\">RowsEvent</span>(RowsEventData::<span class=\"title function_ invoke__\">PartialUpdateRowsEvent</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?))</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            TRANSACTION_PAYLOAD_EVENT =&gt; EventData::<span class=\"title function_ invoke__\">TransactionPayloadEvent</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>()?),</span><br><span class=\"line\">        &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"title function_ invoke__\">Ok</span>(<span class=\"title function_ invoke__\">Some</span>(event_data))</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<ol>\n<li>Use the Rayon framework to enable concurrency</li>\n</ol>\n<p>Many operations in Rayon, such as map(), filter_map(), and others, do not inherently guarantee that the output order will be the same as the input order. However, when used in combination with collect(), they generally ensure the results are in order. So, if the order of data is needed, please write a test case to test your code.</p>\n<figure class=\"highlight rust\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> <span class=\"variable\">result</span> <span class=\"keyword\">in</span> client.<span class=\"title function_ invoke__\">replicate</span>()? &#123;</span><br><span class=\"line\">        <span class=\"keyword\">let</span> (header, event) = result?;</span><br><span class=\"line\">        event_queue.<span class=\"title function_ invoke__\">enqueue</span>((header, event));</span><br><span class=\"line\">        event_count += <span class=\"number\">1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> event_count.<span class=\"title function_ invoke__\">borrow</span>() % <span class=\"number\">100000</span> == <span class=\"number\">0</span> &#123;</span><br><span class=\"line\">            <span class=\"keyword\">let</span> <span class=\"keyword\">mut </span><span class=\"variable\">event_array</span> = event_queue.<span class=\"title function_ invoke__\">drain_all</span>();</span><br><span class=\"line\">            <span class=\"keyword\">let</span> <span class=\"variable\">new_arr</span>: <span class=\"type\">Vec</span>&lt;(EventHeader, <span class=\"type\">Result</span>&lt;BinlogEvent, Error&gt;)&gt; = event_array.<span class=\"title function_ invoke__\">into_par_iter</span>().<span class=\"title function_ invoke__\">map</span>(|(header, payload)| &#123;</span><br><span class=\"line\">                <span class=\"keyword\">let</span> <span class=\"keyword\">mut </span><span class=\"variable\">c_table_map</span> = table_map.<span class=\"title function_ invoke__\">clone</span>();</span><br><span class=\"line\">                <span class=\"keyword\">let</span> <span class=\"variable\">parsed_event</span> = <span class=\"title function_ invoke__\">parse_event</span>(&amp;header, &amp;payload, &amp;<span class=\"keyword\">mut</span> c_table_map);</span><br><span class=\"line\">                (header, parsed_event)</span><br><span class=\"line\">            &#125;).<span class=\"title function_ invoke__\">collect</span>();</span><br><span class=\"line\">            <span class=\"keyword\">let</span> <span class=\"variable\">dur</span> = start_time.<span class=\"title function_ invoke__\">elapsed</span>();</span><br><span class=\"line\">            <span class=\"built_in\">println!</span>(<span class=\"string\">&quot;Process Time Duration: &#123;:?&#125;&quot;</span>, dur);</span><br><span class=\"line\">            <span class=\"built_in\">println!</span>(<span class=\"string\">&quot;Current Binlog Events: &#123;:?&#125;&quot;</span>, event_count);</span><br><span class=\"line\">            <span class=\"built_in\">println!</span>(<span class=\"string\">&quot;Result Length: &#123;:?&#125;&quot;</span>, new_arr.<span class=\"title function_ invoke__\">len</span>());</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<ol>\n<li>Choose the needed events</li>\n</ol>\n<p>Normally, in CDC program, several binlog event types are as follows:</p>\n<figure class=\"highlight rust\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#[allow(non_camel_case_types)]</span></span><br><span class=\"line\"><span class=\"meta\">#[repr(u8)]</span></span><br><span class=\"line\"><span class=\"meta\">#[derive(Debug, Clone, Copy, Eq, PartialEq, Hash)]</span></span><br><span class=\"line\"><span class=\"keyword\">pub</span> <span class=\"keyword\">enum</span> <span class=\"title class_\">EventType</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/// A `QUERY_EVENT` is created for each query that modifies the database,</span></span><br><span class=\"line\">    <span class=\"comment\">/// unless the query is logged row-based.</span></span><br><span class=\"line\">    QUERY_EVENT = <span class=\"number\">0x02</span>,</span><br><span class=\"line\">    <span class=\"comment\">/// to tell the reader what binlog to request next.</span></span><br><span class=\"line\">    ROTATE_EVENT = <span class=\"number\">0x04</span>,</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/// if any transaction commit info needed</span></span><br><span class=\"line\">    XID_EVENT = <span class=\"number\">0x10</span>,</span><br><span class=\"line\">  </span><br><span class=\"line\">    TABLE_MAP_EVENT = <span class=\"number\">0x13</span>,</span><br><span class=\"line\">   </span><br><span class=\"line\">    WRITE_ROWS_EVENT_V1 = <span class=\"number\">0x17</span>,</span><br><span class=\"line\">    UPDATE_ROWS_EVENT_V1 = <span class=\"number\">0x18</span>,</span><br><span class=\"line\">    DELETE_ROWS_EVENT_V1 = <span class=\"number\">0x19</span>,</span><br><span class=\"line\">  </span><br><span class=\"line\">    WRITE_ROWS_EVENT = <span class=\"number\">0x1e</span>,</span><br><span class=\"line\">    UPDATE_ROWS_EVENT = <span class=\"number\">0x1f</span>,</span><br><span class=\"line\">    DELETE_ROWS_EVENT = <span class=\"number\">0x20</span>,</span><br><span class=\"line\">    GTID_EVENT = <span class=\"number\">0x21</span>,</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ol>\n<li>Add iterator for your binlog stream reader</li>\n</ol>\n<p>In my code, <code>client.replicate()</code> will return <code>Result&lt;BinlogEvents, Error&gt;</code>. So we need to implement Iterator trait for BinlogEvents.</p>\n<figure class=\"highlight rust\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">impl</span> <span class=\"title class_\">Iterator</span> <span class=\"keyword\">for</span> <span class=\"title class_\">BinlogEvents</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">type</span> <span class=\"title class_\">Item</span> = <span class=\"type\">Result</span>&lt;(EventHeader, <span class=\"type\">Vec</span>&lt;<span class=\"type\">u8</span>&gt;), Error&gt;;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/// Reads binlog event packets from network stream.</span></span><br><span class=\"line\">    <span class=\"comment\">/// &lt;a href=&quot;https://mariadb.com/kb/en/3-binlog-network-stream/&quot;&gt;See more&lt;/a&gt;</span></span><br><span class=\"line\">    <span class=\"keyword\">fn</span> <span class=\"title function_\">next</span>(&amp;<span class=\"keyword\">mut</span> <span class=\"keyword\">self</span>) <span class=\"punctuation\">-&gt;</span> <span class=\"type\">Option</span>&lt;<span class=\"keyword\">Self</span>::Item&gt; &#123;</span><br><span class=\"line\">        <span class=\"keyword\">let</span> (packet, _) = <span class=\"keyword\">match</span> <span class=\"keyword\">self</span>.channel.<span class=\"title function_ invoke__\">read_packet</span>() &#123;</span><br><span class=\"line\">            <span class=\"title function_ invoke__\">Ok</span>(x) =&gt; x,</span><br><span class=\"line\">            <span class=\"title function_ invoke__\">Err</span>(e) =&gt; <span class=\"keyword\">return</span> <span class=\"title function_ invoke__\">Some</span>(<span class=\"title function_ invoke__\">Err</span>(Error::<span class=\"title function_ invoke__\">IoError</span>(e))),</span><br><span class=\"line\">        &#125;;</span><br><span class=\"line\">        <span class=\"keyword\">match</span> packet[<span class=\"number\">0</span>] &#123;</span><br><span class=\"line\">            ResponseType::OK =&gt; <span class=\"title function_ invoke__\">Some</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_event</span>(&amp;packet)),</span><br><span class=\"line\">            ResponseType::ERROR =&gt; <span class=\"title function_ invoke__\">Some</span>(<span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">read_error</span>(&amp;packet)),</span><br><span class=\"line\">            ResponseType::END_OF_FILE =&gt; &#123;</span><br><span class=\"line\">                <span class=\"keyword\">let</span> <span class=\"variable\">_</span> = EndOfFilePacket::<span class=\"title function_ invoke__\">parse</span>(&amp;packet[<span class=\"number\">1</span>..]);</span><br><span class=\"line\">                <span class=\"literal\">None</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            _ =&gt; <span class=\"title function_ invoke__\">Some</span>(<span class=\"title function_ invoke__\">Err</span>(Error::<span class=\"title function_ invoke__\">String</span>(</span><br><span class=\"line\">                <span class=\"string\">&quot;Unknown network stream status&quot;</span>.<span class=\"title function_ invoke__\">to_string</span>(),</span><br><span class=\"line\">            ))),</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>In another case, the binlog stream reader implements the Stream trait like <code>while let Ok(Some(c)) = cdc_stream.try_next().await</code>.</p>\n<figure class=\"highlight rust\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">impl</span> <span class=\"title class_\">futures_core</span>::stream::Stream <span class=\"keyword\">for</span> <span class=\"title class_\">BinlogStream</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">type</span> <span class=\"title class_\">Item</span> = <span class=\"type\">Result</span>&lt;ChgcapEvent&gt;;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">fn</span> <span class=\"title function_\">poll_next</span>(</span><br><span class=\"line\">        <span class=\"keyword\">self</span>: Pin&lt;&amp;<span class=\"keyword\">mut</span> <span class=\"keyword\">Self</span>&gt;,</span><br><span class=\"line\">        cx: &amp;<span class=\"keyword\">mut</span> std::task::Context&lt;<span class=\"symbol\">&#x27;_</span>&gt;,</span><br><span class=\"line\">    ) <span class=\"punctuation\">-&gt;</span> Poll&lt;<span class=\"type\">Option</span>&lt;<span class=\"keyword\">Self</span>::Item&gt;&gt; &#123;</span><br><span class=\"line\">        <span class=\"keyword\">let</span> <span class=\"variable\">this</span> = <span class=\"keyword\">self</span>.<span class=\"title function_ invoke__\">get_mut</span>();</span><br><span class=\"line\">        <span class=\"comment\">// <span class=\"doctag\">TODO:</span> Support rate limiting.</span></span><br><span class=\"line\">        <span class=\"keyword\">loop</span> &#123;</span><br><span class=\"line\">            <span class=\"keyword\">let</span> <span class=\"variable\">binlog_stream</span> = Pin::<span class=\"title function_ invoke__\">new</span>(&amp;<span class=\"keyword\">mut</span> this.binlog_stream);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">match</span> binlog_stream.<span class=\"title function_ invoke__\">poll_next</span>(cx) &#123;</span><br><span class=\"line\">                Poll::<span class=\"title function_ invoke__\">Ready</span>(t) =&gt; <span class=\"keyword\">match</span> t &#123;</span><br><span class=\"line\">                    <span class=\"title function_ invoke__\">Some</span>(event_result) =&gt; <span class=\"keyword\">match</span> event_result &#123;</span><br><span class=\"line\">                        <span class=\"title function_ invoke__\">Ok</span>(event) =&gt; <span class=\"keyword\">match</span> this.<span class=\"title function_ invoke__\">handle_event</span>(event) &#123; <span class=\"comment\">// here</span></span><br><span class=\"line\">                            <span class=\"title function_ invoke__\">Ok</span>(change) =&gt; <span class=\"keyword\">match</span> change &#123;</span><br><span class=\"line\">                                <span class=\"title function_ invoke__\">Some</span>(c) =&gt; Poll::<span class=\"title function_ invoke__\">Ready</span>(<span class=\"title function_ invoke__\">Some</span>(<span class=\"title function_ invoke__\">Ok</span>(c))),</span><br><span class=\"line\">                                <span class=\"literal\">None</span> =&gt; <span class=\"keyword\">continue</span>, <span class=\"comment\">// Skip this event.</span></span><br><span class=\"line\">                            &#125;,</span><br><span class=\"line\">                            <span class=\"title function_ invoke__\">Err</span>(e) =&gt; Poll::<span class=\"title function_ invoke__\">Ready</span>(<span class=\"title function_ invoke__\">Some</span>(<span class=\"title function_ invoke__\">Err</span>(e))),</span><br><span class=\"line\">                        &#125;,</span><br><span class=\"line\">                        <span class=\"title function_ invoke__\">Err</span>(err) =&gt; Poll::<span class=\"title function_ invoke__\">Ready</span>(<span class=\"title function_ invoke__\">Some</span>(<span class=\"title function_ invoke__\">Err</span>(anyhow!(err)))),</span><br><span class=\"line\">                    &#125;,</span><br><span class=\"line\">                    <span class=\"literal\">None</span> =&gt; Poll::<span class=\"title function_ invoke__\">Ready</span>(<span class=\"literal\">None</span>), <span class=\"comment\">// Completed.</span></span><br><span class=\"line\">                &#125;,</span><br><span class=\"line\">                Poll::Pending =&gt; Poll::Pending,</span><br><span class=\"line\">            &#125;;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<ol>\n<li>Accumulate unprocessed binlog events into a batch like Step 2</li>\n</ol>\n<h4 id=\"Multiple-Slave\"><a href=\"#Multiple-Slave\" class=\"headerlink\" title=\"Multiple Slave\"></a>Multiple Slave</h4><p>In <strong>Batch-Processing</strong> section, we can boost the performance of CDC from tens of thousands of events to over a hundred thousand events per second. But however, if you need higher performance, single-instance batching may not meet your needs.</p>\n<p>Luckily, there is an another road to make it. Before introducing this way, let us take eyes on <strong>table_id</strong>.</p>\n<ul>\n<li>table_id is 6 bytes, need pad little-endian number.</li>\n<li>table_id is a temporary value. it would be changed when:<ul>\n<li>drop and replace table</li>\n<li>rename table</li>\n<li>alter table</li>\n<li>MySQL Engine upgrade</li>\n</ul>\n</li>\n<li>Before every data change events(WriteRowsEvent, UpdateRowsEvent, DeleteRowsEvent), there would be a TableMapEvent including table name, table id and table schema.</li>\n</ul>\n<p>Next, you should know how to get table metadata from TableMapEvent.</p>\n<p><img src=\"/2023/12/21/mysql-cdc/table_map_event.png\" alt=\"TableMapEvent\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">TableMapEvent</span>(<span class=\"title class_ inherited__\">BinLogEvent</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;This event describes the structure of a table.</span></span><br><span class=\"line\"><span class=\"string\">    It&#x27;s sent before a change happens on a table.</span></span><br><span class=\"line\"><span class=\"string\">    An end user of the lib should have no usage of this</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, from_packet, event_size, table_map, ctl_connection, **kwargs</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(TableMapEvent, self).__init__(from_packet, event_size,</span><br><span class=\"line\">                                            table_map, ctl_connection, **kwargs)</span><br><span class=\"line\">        self.__only_tables = kwargs[<span class=\"string\">&quot;only_tables&quot;</span>]</span><br><span class=\"line\">        self.__ignored_tables = kwargs[<span class=\"string\">&quot;ignored_tables&quot;</span>]</span><br><span class=\"line\">        self.__only_schemas = kwargs[<span class=\"string\">&quot;only_schemas&quot;</span>]</span><br><span class=\"line\">        self.__ignored_schemas = kwargs[<span class=\"string\">&quot;ignored_schemas&quot;</span>]</span><br><span class=\"line\">        self.__freeze_schema = kwargs[<span class=\"string\">&quot;freeze_schema&quot;</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Post-Header</span></span><br><span class=\"line\">        self.table_id = self._read_table_id()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.table_id <span class=\"keyword\">in</span> table_map <span class=\"keyword\">and</span> self.__freeze_schema:</span><br><span class=\"line\">            self._processed = <span class=\"literal\">False</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span></span><br><span class=\"line\"></span><br><span class=\"line\">        self.flags = struct.unpack(<span class=\"string\">&#x27;&lt;H&#x27;</span>, self.packet.read(<span class=\"number\">2</span>))[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Payload</span></span><br><span class=\"line\">        self.schema_length = struct.unpack(<span class=\"string\">&quot;!B&quot;</span>, self.packet.read(<span class=\"number\">1</span>))[<span class=\"number\">0</span>]</span><br><span class=\"line\">        self.schema = self.packet.read(self.schema_length).decode()</span><br><span class=\"line\">        self.packet.advance(<span class=\"number\">1</span>)</span><br><span class=\"line\">        self.table_length = struct.unpack(<span class=\"string\">&quot;!B&quot;</span>, self.packet.read(<span class=\"number\">1</span>))[<span class=\"number\">0</span>]</span><br><span class=\"line\">        self.table = self.packet.read(self.table_length).decode()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.__only_tables <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span> <span class=\"keyword\">and</span> self.table <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> self.__only_tables:</span><br><span class=\"line\">            self._processed = <span class=\"literal\">False</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span></span><br><span class=\"line\">        <span class=\"keyword\">elif</span> self.__ignored_tables <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span> <span class=\"keyword\">and</span> self.table <span class=\"keyword\">in</span> self.__ignored_tables:</span><br><span class=\"line\">            self._processed = <span class=\"literal\">False</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.__only_schemas <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span> <span class=\"keyword\">and</span> self.schema <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> self.__only_schemas:</span><br><span class=\"line\">            self._processed = <span class=\"literal\">False</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span></span><br><span class=\"line\">        <span class=\"keyword\">elif</span> self.__ignored_schemas <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span> <span class=\"keyword\">and</span> self.schema <span class=\"keyword\">in</span> self.__ignored_schemas:</span><br><span class=\"line\">            self._processed = <span class=\"literal\">False</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span></span><br><span class=\"line\"></span><br><span class=\"line\">        self.packet.advance(<span class=\"number\">1</span>)</span><br><span class=\"line\">        self.column_count = self.packet.read_length_coded_binary()</span><br><span class=\"line\"></span><br><span class=\"line\">        self.columns = []</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.table_id <span class=\"keyword\">in</span> table_map:</span><br><span class=\"line\">            self.column_schemas = table_map[self.table_id].column_schemas</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            self.column_schemas = self._ctl_connection._get_table_information(self.schema, self.table)</span><br><span class=\"line\"></span><br><span class=\"line\">        ordinal_pos_loc = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(self.column_schemas) != <span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"comment\"># Read columns meta data</span></span><br><span class=\"line\">            column_types = <span class=\"built_in\">bytearray</span>(self.packet.read(self.column_count))</span><br><span class=\"line\">            self.packet.read_length_coded_binary()</span><br><span class=\"line\">            <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">0</span>, <span class=\"built_in\">len</span>(column_types)):</span><br><span class=\"line\">                column_type = column_types[i]</span><br><span class=\"line\">                <span class=\"keyword\">try</span>:</span><br><span class=\"line\">                    column_schema = self.column_schemas[ordinal_pos_loc]</span><br><span class=\"line\"></span><br><span class=\"line\">                    <span class=\"comment\"># only acknowledge the column definition if the iteration matches with ordinal position of</span></span><br><span class=\"line\">                    <span class=\"comment\"># the column. this helps in maintaining support for restricted columnar access</span></span><br><span class=\"line\">                    <span class=\"keyword\">if</span> i != (column_schema[<span class=\"string\">&#x27;ORDINAL_POSITION&#x27;</span>] - <span class=\"number\">1</span>):</span><br><span class=\"line\">                        <span class=\"comment\"># raise IndexError to follow the workflow of dropping columns which are not matching the</span></span><br><span class=\"line\">                        <span class=\"comment\"># underlying table schema</span></span><br><span class=\"line\">                        <span class=\"keyword\">raise</span> IndexError</span><br><span class=\"line\"></span><br><span class=\"line\">                    ordinal_pos_loc += <span class=\"number\">1</span></span><br><span class=\"line\">                <span class=\"keyword\">except</span> IndexError:</span><br><span class=\"line\">                    <span class=\"comment\"># this a dirty hack to prevent row events containing columns which have been dropped prior</span></span><br><span class=\"line\">                    <span class=\"comment\"># to pymysqlreplication start, but replayed from binlog from blowing up the service.</span></span><br><span class=\"line\">                    <span class=\"comment\"># <span class=\"doctag\">TODO:</span> this does not address the issue if the column other than the last one is dropped</span></span><br><span class=\"line\">                    column_schema = &#123;</span><br><span class=\"line\">                        <span class=\"string\">&#x27;COLUMN_NAME&#x27;</span>: <span class=\"string\">&#x27;__dropped_col_&#123;i&#125;__&#x27;</span>.<span class=\"built_in\">format</span>(i=i),</span><br><span class=\"line\">                        <span class=\"string\">&#x27;COLLATION_NAME&#x27;</span>: <span class=\"literal\">None</span>,</span><br><span class=\"line\">                        <span class=\"string\">&#x27;CHARACTER_SET_NAME&#x27;</span>: <span class=\"literal\">None</span>,</span><br><span class=\"line\">                        <span class=\"string\">&#x27;COLUMN_COMMENT&#x27;</span>: <span class=\"literal\">None</span>,</span><br><span class=\"line\">                        <span class=\"string\">&#x27;COLUMN_TYPE&#x27;</span>: <span class=\"string\">&#x27;BLOB&#x27;</span>,  <span class=\"comment\"># we don&#x27;t know what it is, so let&#x27;s not do anything with it.</span></span><br><span class=\"line\">                        <span class=\"string\">&#x27;COLUMN_KEY&#x27;</span>: <span class=\"string\">&#x27;&#x27;</span>,</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                col = Column(column_type, column_schema, from_packet)</span><br><span class=\"line\">                self.columns.append(col)</span><br><span class=\"line\"></span><br><span class=\"line\">        self.table_obj = Table(self.column_schemas, self.table_id, self.schema,</span><br><span class=\"line\">                               self.table, self.columns)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># ith column is nullable if (i - 1)th bit is set to True, not nullable otherwise</span></span><br><span class=\"line\">        <span class=\"comment\">## Refer to definition of and call to row.event._is_null() to interpret bitmap corresponding to columns</span></span><br><span class=\"line\">        self.null_bitmask = self.packet.read((self.column_count + <span class=\"number\">7</span>) / <span class=\"number\">8</span>)</span><br></pre></td></tr></table></figure>\n<p>Congratulations! You have known how to divide your binlog stream reader to different database(schema) / table.</p>\n<p><img src=\"/2023/12/21/mysql-cdc/row_events.png\" alt=\"RowEvents\"></p>\n<p>For TableMapEvent, we can store the table_id for the RowEvents. When any RowEvents parse the table_id and it is not in the table map cache, the event should be ignore.</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li><a href=\"https://arxiv.org/pdf/2010.12597.pdf\">https://arxiv.org/pdf/2010.12597.pdf</a></li>\n<li><a href=\"https://github.com/blackbeam/mysql_async\">https://github.com/blackbeam/mysql_async</a></li>\n<li><a href=\"https://github.com/neverchanje/chgcap-rs\">https://github.com/neverchanje/chgcap-rs</a></li>\n</ol>\n"}],"PostAsset":[{"_id":"source/_posts/bloom/counting_bloom_filter.png","slug":"counting_bloom_filter.png","post":"clo80pb0x00035cc36g0v2e5w","modified":0,"renderable":0},{"_id":"source/_posts/bloom/cuckoo_filter.png","slug":"cuckoo_filter.png","post":"clo80pb0x00035cc36g0v2e5w","modified":0,"renderable":0},{"_id":"source/_posts/bloom/scalable_bloom_filter.png","slug":"scalable_bloom_filter.png","post":"clo80pb0x00035cc36g0v2e5w","modified":0,"renderable":0},{"_id":"source/_posts/bloom/scalable_bloom_filter_slices.png","slug":"scalable_bloom_filter_slices.png","post":"clo80pb0x00035cc36g0v2e5w","modified":0,"renderable":0},{"_id":"source/_posts/z-order/linear-order.png","slug":"linear-order.png","post":"clo80pb0y00055cc3cdo58nh7","modified":0,"renderable":0},{"_id":"source/_posts/z-order/z-order.png","slug":"z-order.png","post":"clo80pb0y00055cc3cdo58nh7","modified":0,"renderable":0},{"_id":"source/_posts/lsm-tree/leveled_compaction.png","slug":"leveled_compaction.png","post":"clo80pb0y00045cc34vtph8t3","modified":0,"renderable":0},{"_id":"source/_posts/lsm-tree/lsm_tree.png","slug":"lsm_tree.png","post":"clo80pb0y00045cc34vtph8t3","modified":0,"renderable":0},{"_id":"source/_posts/lsm-tree/merge_policy.png","slug":"merge_policy.png","post":"clo80pb0y00045cc34vtph8t3","modified":0,"renderable":0},{"_id":"source/_posts/lsm-tree/range_query.png","slug":"range_query.png","post":"clo80pb0y00045cc34vtph8t3","modified":0,"renderable":0},{"_id":"source/_posts/lsm-tree/size_tiered_compaction.png","slug":"size_tiered_compaction.png","post":"clo80pb0y00045cc34vtph8t3","modified":0,"renderable":0},{"_id":"source/_posts/lsm-tree/sorted_runs.png","slug":"sorted_runs.png","post":"clo80pb0y00045cc34vtph8t3","modified":0,"renderable":0},{"_id":"source/_posts/lsm-tree/sparse_index.png","slug":"sparse_index.png","post":"clo80pb0y00045cc34vtph8t3","modified":0,"renderable":0},{"_id":"source/_posts/lsm-tree/sstable.png","slug":"sstable.png","post":"clo80pb0y00045cc34vtph8t3","modified":0,"renderable":0},{"_id":"source/_posts/snowflake-stream/buffer_pool.png","slug":"buffer_pool.png","post":"clqfx3ahf0000b7c37bxb0miw","modified":0,"renderable":0},{"_id":"source/_posts/mysql-cdc/row_events.png","slug":"row_events.png","post":"clqfx3ahg0001b7c3hbtlfxzp","modified":1,"renderable":0},{"_id":"source/_posts/mysql-cdc/table_map_event.png","slug":"table_map_event.png","post":"clqfx3ahg0001b7c3hbtlfxzp","modified":1,"renderable":0}],"PostCategory":[],"PostTag":[{"post_id":"clo80pb0t00005cc3h1h812r4","tag_id":"clo80pb0w00025cc366xgcv0w","_id":"clo80pb0y00075cc3cvit924n"},{"post_id":"clo80pb0x00035cc36g0v2e5w","tag_id":"clo80pb0y00065cc3frqs70vb","_id":"clo80pb0z00095cc36c952cgm"},{"post_id":"clo80pb0y00045cc34vtph8t3","tag_id":"clo80pb0y00085cc3dk579zz0","_id":"clo80pb0z000b5cc374z38ecs"},{"post_id":"clo80pb0y00055cc3cdo58nh7","tag_id":"clo80pb0z000a5cc380pbh22w","_id":"clo80pb0z000c5cc3b44e4ket"},{"post_id":"clqfx3ahf0000b7c37bxb0miw","tag_id":"clqfx3ahg0002b7c38011efeh","_id":"clqfx3ahh0004b7c3ab3c13z4"},{"post_id":"clqfx3ahg0001b7c3hbtlfxzp","tag_id":"clqfx3ahh0003b7c38ht14vkp","_id":"clqfx3ahh0005b7c39u91b1vm"}],"Tag":[{"name":"bigdata","_id":"clo80pb0w00025cc366xgcv0w"},{"name":"Bloom-Filter","_id":"clo80pb0y00065cc3frqs70vb"},{"name":"LSM Storage","_id":"clo80pb0y00085cc3dk579zz0"},{"name":"big data, index","_id":"clo80pb0z000a5cc380pbh22w"},{"name":"stream cdc snowflake","_id":"clqfx3ahg0002b7c38011efeh"},{"name":"cdc mysql binlog","_id":"clqfx3ahh0003b7c38ht14vkp"}]}}