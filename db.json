{"meta":{"version":1,"warehouse":"4.0.2"},"models":{"Asset":[{"_id":"themes/apollo/source/favicon.png_origin","path":"favicon.png_origin","modified":0,"renderable":1},{"_id":"themes/apollo/source/favicon.png","path":"favicon.png","modified":0,"renderable":1},{"_id":"themes/apollo/source/css/apollo.css","path":"css/apollo.css","modified":0,"renderable":1},{"_id":"themes/apollo/source/font/sourcesanspro.woff","path":"font/sourcesanspro.woff","modified":0,"renderable":1},{"_id":"themes/apollo/source/font/sourcesanspro.woff2","path":"font/sourcesanspro.woff2","modified":0,"renderable":1},{"_id":"themes/apollo/source/scss/apollo.scss","path":"scss/apollo.scss","modified":0,"renderable":1}],"Cache":[{"_id":"source/_posts/apache-arrow.md","hash":"2d37266a095b5810623056d296f9a46770963042","modified":1697546557705},{"_id":"source/_posts/lsm-tree.md","hash":"7ad931ccd15329fadf982a3a65d6da66cc615342","modified":1697173512033},{"_id":"source/_posts/z-order.md","hash":"600a820ee9a3d15c5c0e0d6eacda16e340feb52c","modified":1697694017280},{"_id":"source/_posts/bloom.md","hash":"c6b73c846f4ec7d7bdb77203e87bcb5e2262f64f","modified":1696322667406},{"_id":"source/_posts/z-order/linear-order.png","hash":"615614221b1f0deb5af09b531fd7a5ad44741597","modified":1697679014790},{"_id":"source/_posts/z-order/z-order.png","hash":"92ac0fa8c4b8a79b47ef892416a48d311e0b12ce","modified":1697679005841},{"_id":"source/_posts/lsm-tree/lsm_tree.png","hash":"a05ee23c764f9ec6b8bbd8beb7966a0d1d948d53","modified":1696336037896},{"_id":"source/_posts/bloom/scalable_bloom_filter_slices.png","hash":"cceccf33246196109f987e7c2a6d79e8c1991baf","modified":1696320101570},{"_id":"source/_posts/bloom/scalable_bloom_filter.png","hash":"b3465257148d1e6220207fe4905b12be5be476ee","modified":1696319286528},{"_id":"source/_posts/lsm-tree/leveled_compaction.png","hash":"269c1dc419a0d8e0e10b60201e845e961d711bd8","modified":1697172400625},{"_id":"source/_posts/bloom/counting_bloom_filter.png","hash":"4341a36a29824acb03930ce2f78859b0e2552e54","modified":1696322485561},{"_id":"source/_posts/lsm-tree/range_query.png","hash":"77509946cf1f8c4bf0cb35688ba35c074b84ae55","modified":1696730612527},{"_id":"source/_posts/lsm-tree/sstable.png","hash":"cb853e8d7b8d27de9758742434584beb24ef58a0","modified":1696667281071},{"_id":"source/_posts/lsm-tree/merge_policy.png","hash":"1722696f22a890231bb1c3dd0afdcc1c8672770d","modified":1696731591775},{"_id":"source/_posts/lsm-tree/sorted_runs.png","hash":"e14bf61c546d7831745d6231b4f79c70b13d2e39","modified":1696742589621},{"_id":"source/_posts/lsm-tree/sparse_index.png","hash":"c8ff0e09d371b6aeacba785d84f50f9f3396fad2","modified":1696657426319},{"_id":"source/_posts/lsm-tree/size_tiered_compaction.png","hash":"70926f3735c514da0b78b52f4c7a5feb9c8e4aae","modified":1697078833192},{"_id":"themes/apollo/.gitignore","hash":"a006beea0877a0aa3610ee00e73f62cb1d45125b","modified":1696033836459},{"_id":"themes/apollo/LICENSE","hash":"6e31ac9076bfc8f09ae47977419eee4edfb63e5b","modified":1696033836450},{"_id":"themes/apollo/package.json","hash":"eb1e76ec0b7ed6c6c7b2bd32b4f1e1bbe15800ca","modified":1696033836459},{"_id":"themes/apollo/README.md","hash":"b2cdcb158721c13eee6a2ae237d0488a738ed8c5","modified":1696033836458},{"_id":"themes/apollo/gulpfile.js","hash":"857a026b6643a2cd52c65d4ae0dc7fe9618206ee","modified":1696033836459},{"_id":"themes/apollo/_config.yml","hash":"d381543761117d88d3254855612dc9dd3f35c332","modified":1696300027974},{"_id":"themes/apollo/doc/doc-en.md","hash":"409e931a444c02a57b64a0a44dde6e66c1881ca0","modified":1696033836460},{"_id":"themes/apollo/doc/doc-zh.md","hash":"2a6a81840cdaf497969268a12d8f62c98cc38103","modified":1696033836460},{"_id":"themes/apollo/languages/en.yml","hash":"ca168b190932229884db1de755ec2f793c758a16","modified":1696033836458},{"_id":"themes/apollo/languages/zh-cn.yml","hash":"22a2d16fe8c0dddb016b5325b9b9c182a1b49ae1","modified":1696033836458},{"_id":"themes/apollo/layout/archive.jade","hash":"62797414355bf4474092bc3a32726c8340820ffb","modified":1696033836453},{"_id":"themes/apollo/layout/index.jade","hash":"55f2f1b4b5364a0e09cb18e1112664c6415fb881","modified":1696033836453},{"_id":"themes/apollo/layout/post.jade","hash":"245c26244c075c3632d1545c3b228ee9d112f15d","modified":1696033836453},{"_id":"themes/apollo/source/favicon.png_origin","hash":"a9cdcb22d1e74d5480323e19d1983de5a6873b8c","modified":1696033836457},{"_id":"themes/apollo/layout/mixins/paginator.jade","hash":"f4ee2fb61a32e199b48cf93771749edc8a007391","modified":1696033836453},{"_id":"themes/apollo/layout/mixins/post.jade","hash":"f23c6c40e14cdf16783b3c2baf736c9dce18408c","modified":1696033836453},{"_id":"themes/apollo/layout/partial/comment.jade","hash":"ff0a2c269c2434da2ac5529872f1d6184a71f96d","modified":1696033836451},{"_id":"themes/apollo/layout/partial/head.jade","hash":"43d2db73f0247a9a3ed00ecb95b537a872b7201a","modified":1696033836451},{"_id":"themes/apollo/layout/partial/nav.jade","hash":"f4842d9d3d763fbb823d112a6f49f24cc42a0ad4","modified":1696033836451},{"_id":"themes/apollo/layout/partial/layout.jade","hash":"529c2ec06cfbc3d5b6d66dd320db50dfab5577a6","modified":1696033836452},{"_id":"themes/apollo/layout/partial/copyright.jade","hash":"74c38c649c1781669f30a05587f07459ae8944db","modified":1696048884120},{"_id":"themes/apollo/layout/partial/scripts.jade","hash":"6bff591ae3d1ff6750f239c4c933ad61f009f36a","modified":1696033836452},{"_id":"themes/apollo/source/css/apollo.css","hash":"714851e684a7fb17e21ddeed56a1112b432eaf94","modified":1696666644460},{"_id":"themes/apollo/source/scss/apollo.scss","hash":"e0092f469264b55b25e0d441274f1c812147e7d1","modified":1696033836454},{"_id":"themes/apollo/source/scss/_partial/archive-post-list.scss","hash":"d2f740a7d48349b7536777c795f82ab740836d0f","modified":1696033836455},{"_id":"themes/apollo/source/font/sourcesanspro.woff","hash":"a6722c9b6439b7a020a9be3d3178970757a9265c","modified":1696033836457},{"_id":"themes/apollo/source/font/sourcesanspro.woff2","hash":"da65f527a8da65d5eb6721626d28cfdb46ab104a","modified":1696033836457},{"_id":"themes/apollo/source/scss/_partial/base.scss","hash":"ae967b2049ecb9b8c4e139ecce32fd9fb5358ac5","modified":1696033836455},{"_id":"themes/apollo/source/scss/_partial/home-post-list.scss","hash":"6b5c59f3d2295944f934aee2c1156012a3306d5d","modified":1696033836456},{"_id":"themes/apollo/source/scss/_partial/copyright.scss","hash":"1309667e3000037170cfbb5b8c9c65f4ffcf6814","modified":1696033836455},{"_id":"themes/apollo/source/scss/_partial/footer.scss","hash":"094aca6e52f11b139ac7980ca03fa7b9d8fc7b2f","modified":1696033836456},{"_id":"themes/apollo/source/scss/_partial/normalize.scss","hash":"fd0b27bed6f103ea95b08f698ea663ff576dbcf1","modified":1696033836456},{"_id":"themes/apollo/source/favicon.png","hash":"db29234c8b16c9f841ad8f0801d77bbf40024ef7","modified":1696744340339},{"_id":"themes/apollo/source/scss/_partial/header.scss","hash":"153bde88bf8ffeae4ffd813d8cc694dd83d33d94","modified":1696033836456},{"_id":"themes/apollo/source/scss/_partial/mq.scss","hash":"fc5dda52cfbb10e27e2471e03f4606fb3d588225","modified":1696033836455},{"_id":"themes/apollo/source/scss/_partial/post.scss","hash":"7a2579935781913e4c7bce71e3de9f1449107c51","modified":1696666424231},{"_id":"public/atom.xml","hash":"64e087910399328be25778f1db9df45912f79300","modified":1697694026084},{"_id":"public/sitemap.xml","hash":"c9e8fdf7de0c1d99c8ef6a6686e8015b2e1b0717","modified":1697694026084},{"_id":"public/sitemap.txt","hash":"ee69b2bebc55f284039a3d156146e70b0bc23a43","modified":1697694026084},{"_id":"public/2023/10/19/z-order/index.html","hash":"d375f0b8da7463da4fb6595bfbe58e69b35ae9c1","modified":1697694026084},{"_id":"public/2023/10/02/lsm-tree/index.html","hash":"e67fe04be2849949beff2f0ea2e41ef292e593ae","modified":1697694026084},{"_id":"public/2023/10/08/apache-arrow/index.html","hash":"cd2c31d97b4e48c26193c3393fae08c96f115b02","modified":1697694026084},{"_id":"public/2023/10/02/bloom/index.html","hash":"4e6b17c0283d462aeda23f35f03509fe0929822a","modified":1697694026084},{"_id":"public/archives/index.html","hash":"597f3253c44ba1156bcc1135b881aee95df4db5b","modified":1697694026084},{"_id":"public/index.html","hash":"9450e805e516adbbf90dec39a53645f29f34912a","modified":1697694026084},{"_id":"public/tags/bigdata/index.html","hash":"51162cbd11e09eb4b2a2c15cc11bb7b14121f9c3","modified":1697694026084},{"_id":"public/tags/LSM-Storage/index.html","hash":"cdfcba4de3e9246b8d49eb2b0faa4790b0d4c276","modified":1697694026084},{"_id":"public/tags/big-data-index/index.html","hash":"a8356a1cfcfebcca373e290fceb878970e71fdae","modified":1697694026084},{"_id":"public/tags/Bloom-Filter/index.html","hash":"0a8c14e4e396689f092e9ce78bd72bbede30b2eb","modified":1697694026084},{"_id":"public/font/sourcesanspro.woff","hash":"a6722c9b6439b7a020a9be3d3178970757a9265c","modified":1697694026084},{"_id":"public/favicon.png_origin","hash":"a9cdcb22d1e74d5480323e19d1983de5a6873b8c","modified":1697694026084},{"_id":"public/favicon.png","hash":"db29234c8b16c9f841ad8f0801d77bbf40024ef7","modified":1697694026084},{"_id":"public/2023/10/02/lsm-tree/lsm_tree.png","hash":"a05ee23c764f9ec6b8bbd8beb7966a0d1d948d53","modified":1697694026084},{"_id":"public/scss/apollo.scss","hash":"e0092f469264b55b25e0d441274f1c812147e7d1","modified":1697694026084},{"_id":"public/2023/10/02/lsm-tree/leveled_compaction.png","hash":"269c1dc419a0d8e0e10b60201e845e961d711bd8","modified":1697694026084},{"_id":"public/font/sourcesanspro.woff2","hash":"da65f527a8da65d5eb6721626d28cfdb46ab104a","modified":1697694026084},{"_id":"public/2023/10/02/lsm-tree/range_query.png","hash":"77509946cf1f8c4bf0cb35688ba35c074b84ae55","modified":1697694026084},{"_id":"public/2023/10/19/z-order/linear-order.png","hash":"615614221b1f0deb5af09b531fd7a5ad44741597","modified":1697694026084},{"_id":"public/2023/10/02/bloom/counting_bloom_filter.png","hash":"4341a36a29824acb03930ce2f78859b0e2552e54","modified":1697694026084},{"_id":"public/2023/10/02/bloom/scalable_bloom_filter.png","hash":"b3465257148d1e6220207fe4905b12be5be476ee","modified":1697694026084},{"_id":"public/2023/10/19/z-order/z-order.png","hash":"92ac0fa8c4b8a79b47ef892416a48d311e0b12ce","modified":1697694026084},{"_id":"public/2023/10/02/bloom/scalable_bloom_filter_slices.png","hash":"cceccf33246196109f987e7c2a6d79e8c1991baf","modified":1697694026084},{"_id":"public/2023/10/02/lsm-tree/merge_policy.png","hash":"1722696f22a890231bb1c3dd0afdcc1c8672770d","modified":1697694026084},{"_id":"public/2023/10/02/lsm-tree/sstable.png","hash":"cb853e8d7b8d27de9758742434584beb24ef58a0","modified":1697694026084},{"_id":"public/css/apollo.css","hash":"714851e684a7fb17e21ddeed56a1112b432eaf94","modified":1697694026084},{"_id":"public/2023/10/02/lsm-tree/sparse_index.png","hash":"c8ff0e09d371b6aeacba785d84f50f9f3396fad2","modified":1697694026084},{"_id":"public/2023/10/02/lsm-tree/sorted_runs.png","hash":"e14bf61c546d7831745d6231b4f79c70b13d2e39","modified":1697694026084},{"_id":"public/2023/10/02/lsm-tree/size_tiered_compaction.png","hash":"70926f3735c514da0b78b52f4c7a5feb9c8e4aae","modified":1697694026084}],"Category":[],"Data":[],"Page":[],"Post":[{"title":"Apache Arrow For the first time","date":"2023-10-08T12:17:14.000Z","_content":"内存格式，和parquet文件存储格式差不多，但是某些行为上和parquet不一致，比如对待null值的处理是不一样的。\n至于为什么能节省序列化和反序列化，arrow/arrow flight/arrow2之间的爱恨情仇还得再看","source":"_posts/apache-arrow.md","raw":"---\ntitle: Apache Arrow For the first time\ndate: 2023-10-08 20:17:14\ntags: bigdata \n---\n内存格式，和parquet文件存储格式差不多，但是某些行为上和parquet不一致，比如对待null值的处理是不一样的。\n至于为什么能节省序列化和反序列化，arrow/arrow flight/arrow2之间的爱恨情仇还得再看","slug":"apache-arrow","published":1,"updated":"2023-10-19T08:57:56.332Z","_id":"clnwr6uke0000i5c337167pvz","comments":1,"layout":"post","photos":[],"link":"","content":"<p>内存格式，和parquet文件存储格式差不多，但是某些行为上和parquet不一致，比如对待null值的处理是不一样的。<br>至于为什么能节省序列化和反序列化，arrow&#x2F;arrow flight&#x2F;arrow2之间的爱恨情仇还得再看</p>\n","site":{"data":{}},"excerpt":"","more":"<p>内存格式，和parquet文件存储格式差不多，但是某些行为上和parquet不一致，比如对待null值的处理是不一样的。<br>至于为什么能节省序列化和反序列化，arrow&#x2F;arrow flight&#x2F;arrow2之间的爱恨情仇还得再看</p>\n"},{"title":"the principle and application of LSM Tree","date":"2023-10-02T11:24:02.000Z","_content":"# LSM Tree\n\n> LSM-Tree is a kind of storage engine rather than storage format.\n\n## Feature\n\n* Ordered\n* Block Storage(Disk)-oriented\n* Hierarchical\n\n## Structure\n\n![LSM-Tree Structure](lsm_tree.png#pic_center)\n\n## Workflow\n\n### WAL\n\nWhen LSM Tree received a input(insert/update/delete) operation, to avoid accidental crashing or shutdown，It is neccessary to write ahead log(WAL) that saves operation records into log files.\n\n```go\ntype Wal struct {\n\tf    *os.File\n\tpath string\n\tlock sync.Locker\n}\n```\n\n### Memtable\n\nMemtable is an append-only data structure (every node inserted cannot be changed by logical delete and removing duplicates based on update time.) like Binary search tree (e.g. RedBlack tree - LevelDB) or SkipList (e.g. HBase, more popular).\n\n### Sorted String Table (SSTables)\n\nUsually, a SSTable consists of an index block and multiple data blocks. Data block structure is as follows:\n\n![Sorted Strings Table](./lsm-tree/sstable.png#pic_center)\n\nwhere data is only ordered in segment layer rather than global.\n\n### Optimization\n\n#### improve read performance\n\nFor LSM Tree, what we concerned is read performance rather than write. As data increases, some methods to improve read performance need to come into our pictures.\n\n* **Sparse Index**\n\nAs mentioned above, SStable has several segments that data are orderly stored. Without any optimization, we can also use binary search algorithm to find a certain element by scanning all the SStables and every Segments in each SStable unitl finding it. But unfortunately, for binary search algorithm, the minimum memory unit is segment for us to find the key in *O(logn)* or spent a Disk IO, both are too expensive in big data scenarios. So it is neccessary to  build a sparse index in memory to accelerate query efficiency.\n\n![sparse index](sparse_index.png#pic_center)\n\n> Sparse indexes only contain entries for documents that have the indexed field, even if the index field contains a null value. The index skips over any document that is missing the indexed field. The index is \"sparse\" because it does not include all documents of a collection.\n\n* **Bloom Filter**\n\nWhen the number of SStable increases in Disk, if some key is not present in the records, we need to scan all the SStable to find that key. Bloom filter is to overcome this issue. Unlike sparse indexes, [Bloom filters](https://adooobe.github.io/2023/10/02/bloom/) are designed to address the performance issues that arise when querying for non-existent keys.\n\nIt is worth noting that Bloom Filters should be updated as compaction operations because compaction will delete some tombstone data so that BF can't work normally.\n\n### Compaction\n\n#### questions in query\n\nLet's talk about query in LSM Tree first. There are two query methods: *point lookup query* and *range query*.\n\n* **point lookup query**: find the element what we want from new segment to old one.\n* **range query**: when a big range query is executed, data have to be found in memtable, immutable memtable and multiple SSTalbes in different levels. (Notice: range query should be **key range query** like the follow picture)\n\n![LSM Tree range query](range_query.png#pic_center)\n\nDuring range reads, the iterator will seek to the start range similar to point lookup (using Binary search with in SSTs) using`SeekTo()` call. After seeking to start range, there will be series of iterators created one for each memtable, one for each Level-0 files (because of overlapping nature of SSTs in L0) and one for each level later on. A merging iterator will collect keys from each of these iterators and gives the data in sorted order till the End range is reached.\n\n```go\nitr := txn.NewIterator(badger.DefaultIteratorOptions)   \nfor itr.Seek(\"startKey\"); itr.Valid(); itr.Next() {\n    item := itr.Item()\n    key := item.Key()\n    if bytes.Compare(key, \"endKey\") > 0 {\n      break\n    }\n    // rest of the logic.\n}\n```\n\nStep1. find *startkey* position (write as startPosition) by `seek()` and move sub iterator to `startPosition + 1`\n\nStep2. compare the sub iterators' element, return the minimal value and move the itr pointer.\n\nStep3. repeat Step2 until the returned element > endkey\n\nSo in range query, as SSTables become more and more, query execution also becomes heavier and heavier.\n\n#### Sorted Run\n\n> LSM tree organizes files into several sorted runs. A sorted run consists of one or multiple [data file](https://nightlies.apache.org/flink/flink-table-store-docs-release-0.3/docs/concepts/file-layouts/#data-files)s and each data file belongs to exactly one sorted run.\n>\n> ![sorted runs](sorted_runs.png#pic_center)\n>\n> As you can see, different sorted runs may have overlapping primary key ranges, and may even contain the same primary key. When querying the LSM tree, all sorted runs must be combined and all records with the same primary key must be merged according to the user-specified [merge engine](https://nightlies.apache.org/flink/flink-table-store-docs-release-0.3/docs/features/table-types/#merge-engines) and the timestamp of each record.\n\nIn my opinion, in LSM Tree, a single logically ordered and no-repeat structure can be regarded as a `sorted run`.\n\n#### Methods\n\n![LSM Tree merge policies](merge_policy.png#pic_center)\n\n* tiered compaction(suitable for reads scenarios)\n  ![size_tiered_compaction](size_tiered_compaction.png#pic_center)\n  * high read and space amplification\n  * in each tier, storage unit(memtable/sstable in level 0) size won't be changed unitl it is merged into next-level SSTable. The size of a single SSTable in the next level is the sum of the number of SSTables in the previous level.\n* leveled compaction(suitable for writes scenarios)\n  * high write amplification\n  * As shown in the following figure, each leveled level is an ordered run that consists of multiple sstables. These sstables also maintain an orderly relationship with each other. When the data size of each level reaches the upper limit, this level will merge with the run of the next level. This method combines multiple runs of the level to one, reducing the read amplification and space amplification. Also, the smaller sstables provide fine-grained task splitting and control. This way, controlling the task size is actually controlling the size of the temporary space. In other words, leveled merge policies will merge SSTables into next level with the same range to reduce space and read amplifications.\n\n![leveled compaction](leveled_compaction.png#pic_center)\n\n## Improvement\n\n* **bLSM**\n* **Diff-Index LSM**\n\n## Reference\n\n1. https://www.cnblogs.com/whuanle/p/16297025.html\n2. https://www.geeksforgeeks.org/introduction-to-log-structured-merge-lsm-tree/\n3. https://www.mongodb.com/docs/manual/core/index-sparse/#:~:text=Sparse%20indexes%20only%20contain%20entries,all%20documents%20of%20a%20collection.\n4. https://hzhu212.github.io/posts/2d7c5edb/\n5. https://www.alibabacloud.com/blog/an-in-depth-discussion-on-the-lsm-compaction-mechanism_596780\n6. https://zhuanlan.zhihu.com/p/380013595\n7. https://github.com/facebook/rocksdb/wiki/Iterator-Implementation\n","source":"_posts/lsm-tree.md","raw":"---\ntitle: the principle and application of LSM Tree \ndate: 2023-10-02 19:24:02\ntags: LSM Storage\n---\n# LSM Tree\n\n> LSM-Tree is a kind of storage engine rather than storage format.\n\n## Feature\n\n* Ordered\n* Block Storage(Disk)-oriented\n* Hierarchical\n\n## Structure\n\n![LSM-Tree Structure](lsm_tree.png#pic_center)\n\n## Workflow\n\n### WAL\n\nWhen LSM Tree received a input(insert/update/delete) operation, to avoid accidental crashing or shutdown，It is neccessary to write ahead log(WAL) that saves operation records into log files.\n\n```go\ntype Wal struct {\n\tf    *os.File\n\tpath string\n\tlock sync.Locker\n}\n```\n\n### Memtable\n\nMemtable is an append-only data structure (every node inserted cannot be changed by logical delete and removing duplicates based on update time.) like Binary search tree (e.g. RedBlack tree - LevelDB) or SkipList (e.g. HBase, more popular).\n\n### Sorted String Table (SSTables)\n\nUsually, a SSTable consists of an index block and multiple data blocks. Data block structure is as follows:\n\n![Sorted Strings Table](./lsm-tree/sstable.png#pic_center)\n\nwhere data is only ordered in segment layer rather than global.\n\n### Optimization\n\n#### improve read performance\n\nFor LSM Tree, what we concerned is read performance rather than write. As data increases, some methods to improve read performance need to come into our pictures.\n\n* **Sparse Index**\n\nAs mentioned above, SStable has several segments that data are orderly stored. Without any optimization, we can also use binary search algorithm to find a certain element by scanning all the SStables and every Segments in each SStable unitl finding it. But unfortunately, for binary search algorithm, the minimum memory unit is segment for us to find the key in *O(logn)* or spent a Disk IO, both are too expensive in big data scenarios. So it is neccessary to  build a sparse index in memory to accelerate query efficiency.\n\n![sparse index](sparse_index.png#pic_center)\n\n> Sparse indexes only contain entries for documents that have the indexed field, even if the index field contains a null value. The index skips over any document that is missing the indexed field. The index is \"sparse\" because it does not include all documents of a collection.\n\n* **Bloom Filter**\n\nWhen the number of SStable increases in Disk, if some key is not present in the records, we need to scan all the SStable to find that key. Bloom filter is to overcome this issue. Unlike sparse indexes, [Bloom filters](https://adooobe.github.io/2023/10/02/bloom/) are designed to address the performance issues that arise when querying for non-existent keys.\n\nIt is worth noting that Bloom Filters should be updated as compaction operations because compaction will delete some tombstone data so that BF can't work normally.\n\n### Compaction\n\n#### questions in query\n\nLet's talk about query in LSM Tree first. There are two query methods: *point lookup query* and *range query*.\n\n* **point lookup query**: find the element what we want from new segment to old one.\n* **range query**: when a big range query is executed, data have to be found in memtable, immutable memtable and multiple SSTalbes in different levels. (Notice: range query should be **key range query** like the follow picture)\n\n![LSM Tree range query](range_query.png#pic_center)\n\nDuring range reads, the iterator will seek to the start range similar to point lookup (using Binary search with in SSTs) using`SeekTo()` call. After seeking to start range, there will be series of iterators created one for each memtable, one for each Level-0 files (because of overlapping nature of SSTs in L0) and one for each level later on. A merging iterator will collect keys from each of these iterators and gives the data in sorted order till the End range is reached.\n\n```go\nitr := txn.NewIterator(badger.DefaultIteratorOptions)   \nfor itr.Seek(\"startKey\"); itr.Valid(); itr.Next() {\n    item := itr.Item()\n    key := item.Key()\n    if bytes.Compare(key, \"endKey\") > 0 {\n      break\n    }\n    // rest of the logic.\n}\n```\n\nStep1. find *startkey* position (write as startPosition) by `seek()` and move sub iterator to `startPosition + 1`\n\nStep2. compare the sub iterators' element, return the minimal value and move the itr pointer.\n\nStep3. repeat Step2 until the returned element > endkey\n\nSo in range query, as SSTables become more and more, query execution also becomes heavier and heavier.\n\n#### Sorted Run\n\n> LSM tree organizes files into several sorted runs. A sorted run consists of one or multiple [data file](https://nightlies.apache.org/flink/flink-table-store-docs-release-0.3/docs/concepts/file-layouts/#data-files)s and each data file belongs to exactly one sorted run.\n>\n> ![sorted runs](sorted_runs.png#pic_center)\n>\n> As you can see, different sorted runs may have overlapping primary key ranges, and may even contain the same primary key. When querying the LSM tree, all sorted runs must be combined and all records with the same primary key must be merged according to the user-specified [merge engine](https://nightlies.apache.org/flink/flink-table-store-docs-release-0.3/docs/features/table-types/#merge-engines) and the timestamp of each record.\n\nIn my opinion, in LSM Tree, a single logically ordered and no-repeat structure can be regarded as a `sorted run`.\n\n#### Methods\n\n![LSM Tree merge policies](merge_policy.png#pic_center)\n\n* tiered compaction(suitable for reads scenarios)\n  ![size_tiered_compaction](size_tiered_compaction.png#pic_center)\n  * high read and space amplification\n  * in each tier, storage unit(memtable/sstable in level 0) size won't be changed unitl it is merged into next-level SSTable. The size of a single SSTable in the next level is the sum of the number of SSTables in the previous level.\n* leveled compaction(suitable for writes scenarios)\n  * high write amplification\n  * As shown in the following figure, each leveled level is an ordered run that consists of multiple sstables. These sstables also maintain an orderly relationship with each other. When the data size of each level reaches the upper limit, this level will merge with the run of the next level. This method combines multiple runs of the level to one, reducing the read amplification and space amplification. Also, the smaller sstables provide fine-grained task splitting and control. This way, controlling the task size is actually controlling the size of the temporary space. In other words, leveled merge policies will merge SSTables into next level with the same range to reduce space and read amplifications.\n\n![leveled compaction](leveled_compaction.png#pic_center)\n\n## Improvement\n\n* **bLSM**\n* **Diff-Index LSM**\n\n## Reference\n\n1. https://www.cnblogs.com/whuanle/p/16297025.html\n2. https://www.geeksforgeeks.org/introduction-to-log-structured-merge-lsm-tree/\n3. https://www.mongodb.com/docs/manual/core/index-sparse/#:~:text=Sparse%20indexes%20only%20contain%20entries,all%20documents%20of%20a%20collection.\n4. https://hzhu212.github.io/posts/2d7c5edb/\n5. https://www.alibabacloud.com/blog/an-in-depth-discussion-on-the-lsm-compaction-mechanism_596780\n6. https://zhuanlan.zhihu.com/p/380013595\n7. https://github.com/facebook/rocksdb/wiki/Iterator-Implementation\n","slug":"lsm-tree","published":1,"updated":"2023-10-19T07:51:30.642Z","_id":"clnwr6ukh0001i5c36ui51o86","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"LSM-Tree\"><a href=\"#LSM-Tree\" class=\"headerlink\" title=\"LSM Tree\"></a>LSM Tree</h1><blockquote>\n<p>LSM-Tree is a kind of storage engine rather than storage format.</p>\n</blockquote>\n<h2 id=\"Feature\"><a href=\"#Feature\" class=\"headerlink\" title=\"Feature\"></a>Feature</h2><ul>\n<li>Ordered</li>\n<li>Block Storage(Disk)-oriented</li>\n<li>Hierarchical</li>\n</ul>\n<h2 id=\"Structure\"><a href=\"#Structure\" class=\"headerlink\" title=\"Structure\"></a>Structure</h2><p><img src=\"/2023/10/02/lsm-tree/lsm_tree.png#pic_center\" alt=\"LSM-Tree Structure\"></p>\n<h2 id=\"Workflow\"><a href=\"#Workflow\" class=\"headerlink\" title=\"Workflow\"></a>Workflow</h2><h3 id=\"WAL\"><a href=\"#WAL\" class=\"headerlink\" title=\"WAL\"></a>WAL</h3><p>When LSM Tree received a input(insert&#x2F;update&#x2F;delete) operation, to avoid accidental crashing or shutdown，It is neccessary to write ahead log(WAL) that saves operation records into log files.</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">type</span> Wal <span class=\"keyword\">struct</span> &#123;</span><br><span class=\"line\">\tf    *os.File</span><br><span class=\"line\">\tpath <span class=\"type\">string</span></span><br><span class=\"line\">\tlock sync.Locker</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Memtable\"><a href=\"#Memtable\" class=\"headerlink\" title=\"Memtable\"></a>Memtable</h3><p>Memtable is an append-only data structure (every node inserted cannot be changed by logical delete and removing duplicates based on update time.) like Binary search tree (e.g. RedBlack tree - LevelDB) or SkipList (e.g. HBase, more popular).</p>\n<h3 id=\"Sorted-String-Table-SSTables\"><a href=\"#Sorted-String-Table-SSTables\" class=\"headerlink\" title=\"Sorted String Table (SSTables)\"></a>Sorted String Table (SSTables)</h3><p>Usually, a SSTable consists of an index block and multiple data blocks. Data block structure is as follows:</p>\n<p><img src=\"/2023/10/02/lsm-tree/sstable.png#pic_center\" alt=\"Sorted Strings Table\"></p>\n<p>where data is only ordered in segment layer rather than global.</p>\n<h3 id=\"Optimization\"><a href=\"#Optimization\" class=\"headerlink\" title=\"Optimization\"></a>Optimization</h3><h4 id=\"improve-read-performance\"><a href=\"#improve-read-performance\" class=\"headerlink\" title=\"improve read performance\"></a>improve read performance</h4><p>For LSM Tree, what we concerned is read performance rather than write. As data increases, some methods to improve read performance need to come into our pictures.</p>\n<ul>\n<li><strong>Sparse Index</strong></li>\n</ul>\n<p>As mentioned above, SStable has several segments that data are orderly stored. Without any optimization, we can also use binary search algorithm to find a certain element by scanning all the SStables and every Segments in each SStable unitl finding it. But unfortunately, for binary search algorithm, the minimum memory unit is segment for us to find the key in <em>O(logn)</em> or spent a Disk IO, both are too expensive in big data scenarios. So it is neccessary to  build a sparse index in memory to accelerate query efficiency.</p>\n<p><img src=\"/2023/10/02/lsm-tree/sparse_index.png#pic_center\" alt=\"sparse index\"></p>\n<blockquote>\n<p>Sparse indexes only contain entries for documents that have the indexed field, even if the index field contains a null value. The index skips over any document that is missing the indexed field. The index is “sparse” because it does not include all documents of a collection.</p>\n</blockquote>\n<ul>\n<li><strong>Bloom Filter</strong></li>\n</ul>\n<p>When the number of SStable increases in Disk, if some key is not present in the records, we need to scan all the SStable to find that key. Bloom filter is to overcome this issue. Unlike sparse indexes, <a href=\"https://adooobe.github.io/2023/10/02/bloom/\">Bloom filters</a> are designed to address the performance issues that arise when querying for non-existent keys.</p>\n<p>It is worth noting that Bloom Filters should be updated as compaction operations because compaction will delete some tombstone data so that BF can’t work normally.</p>\n<h3 id=\"Compaction\"><a href=\"#Compaction\" class=\"headerlink\" title=\"Compaction\"></a>Compaction</h3><h4 id=\"questions-in-query\"><a href=\"#questions-in-query\" class=\"headerlink\" title=\"questions in query\"></a>questions in query</h4><p>Let’s talk about query in LSM Tree first. There are two query methods: <em>point lookup query</em> and <em>range query</em>.</p>\n<ul>\n<li><strong>point lookup query</strong>: find the element what we want from new segment to old one.</li>\n<li><strong>range query</strong>: when a big range query is executed, data have to be found in memtable, immutable memtable and multiple SSTalbes in different levels. (Notice: range query should be <strong>key range query</strong> like the follow picture)</li>\n</ul>\n<p><img src=\"/2023/10/02/lsm-tree/range_query.png#pic_center\" alt=\"LSM Tree range query\"></p>\n<p>During range reads, the iterator will seek to the start range similar to point lookup (using Binary search with in SSTs) using<code>SeekTo()</code> call. After seeking to start range, there will be series of iterators created one for each memtable, one for each Level-0 files (because of overlapping nature of SSTs in L0) and one for each level later on. A merging iterator will collect keys from each of these iterators and gives the data in sorted order till the End range is reached.</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">itr := txn.NewIterator(badger.DefaultIteratorOptions)   </span><br><span class=\"line\"><span class=\"keyword\">for</span> itr.Seek(<span class=\"string\">&quot;startKey&quot;</span>); itr.Valid(); itr.Next() &#123;</span><br><span class=\"line\">    item := itr.Item()</span><br><span class=\"line\">    key := item.Key()</span><br><span class=\"line\">    <span class=\"keyword\">if</span> bytes.Compare(key, <span class=\"string\">&quot;endKey&quot;</span>) &gt; <span class=\"number\">0</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">break</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// rest of the logic.</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>Step1. find <em>startkey</em> position (write as startPosition) by <code>seek()</code> and move sub iterator to <code>startPosition + 1</code></p>\n<p>Step2. compare the sub iterators’ element, return the minimal value and move the itr pointer.</p>\n<p>Step3. repeat Step2 until the returned element &gt; endkey</p>\n<p>So in range query, as SSTables become more and more, query execution also becomes heavier and heavier.</p>\n<h4 id=\"Sorted-Run\"><a href=\"#Sorted-Run\" class=\"headerlink\" title=\"Sorted Run\"></a>Sorted Run</h4><blockquote>\n<p>LSM tree organizes files into several sorted runs. A sorted run consists of one or multiple <a href=\"https://nightlies.apache.org/flink/flink-table-store-docs-release-0.3/docs/concepts/file-layouts/#data-files\">data file</a>s and each data file belongs to exactly one sorted run.</p>\n<p><img src=\"/2023/10/02/lsm-tree/sorted_runs.png#pic_center\" alt=\"sorted runs\"></p>\n<p>As you can see, different sorted runs may have overlapping primary key ranges, and may even contain the same primary key. When querying the LSM tree, all sorted runs must be combined and all records with the same primary key must be merged according to the user-specified <a href=\"https://nightlies.apache.org/flink/flink-table-store-docs-release-0.3/docs/features/table-types/#merge-engines\">merge engine</a> and the timestamp of each record.</p>\n</blockquote>\n<p>In my opinion, in LSM Tree, a single logically ordered and no-repeat structure can be regarded as a <code>sorted run</code>.</p>\n<h4 id=\"Methods\"><a href=\"#Methods\" class=\"headerlink\" title=\"Methods\"></a>Methods</h4><p><img src=\"/2023/10/02/lsm-tree/merge_policy.png#pic_center\" alt=\"LSM Tree merge policies\"></p>\n<ul>\n<li>tiered compaction(suitable for reads scenarios)<br><img src=\"/2023/10/02/lsm-tree/size_tiered_compaction.png#pic_center\" alt=\"size_tiered_compaction\"><ul>\n<li>high read and space amplification</li>\n<li>in each tier, storage unit(memtable&#x2F;sstable in level 0) size won’t be changed unitl it is merged into next-level SSTable. The size of a single SSTable in the next level is the sum of the number of SSTables in the previous level.</li>\n</ul>\n</li>\n<li>leveled compaction(suitable for writes scenarios)<ul>\n<li>high write amplification</li>\n<li>As shown in the following figure, each leveled level is an ordered run that consists of multiple sstables. These sstables also maintain an orderly relationship with each other. When the data size of each level reaches the upper limit, this level will merge with the run of the next level. This method combines multiple runs of the level to one, reducing the read amplification and space amplification. Also, the smaller sstables provide fine-grained task splitting and control. This way, controlling the task size is actually controlling the size of the temporary space. In other words, leveled merge policies will merge SSTables into next level with the same range to reduce space and read amplifications.</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"/2023/10/02/lsm-tree/leveled_compaction.png#pic_center\" alt=\"leveled compaction\"></p>\n<h2 id=\"Improvement\"><a href=\"#Improvement\" class=\"headerlink\" title=\"Improvement\"></a>Improvement</h2><ul>\n<li><strong>bLSM</strong></li>\n<li><strong>Diff-Index LSM</strong></li>\n</ul>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li><a href=\"https://www.cnblogs.com/whuanle/p/16297025.html\">https://www.cnblogs.com/whuanle/p/16297025.html</a></li>\n<li><a href=\"https://www.geeksforgeeks.org/introduction-to-log-structured-merge-lsm-tree/\">https://www.geeksforgeeks.org/introduction-to-log-structured-merge-lsm-tree/</a></li>\n<li><a href=\"https://www.mongodb.com/docs/manual/core/index-sparse/#:~:text=Sparse%20indexes%20only%20contain%20entries,all%20documents%20of%20a%20collection\">https://www.mongodb.com/docs/manual/core/index-sparse/#:~:text=Sparse%20indexes%20only%20contain%20entries,all%20documents%20of%20a%20collection</a>.</li>\n<li><a href=\"https://hzhu212.github.io/posts/2d7c5edb/\">https://hzhu212.github.io/posts/2d7c5edb/</a></li>\n<li><a href=\"https://www.alibabacloud.com/blog/an-in-depth-discussion-on-the-lsm-compaction-mechanism_596780\">https://www.alibabacloud.com/blog/an-in-depth-discussion-on-the-lsm-compaction-mechanism_596780</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/380013595\">https://zhuanlan.zhihu.com/p/380013595</a></li>\n<li><a href=\"https://github.com/facebook/rocksdb/wiki/Iterator-Implementation\">https://github.com/facebook/rocksdb/wiki/Iterator-Implementation</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"LSM-Tree\"><a href=\"#LSM-Tree\" class=\"headerlink\" title=\"LSM Tree\"></a>LSM Tree</h1><blockquote>\n<p>LSM-Tree is a kind of storage engine rather than storage format.</p>\n</blockquote>\n<h2 id=\"Feature\"><a href=\"#Feature\" class=\"headerlink\" title=\"Feature\"></a>Feature</h2><ul>\n<li>Ordered</li>\n<li>Block Storage(Disk)-oriented</li>\n<li>Hierarchical</li>\n</ul>\n<h2 id=\"Structure\"><a href=\"#Structure\" class=\"headerlink\" title=\"Structure\"></a>Structure</h2><p><img src=\"/2023/10/02/lsm-tree/lsm_tree.png#pic_center\" alt=\"LSM-Tree Structure\"></p>\n<h2 id=\"Workflow\"><a href=\"#Workflow\" class=\"headerlink\" title=\"Workflow\"></a>Workflow</h2><h3 id=\"WAL\"><a href=\"#WAL\" class=\"headerlink\" title=\"WAL\"></a>WAL</h3><p>When LSM Tree received a input(insert&#x2F;update&#x2F;delete) operation, to avoid accidental crashing or shutdown，It is neccessary to write ahead log(WAL) that saves operation records into log files.</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">type</span> Wal <span class=\"keyword\">struct</span> &#123;</span><br><span class=\"line\">\tf    *os.File</span><br><span class=\"line\">\tpath <span class=\"type\">string</span></span><br><span class=\"line\">\tlock sync.Locker</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Memtable\"><a href=\"#Memtable\" class=\"headerlink\" title=\"Memtable\"></a>Memtable</h3><p>Memtable is an append-only data structure (every node inserted cannot be changed by logical delete and removing duplicates based on update time.) like Binary search tree (e.g. RedBlack tree - LevelDB) or SkipList (e.g. HBase, more popular).</p>\n<h3 id=\"Sorted-String-Table-SSTables\"><a href=\"#Sorted-String-Table-SSTables\" class=\"headerlink\" title=\"Sorted String Table (SSTables)\"></a>Sorted String Table (SSTables)</h3><p>Usually, a SSTable consists of an index block and multiple data blocks. Data block structure is as follows:</p>\n<p><img src=\"/2023/10/02/lsm-tree/sstable.png#pic_center\" alt=\"Sorted Strings Table\"></p>\n<p>where data is only ordered in segment layer rather than global.</p>\n<h3 id=\"Optimization\"><a href=\"#Optimization\" class=\"headerlink\" title=\"Optimization\"></a>Optimization</h3><h4 id=\"improve-read-performance\"><a href=\"#improve-read-performance\" class=\"headerlink\" title=\"improve read performance\"></a>improve read performance</h4><p>For LSM Tree, what we concerned is read performance rather than write. As data increases, some methods to improve read performance need to come into our pictures.</p>\n<ul>\n<li><strong>Sparse Index</strong></li>\n</ul>\n<p>As mentioned above, SStable has several segments that data are orderly stored. Without any optimization, we can also use binary search algorithm to find a certain element by scanning all the SStables and every Segments in each SStable unitl finding it. But unfortunately, for binary search algorithm, the minimum memory unit is segment for us to find the key in <em>O(logn)</em> or spent a Disk IO, both are too expensive in big data scenarios. So it is neccessary to  build a sparse index in memory to accelerate query efficiency.</p>\n<p><img src=\"/2023/10/02/lsm-tree/sparse_index.png#pic_center\" alt=\"sparse index\"></p>\n<blockquote>\n<p>Sparse indexes only contain entries for documents that have the indexed field, even if the index field contains a null value. The index skips over any document that is missing the indexed field. The index is “sparse” because it does not include all documents of a collection.</p>\n</blockquote>\n<ul>\n<li><strong>Bloom Filter</strong></li>\n</ul>\n<p>When the number of SStable increases in Disk, if some key is not present in the records, we need to scan all the SStable to find that key. Bloom filter is to overcome this issue. Unlike sparse indexes, <a href=\"https://adooobe.github.io/2023/10/02/bloom/\">Bloom filters</a> are designed to address the performance issues that arise when querying for non-existent keys.</p>\n<p>It is worth noting that Bloom Filters should be updated as compaction operations because compaction will delete some tombstone data so that BF can’t work normally.</p>\n<h3 id=\"Compaction\"><a href=\"#Compaction\" class=\"headerlink\" title=\"Compaction\"></a>Compaction</h3><h4 id=\"questions-in-query\"><a href=\"#questions-in-query\" class=\"headerlink\" title=\"questions in query\"></a>questions in query</h4><p>Let’s talk about query in LSM Tree first. There are two query methods: <em>point lookup query</em> and <em>range query</em>.</p>\n<ul>\n<li><strong>point lookup query</strong>: find the element what we want from new segment to old one.</li>\n<li><strong>range query</strong>: when a big range query is executed, data have to be found in memtable, immutable memtable and multiple SSTalbes in different levels. (Notice: range query should be <strong>key range query</strong> like the follow picture)</li>\n</ul>\n<p><img src=\"/2023/10/02/lsm-tree/range_query.png#pic_center\" alt=\"LSM Tree range query\"></p>\n<p>During range reads, the iterator will seek to the start range similar to point lookup (using Binary search with in SSTs) using<code>SeekTo()</code> call. After seeking to start range, there will be series of iterators created one for each memtable, one for each Level-0 files (because of overlapping nature of SSTs in L0) and one for each level later on. A merging iterator will collect keys from each of these iterators and gives the data in sorted order till the End range is reached.</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">itr := txn.NewIterator(badger.DefaultIteratorOptions)   </span><br><span class=\"line\"><span class=\"keyword\">for</span> itr.Seek(<span class=\"string\">&quot;startKey&quot;</span>); itr.Valid(); itr.Next() &#123;</span><br><span class=\"line\">    item := itr.Item()</span><br><span class=\"line\">    key := item.Key()</span><br><span class=\"line\">    <span class=\"keyword\">if</span> bytes.Compare(key, <span class=\"string\">&quot;endKey&quot;</span>) &gt; <span class=\"number\">0</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">break</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// rest of the logic.</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>Step1. find <em>startkey</em> position (write as startPosition) by <code>seek()</code> and move sub iterator to <code>startPosition + 1</code></p>\n<p>Step2. compare the sub iterators’ element, return the minimal value and move the itr pointer.</p>\n<p>Step3. repeat Step2 until the returned element &gt; endkey</p>\n<p>So in range query, as SSTables become more and more, query execution also becomes heavier and heavier.</p>\n<h4 id=\"Sorted-Run\"><a href=\"#Sorted-Run\" class=\"headerlink\" title=\"Sorted Run\"></a>Sorted Run</h4><blockquote>\n<p>LSM tree organizes files into several sorted runs. A sorted run consists of one or multiple <a href=\"https://nightlies.apache.org/flink/flink-table-store-docs-release-0.3/docs/concepts/file-layouts/#data-files\">data file</a>s and each data file belongs to exactly one sorted run.</p>\n<p><img src=\"/2023/10/02/lsm-tree/sorted_runs.png#pic_center\" alt=\"sorted runs\"></p>\n<p>As you can see, different sorted runs may have overlapping primary key ranges, and may even contain the same primary key. When querying the LSM tree, all sorted runs must be combined and all records with the same primary key must be merged according to the user-specified <a href=\"https://nightlies.apache.org/flink/flink-table-store-docs-release-0.3/docs/features/table-types/#merge-engines\">merge engine</a> and the timestamp of each record.</p>\n</blockquote>\n<p>In my opinion, in LSM Tree, a single logically ordered and no-repeat structure can be regarded as a <code>sorted run</code>.</p>\n<h4 id=\"Methods\"><a href=\"#Methods\" class=\"headerlink\" title=\"Methods\"></a>Methods</h4><p><img src=\"/2023/10/02/lsm-tree/merge_policy.png#pic_center\" alt=\"LSM Tree merge policies\"></p>\n<ul>\n<li>tiered compaction(suitable for reads scenarios)<br><img src=\"/2023/10/02/lsm-tree/size_tiered_compaction.png#pic_center\" alt=\"size_tiered_compaction\"><ul>\n<li>high read and space amplification</li>\n<li>in each tier, storage unit(memtable&#x2F;sstable in level 0) size won’t be changed unitl it is merged into next-level SSTable. The size of a single SSTable in the next level is the sum of the number of SSTables in the previous level.</li>\n</ul>\n</li>\n<li>leveled compaction(suitable for writes scenarios)<ul>\n<li>high write amplification</li>\n<li>As shown in the following figure, each leveled level is an ordered run that consists of multiple sstables. These sstables also maintain an orderly relationship with each other. When the data size of each level reaches the upper limit, this level will merge with the run of the next level. This method combines multiple runs of the level to one, reducing the read amplification and space amplification. Also, the smaller sstables provide fine-grained task splitting and control. This way, controlling the task size is actually controlling the size of the temporary space. In other words, leveled merge policies will merge SSTables into next level with the same range to reduce space and read amplifications.</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"/2023/10/02/lsm-tree/leveled_compaction.png#pic_center\" alt=\"leveled compaction\"></p>\n<h2 id=\"Improvement\"><a href=\"#Improvement\" class=\"headerlink\" title=\"Improvement\"></a>Improvement</h2><ul>\n<li><strong>bLSM</strong></li>\n<li><strong>Diff-Index LSM</strong></li>\n</ul>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li><a href=\"https://www.cnblogs.com/whuanle/p/16297025.html\">https://www.cnblogs.com/whuanle/p/16297025.html</a></li>\n<li><a href=\"https://www.geeksforgeeks.org/introduction-to-log-structured-merge-lsm-tree/\">https://www.geeksforgeeks.org/introduction-to-log-structured-merge-lsm-tree/</a></li>\n<li><a href=\"https://www.mongodb.com/docs/manual/core/index-sparse/#:~:text=Sparse%20indexes%20only%20contain%20entries,all%20documents%20of%20a%20collection\">https://www.mongodb.com/docs/manual/core/index-sparse/#:~:text=Sparse%20indexes%20only%20contain%20entries,all%20documents%20of%20a%20collection</a>.</li>\n<li><a href=\"https://hzhu212.github.io/posts/2d7c5edb/\">https://hzhu212.github.io/posts/2d7c5edb/</a></li>\n<li><a href=\"https://www.alibabacloud.com/blog/an-in-depth-discussion-on-the-lsm-compaction-mechanism_596780\">https://www.alibabacloud.com/blog/an-in-depth-discussion-on-the-lsm-compaction-mechanism_596780</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/380013595\">https://zhuanlan.zhihu.com/p/380013595</a></li>\n<li><a href=\"https://github.com/facebook/rocksdb/wiki/Iterator-Implementation\">https://github.com/facebook/rocksdb/wiki/Iterator-Implementation</a></li>\n</ol>\n"},{"title":"z-order","date":"2023-10-19T05:33:09.000Z","_content":"\n## What is Z-order & Why Z-order\n\nZ-order is a concept of sorting data. Specifically，to quote Wikipedia, `“Z-order maps multidimensional data to one dimension while preserving locality of the data points.”` In the field of big data, Columns in table are regarded as dimensions to some extension. In other words, Z-order brings us a better way to organize data.\n\nFirstly, let's think about two different ordered data, Z-order data and Linear-order data.\n\n![z-order data](z-order.png#pic_center)\n\n![linear-order data](linear-order.png#pic_center)\n\nIt is difficult for us to determine which one performs better immediately so there is a simple example to examine why z-order data may be the better one.\n\n\n| x | y | z |\n| :-: | :-: | :-: |\n| 0 | 0 | 1 |\n| 0 | 3 | 1 |\n| 1 | 0 | 1 |\n| 1 | 2 | 1 |\n| 2 | 1 | 1 |\n| 2 | 3 | 1 |\n| 3 | 0 | 1 |\n| 3 | 3 | 1 |\n\ndata in z-order:\n\n\n| x | y | z |\n| :-: | :-: | :-: |\n| 0 | 0 | 1 |\n| 1 | 0 | 1 |\n| 3 | 0 | 1 |\n| 2 | 1 | 1 |\n| 1 | 2 | 1 |\n| 0 | 3 | 1 |\n| 2 | 3 | 1 |\n| 3 | 3 | 1 |\n\nNow, think about executing a query as follows:\n\n1. `select sum(z) from test where x = 2`\n2. `select sum(z) from test where y = 2`\n\nHow do we know the query behavior of this SQL? Let's figure out it in clickhouse.\n\ndefault minmax index:\n\n\n| granule | x_min | x_max | y_min | y_max |\n| --------- | ------- | ------- | ------- | ------- |\n| 0       | 0     | 0     | 0     | 3     |\n| 1       | 1     | 1     | 0     | 2     |\n| 2       | 2     | 2     | 1     | 3     |\n| 3       | 3     | 3     | 0     | 3     |\n\nminmax index with z-order:\n\n\n| granule | x_min | x_max | y_min | y_max |\n| --------- | ------- | ------- | ------- | ------- |\n| 0       | 0     | 1     | 0     | 0     |\n| 1       | 2     | 3     | 0     | 1     |\n| 2       | 0     | 1     | 2     | 3     |\n| 3       | 2     | 3     | 3     | 3     |\n\nFor default minmax index, we filter only one granule when filtering `x=2` and  4 granules when filtering `y=2`. For z-order data, the results are\n2 and 2. It seems that z-order data may be litter better than default one but what if we have more than 3 dimensions filter conditions? Therefore, arranging data according to z-order can help us filter out data does not need to be read, achieving better dataskipping.\n\n## Z-order in OLTP & OLAP\n\nIn conclusion, Z-order in OLAP can be more popular than OLTP. There are two reasons as follows.\n\nOne is due to the order of physical data that data can only be ordered as one rule in disk. If we put data into disk ordered by primary key, it can't be ordered by another key. It means we can't get satisfying performance when filtering some other column as same as filtering sprimary key columns. Another one is we can create serveral B-Tree index in OLTP to replace the capabilities of z-order but OLAP not (because index in memory is too expensive for OLAP).\n\n## Extension: GeoHash VS Morton Code\n\n1. https://zhuanlan.zhihu.com/p/35940647\n2. https://zhuanlan.zhihu.com/p/468542418 (z-order code, but I don't understand:( )\n\n## Reference\n\n1. https://blog.cloudera.com/speeding-up-queries-with-z-order/\n2. https://izualzhy.cn/lakehouse-zorder\n3. https://juejin.cn/post/7118344872947515428\n4. https://zhuanlan.zhihu.com/p/491256487\n","source":"_posts/z-order.md","raw":"title: z-order\ndate: 2023-10-19 13:33:09\ntags: big data, index\n---------------------\n\n## What is Z-order & Why Z-order\n\nZ-order is a concept of sorting data. Specifically，to quote Wikipedia, `“Z-order maps multidimensional data to one dimension while preserving locality of the data points.”` In the field of big data, Columns in table are regarded as dimensions to some extension. In other words, Z-order brings us a better way to organize data.\n\nFirstly, let's think about two different ordered data, Z-order data and Linear-order data.\n\n![z-order data](z-order.png#pic_center)\n\n![linear-order data](linear-order.png#pic_center)\n\nIt is difficult for us to determine which one performs better immediately so there is a simple example to examine why z-order data may be the better one.\n\n\n| x | y | z |\n| :-: | :-: | :-: |\n| 0 | 0 | 1 |\n| 0 | 3 | 1 |\n| 1 | 0 | 1 |\n| 1 | 2 | 1 |\n| 2 | 1 | 1 |\n| 2 | 3 | 1 |\n| 3 | 0 | 1 |\n| 3 | 3 | 1 |\n\ndata in z-order:\n\n\n| x | y | z |\n| :-: | :-: | :-: |\n| 0 | 0 | 1 |\n| 1 | 0 | 1 |\n| 3 | 0 | 1 |\n| 2 | 1 | 1 |\n| 1 | 2 | 1 |\n| 0 | 3 | 1 |\n| 2 | 3 | 1 |\n| 3 | 3 | 1 |\n\nNow, think about executing a query as follows:\n\n1. `select sum(z) from test where x = 2`\n2. `select sum(z) from test where y = 2`\n\nHow do we know the query behavior of this SQL? Let's figure out it in clickhouse.\n\ndefault minmax index:\n\n\n| granule | x_min | x_max | y_min | y_max |\n| --------- | ------- | ------- | ------- | ------- |\n| 0       | 0     | 0     | 0     | 3     |\n| 1       | 1     | 1     | 0     | 2     |\n| 2       | 2     | 2     | 1     | 3     |\n| 3       | 3     | 3     | 0     | 3     |\n\nminmax index with z-order:\n\n\n| granule | x_min | x_max | y_min | y_max |\n| --------- | ------- | ------- | ------- | ------- |\n| 0       | 0     | 1     | 0     | 0     |\n| 1       | 2     | 3     | 0     | 1     |\n| 2       | 0     | 1     | 2     | 3     |\n| 3       | 2     | 3     | 3     | 3     |\n\nFor default minmax index, we filter only one granule when filtering `x=2` and  4 granules when filtering `y=2`. For z-order data, the results are\n2 and 2. It seems that z-order data may be litter better than default one but what if we have more than 3 dimensions filter conditions? Therefore, arranging data according to z-order can help us filter out data does not need to be read, achieving better dataskipping.\n\n## Z-order in OLTP & OLAP\n\nIn conclusion, Z-order in OLAP can be more popular than OLTP. There are two reasons as follows.\n\nOne is due to the order of physical data that data can only be ordered as one rule in disk. If we put data into disk ordered by primary key, it can't be ordered by another key. It means we can't get satisfying performance when filtering some other column as same as filtering sprimary key columns. Another one is we can create serveral B-Tree index in OLTP to replace the capabilities of z-order but OLAP not (because index in memory is too expensive for OLAP).\n\n## Extension: GeoHash VS Morton Code\n\n1. https://zhuanlan.zhihu.com/p/35940647\n2. https://zhuanlan.zhihu.com/p/468542418 (z-order code, but I don't understand:( )\n\n## Reference\n\n1. https://blog.cloudera.com/speeding-up-queries-with-z-order/\n2. https://izualzhy.cn/lakehouse-zorder\n3. https://juejin.cn/post/7118344872947515428\n4. https://zhuanlan.zhihu.com/p/491256487\n","slug":"z-order","published":1,"updated":"2023-10-23T01:58:15.691Z","_id":"clnwr6uki0003i5c3bh48ab32","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"What-is-Z-order-Why-Z-order\"><a href=\"#What-is-Z-order-Why-Z-order\" class=\"headerlink\" title=\"What is Z-order &amp; Why Z-order\"></a>What is Z-order &amp; Why Z-order</h2><p>Z-order is a concept of sorting data. Specifically，to quote Wikipedia, <code>“Z-order maps multidimensional data to one dimension while preserving locality of the data points.”</code> In the field of big data, Columns in table are regarded as dimensions to some extension. In other words, Z-order brings us a better way to organize data.</p>\n<p>Firstly, let’s think about two different ordered data, Z-order data and Linear-order data.</p>\n<p><img src=\"/2023/10/19/z-order/z-order.png#pic_center\" alt=\"z-order data\"></p>\n<p><img src=\"/2023/10/19/z-order/linear-order.png#pic_center\" alt=\"linear-order data\"></p>\n<p>It is difficult for us to determine which one performs better immediately so there is a simple example to examine why z-order data may be the better one.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">x</th>\n<th align=\"center\">y</th>\n<th align=\"center\">z</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">1</td>\n</tr>\n<tr>\n<td align=\"center\">0</td>\n<td align=\"center\">3</td>\n<td align=\"center\">1</td>\n</tr>\n<tr>\n<td align=\"center\">1</td>\n<td align=\"center\">0</td>\n<td align=\"center\">1</td>\n</tr>\n<tr>\n<td align=\"center\">1</td>\n<td align=\"center\">2</td>\n<td align=\"center\">1</td>\n</tr>\n<tr>\n<td align=\"center\">2</td>\n<td align=\"center\">1</td>\n<td align=\"center\">1</td>\n</tr>\n<tr>\n<td align=\"center\">2</td>\n<td align=\"center\">3</td>\n<td align=\"center\">1</td>\n</tr>\n<tr>\n<td align=\"center\">3</td>\n<td align=\"center\">0</td>\n<td align=\"center\">1</td>\n</tr>\n<tr>\n<td align=\"center\">3</td>\n<td align=\"center\">3</td>\n<td align=\"center\">1</td>\n</tr>\n</tbody></table>\n<p>data in z-order:</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">x</th>\n<th align=\"center\">y</th>\n<th align=\"center\">z</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">1</td>\n</tr>\n<tr>\n<td align=\"center\">1</td>\n<td align=\"center\">0</td>\n<td align=\"center\">1</td>\n</tr>\n<tr>\n<td align=\"center\">3</td>\n<td align=\"center\">0</td>\n<td align=\"center\">1</td>\n</tr>\n<tr>\n<td align=\"center\">2</td>\n<td align=\"center\">1</td>\n<td align=\"center\">1</td>\n</tr>\n<tr>\n<td align=\"center\">1</td>\n<td align=\"center\">2</td>\n<td align=\"center\">1</td>\n</tr>\n<tr>\n<td align=\"center\">0</td>\n<td align=\"center\">3</td>\n<td align=\"center\">1</td>\n</tr>\n<tr>\n<td align=\"center\">2</td>\n<td align=\"center\">3</td>\n<td align=\"center\">1</td>\n</tr>\n<tr>\n<td align=\"center\">3</td>\n<td align=\"center\">3</td>\n<td align=\"center\">1</td>\n</tr>\n</tbody></table>\n<p>Now, think about executing a query as follows:</p>\n<ol>\n<li><code>select sum(z) from test where x = 2</code></li>\n<li><code>select sum(z) from test where y = 2</code></li>\n</ol>\n<p>How do we know the query behavior of this SQL? Let’s figure out it in clickhouse.</p>\n<p>default minmax index:</p>\n<table>\n<thead>\n<tr>\n<th>granule</th>\n<th>x_min</th>\n<th>x_max</th>\n<th>y_min</th>\n<th>y_max</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>3</td>\n</tr>\n<tr>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>0</td>\n<td>2</td>\n</tr>\n<tr>\n<td>2</td>\n<td>2</td>\n<td>2</td>\n<td>1</td>\n<td>3</td>\n</tr>\n<tr>\n<td>3</td>\n<td>3</td>\n<td>3</td>\n<td>0</td>\n<td>3</td>\n</tr>\n</tbody></table>\n<p>minmax index with z-order:</p>\n<table>\n<thead>\n<tr>\n<th>granule</th>\n<th>x_min</th>\n<th>x_max</th>\n<th>y_min</th>\n<th>y_max</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n<td>0</td>\n<td>0</td>\n</tr>\n<tr>\n<td>1</td>\n<td>2</td>\n<td>3</td>\n<td>0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>2</td>\n<td>0</td>\n<td>1</td>\n<td>2</td>\n<td>3</td>\n</tr>\n<tr>\n<td>3</td>\n<td>2</td>\n<td>3</td>\n<td>3</td>\n<td>3</td>\n</tr>\n</tbody></table>\n<p>For default minmax index, we filter only one granule when filtering <code>x=2</code> and  4 granules when filtering <code>y=2</code>. For z-order data, the results are<br>2 and 2. It seems that z-order data may be litter better than default one but what if we have more than 3 dimensions filter conditions? Therefore, arranging data according to z-order can help us filter out data does not need to be read, achieving better dataskipping.</p>\n<h2 id=\"Z-order-in-OLTP-OLAP\"><a href=\"#Z-order-in-OLTP-OLAP\" class=\"headerlink\" title=\"Z-order in OLTP &amp; OLAP\"></a>Z-order in OLTP &amp; OLAP</h2><p>In conclusion, Z-order in OLAP can be more popular than OLTP. There are two reasons as follows.</p>\n<p>One is due to the order of physical data that data can only be ordered as one rule in disk. If we put data into disk ordered by primary key, it can’t be ordered by another key. It means we can’t get satisfying performance when filtering some other column as same as filtering sprimary key columns. Another one is we can create serveral B-Tree index in OLTP to replace the capabilities of z-order but OLAP not (because index in memory is too expensive for OLAP).</p>\n<h2 id=\"Extension-GeoHash-VS-Morton-Code\"><a href=\"#Extension-GeoHash-VS-Morton-Code\" class=\"headerlink\" title=\"Extension: GeoHash VS Morton Code\"></a>Extension: GeoHash VS Morton Code</h2><ol>\n<li><a href=\"https://zhuanlan.zhihu.com/p/35940647\">https://zhuanlan.zhihu.com/p/35940647</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/468542418\">https://zhuanlan.zhihu.com/p/468542418</a> (z-order code, but I don’t understand:( )</li>\n</ol>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li><a href=\"https://blog.cloudera.com/speeding-up-queries-with-z-order/\">https://blog.cloudera.com/speeding-up-queries-with-z-order/</a></li>\n<li><a href=\"https://izualzhy.cn/lakehouse-zorder\">https://izualzhy.cn/lakehouse-zorder</a></li>\n<li><a href=\"https://juejin.cn/post/7118344872947515428\">https://juejin.cn/post/7118344872947515428</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/491256487\">https://zhuanlan.zhihu.com/p/491256487</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"What-is-Z-order-Why-Z-order\"><a href=\"#What-is-Z-order-Why-Z-order\" class=\"headerlink\" title=\"What is Z-order &amp; Why Z-order\"></a>What is Z-order &amp; Why Z-order</h2><p>Z-order is a concept of sorting data. Specifically，to quote Wikipedia, <code>“Z-order maps multidimensional data to one dimension while preserving locality of the data points.”</code> In the field of big data, Columns in table are regarded as dimensions to some extension. In other words, Z-order brings us a better way to organize data.</p>\n<p>Firstly, let’s think about two different ordered data, Z-order data and Linear-order data.</p>\n<p><img src=\"/2023/10/19/z-order/z-order.png#pic_center\" alt=\"z-order data\"></p>\n<p><img src=\"/2023/10/19/z-order/linear-order.png#pic_center\" alt=\"linear-order data\"></p>\n<p>It is difficult for us to determine which one performs better immediately so there is a simple example to examine why z-order data may be the better one.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">x</th>\n<th align=\"center\">y</th>\n<th align=\"center\">z</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">1</td>\n</tr>\n<tr>\n<td align=\"center\">0</td>\n<td align=\"center\">3</td>\n<td align=\"center\">1</td>\n</tr>\n<tr>\n<td align=\"center\">1</td>\n<td align=\"center\">0</td>\n<td align=\"center\">1</td>\n</tr>\n<tr>\n<td align=\"center\">1</td>\n<td align=\"center\">2</td>\n<td align=\"center\">1</td>\n</tr>\n<tr>\n<td align=\"center\">2</td>\n<td align=\"center\">1</td>\n<td align=\"center\">1</td>\n</tr>\n<tr>\n<td align=\"center\">2</td>\n<td align=\"center\">3</td>\n<td align=\"center\">1</td>\n</tr>\n<tr>\n<td align=\"center\">3</td>\n<td align=\"center\">0</td>\n<td align=\"center\">1</td>\n</tr>\n<tr>\n<td align=\"center\">3</td>\n<td align=\"center\">3</td>\n<td align=\"center\">1</td>\n</tr>\n</tbody></table>\n<p>data in z-order:</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">x</th>\n<th align=\"center\">y</th>\n<th align=\"center\">z</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">1</td>\n</tr>\n<tr>\n<td align=\"center\">1</td>\n<td align=\"center\">0</td>\n<td align=\"center\">1</td>\n</tr>\n<tr>\n<td align=\"center\">3</td>\n<td align=\"center\">0</td>\n<td align=\"center\">1</td>\n</tr>\n<tr>\n<td align=\"center\">2</td>\n<td align=\"center\">1</td>\n<td align=\"center\">1</td>\n</tr>\n<tr>\n<td align=\"center\">1</td>\n<td align=\"center\">2</td>\n<td align=\"center\">1</td>\n</tr>\n<tr>\n<td align=\"center\">0</td>\n<td align=\"center\">3</td>\n<td align=\"center\">1</td>\n</tr>\n<tr>\n<td align=\"center\">2</td>\n<td align=\"center\">3</td>\n<td align=\"center\">1</td>\n</tr>\n<tr>\n<td align=\"center\">3</td>\n<td align=\"center\">3</td>\n<td align=\"center\">1</td>\n</tr>\n</tbody></table>\n<p>Now, think about executing a query as follows:</p>\n<ol>\n<li><code>select sum(z) from test where x = 2</code></li>\n<li><code>select sum(z) from test where y = 2</code></li>\n</ol>\n<p>How do we know the query behavior of this SQL? Let’s figure out it in clickhouse.</p>\n<p>default minmax index:</p>\n<table>\n<thead>\n<tr>\n<th>granule</th>\n<th>x_min</th>\n<th>x_max</th>\n<th>y_min</th>\n<th>y_max</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>3</td>\n</tr>\n<tr>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>0</td>\n<td>2</td>\n</tr>\n<tr>\n<td>2</td>\n<td>2</td>\n<td>2</td>\n<td>1</td>\n<td>3</td>\n</tr>\n<tr>\n<td>3</td>\n<td>3</td>\n<td>3</td>\n<td>0</td>\n<td>3</td>\n</tr>\n</tbody></table>\n<p>minmax index with z-order:</p>\n<table>\n<thead>\n<tr>\n<th>granule</th>\n<th>x_min</th>\n<th>x_max</th>\n<th>y_min</th>\n<th>y_max</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n<td>0</td>\n<td>0</td>\n</tr>\n<tr>\n<td>1</td>\n<td>2</td>\n<td>3</td>\n<td>0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>2</td>\n<td>0</td>\n<td>1</td>\n<td>2</td>\n<td>3</td>\n</tr>\n<tr>\n<td>3</td>\n<td>2</td>\n<td>3</td>\n<td>3</td>\n<td>3</td>\n</tr>\n</tbody></table>\n<p>For default minmax index, we filter only one granule when filtering <code>x=2</code> and  4 granules when filtering <code>y=2</code>. For z-order data, the results are<br>2 and 2. It seems that z-order data may be litter better than default one but what if we have more than 3 dimensions filter conditions? Therefore, arranging data according to z-order can help us filter out data does not need to be read, achieving better dataskipping.</p>\n<h2 id=\"Z-order-in-OLTP-OLAP\"><a href=\"#Z-order-in-OLTP-OLAP\" class=\"headerlink\" title=\"Z-order in OLTP &amp; OLAP\"></a>Z-order in OLTP &amp; OLAP</h2><p>In conclusion, Z-order in OLAP can be more popular than OLTP. There are two reasons as follows.</p>\n<p>One is due to the order of physical data that data can only be ordered as one rule in disk. If we put data into disk ordered by primary key, it can’t be ordered by another key. It means we can’t get satisfying performance when filtering some other column as same as filtering sprimary key columns. Another one is we can create serveral B-Tree index in OLTP to replace the capabilities of z-order but OLAP not (because index in memory is too expensive for OLAP).</p>\n<h2 id=\"Extension-GeoHash-VS-Morton-Code\"><a href=\"#Extension-GeoHash-VS-Morton-Code\" class=\"headerlink\" title=\"Extension: GeoHash VS Morton Code\"></a>Extension: GeoHash VS Morton Code</h2><ol>\n<li><a href=\"https://zhuanlan.zhihu.com/p/35940647\">https://zhuanlan.zhihu.com/p/35940647</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/468542418\">https://zhuanlan.zhihu.com/p/468542418</a> (z-order code, but I don’t understand:( )</li>\n</ol>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li><a href=\"https://blog.cloudera.com/speeding-up-queries-with-z-order/\">https://blog.cloudera.com/speeding-up-queries-with-z-order/</a></li>\n<li><a href=\"https://izualzhy.cn/lakehouse-zorder\">https://izualzhy.cn/lakehouse-zorder</a></li>\n<li><a href=\"https://juejin.cn/post/7118344872947515428\">https://juejin.cn/post/7118344872947515428</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/491256487\">https://zhuanlan.zhihu.com/p/491256487</a></li>\n</ol>\n"},{"title":"Bloom Filter Implementation and Optimization","date":"2023-10-02T11:11:52.000Z","mathjax":true,"_content":"# Bloom Filter\n\n## What is Bloom Filter\n\nA bloom filter is a probabilistic data structure that is based on hashing. It is extremely space efficient and is typically used to add elements to a set and test if an element is in a set. Though, the elements themselves are not added to a set. Instead a hash of the elements is added to the set.\n\n## Implementation\n\n```python\nimport hashlib\n\nclass BloomFilter:\n    def __init__(self, m, k):\n        self.m = m\n        self.k = k\n        self.data = [0]*m\n        self.n = 0\n    def insert(self, element):\n        if self.k == 1:\n            hash1 = h1(element) % self.m\n            self.data[hash1] = 1\n        elif self.k == 2:\n            hash1 = h1(element) % self.m\n            hash2 = h2(element) % self.m\n            self.data[hash1] = 1\n            self.data[hash2] = 1\n        self.n += 1\n    def search(self, element):\n        if self.k == 1:\n            hash1 = h1(element) % self.m\n            if self.data[hash1] == 0:\n                return \"Not in Bloom Filter\"\n        elif self.k == 2:\n            hash1 = h1(element) % self.m\n            hash2 = h2(element) % self.m\n            if self.data[hash1] == 0 or self.data[hash2] == 0:\n                return \"Not in Bloom Filter\"\n        prob = (1.0 - ((1.0 - 1.0/self.m) * (self.k*self.n))) * self.k\n        return \"Might be in Bloom Filter with false positive probability \"+str(prob)\n\ndef h1(w):\n    h = hashlib.md5(w)\n    return hash(h.digest().encode('base64')[:6])%10\n\ndef h2(w):\n    h = hashlib.sha256(w)\n    return hash(h.digest().encode('base64')[:6])%10\n```\n\nIn this sample, we implement a simple Bloom Filter which has two hash function(controlled by *k*) to compute the position of input element in the bit array (named data, array size is *m*).\n\nWhen an element inserts into BloomFilter, two positions will be changed to 1 in the bit array computed by hash function so that if the element comes again, BloomFilter will get and check if the two positions are both 1. So when other element comes, if any position is not 1, the element doesn't exist in the data array.\n\nIt is worth noting that different elements can get same hash values computed by hash function. It means one grid in the bit array is not independently used by unique element but shared. In other words, it can be covered.\n\n## drawback\n\nAs mentioned above, Bloom Filter can creat false positives that Bloom Filter has a certain probability of mistakenly identifying non-existent elements.\n\nWe have two choices of parameters when building a bloom filter, `m` and `k`. They should each be chosen to dampen the number of false positives as much as possible while still maintaining whatever space requirement the filter needs.\n\nIf we have a bloom filter with `m` bits and `k` hash functions, the probability that a certain bit will still be zero after one insertion is\n\n$(1-1/m)^k$\n\nThen, after `n` insertions, the probability of it still being zero after `n` insertions is\n\n$(1-1/m)^{nk}$\n\nSo, that means the probability of a false positive is\n\n$(1-(1-1/m)^{nk})^k$\n\nIn each of these equations, raising the value of k (the number of hash functions) will make the probability of a false positive less likely. However, it is not computationally efficient to have an enormous value for *k*. To minimize this equation, we must choose the best *k*. We do it this way because we assume that the programmer has already chosen an **m** based on their space constraints and that they have some idea what their potential *n* will be. So the *k* value that minimizes that equation is\n\n$k=ln(2)⋅m/n$\n\n## Use Cases\n\n### Hudi Upsert\n\n### LSM-Tree\n\nSee [LSM-Tree](https://adooobe.github.io/2023/10/02/lsm-tree/)\n\n### Others\n\n1. [White List Question](https://zhuanlan.zhihu.com/p/294069121)\n2. [Redis Cache Breakdown](https://www.51cto.com/article/753025.html)\n\n## Improvement\n\nThere are two limitations that have always restricted the usage of Bloom Filter, bounded source and append-only. The following are several improvement methods that revolve around addressing these two issues.\n\n#### Scalable Bloom Filters (SBF)\n\n![scalable bloom filter structure](scalable_bloom_filter.png#pic_center)\n\nwhen the filter reaches some fulfillness threshold, it becomes read-only and new bigger and writable filter is created in its place. If in its turn it becomes saturated, the operation is repeated. Every new filter, in order to keep the false positives rate close to the targeted one, has more hash functions than the previous filter.\n\nIn Scalable Bloom filter the membership test is applied on all created (read-only + writable) filters. If the item is missing in all filters, it's considered as not existing. In the other side, if one of the filters reports its existence, it means that the element may be in the dataset. An important point to notice here is that Scalable Bloom filter uses a variant of Bloom filters where the bit vector is divided in *k* slices where each stores *M/k* bits (*M* is the size of whole bit vector). Since the number of slices is equal to the number of hash functions, each hash function works on its own slice:\n\n![scalable bloom filter slices](scalable_bloom_filter_slices.png#pic_center)\n\nThanks to the slices each element is always described by *k* bits resulting on more robust filter without the elements more prone to the false positives than the others.\n\n#### Counting Bloom Filter (CBF)\n\nFor the second question, CBF provides ability to delete elements in Bloom Filters. But unfortunately, the premise is that we must ensure that the deleted element is present in the Bloom filter.\n\n![counting bloom filter structure](counting_bloom_filter.png#pic_center)\n\n## Reference\n\n1. https://www.waitingforcode.com/big-data-algorithms/scalable-bloom-filter/read\n2. Baquero, C., & Almeida, J. (2007, January). Scalable bloom filters. In European Conference on Principles of Data Mining and Knowledge Discovery (pp. 244-256). Springer, Berlin, Heidelberg.\n","source":"_posts/bloom.md","raw":"---\ntitle: Bloom Filter Implementation and Optimization\ndate: 2023-10-02 19:11:52\ntags: Bloom-Filter\nmathjax: true\n---\n# Bloom Filter\n\n## What is Bloom Filter\n\nA bloom filter is a probabilistic data structure that is based on hashing. It is extremely space efficient and is typically used to add elements to a set and test if an element is in a set. Though, the elements themselves are not added to a set. Instead a hash of the elements is added to the set.\n\n## Implementation\n\n```python\nimport hashlib\n\nclass BloomFilter:\n    def __init__(self, m, k):\n        self.m = m\n        self.k = k\n        self.data = [0]*m\n        self.n = 0\n    def insert(self, element):\n        if self.k == 1:\n            hash1 = h1(element) % self.m\n            self.data[hash1] = 1\n        elif self.k == 2:\n            hash1 = h1(element) % self.m\n            hash2 = h2(element) % self.m\n            self.data[hash1] = 1\n            self.data[hash2] = 1\n        self.n += 1\n    def search(self, element):\n        if self.k == 1:\n            hash1 = h1(element) % self.m\n            if self.data[hash1] == 0:\n                return \"Not in Bloom Filter\"\n        elif self.k == 2:\n            hash1 = h1(element) % self.m\n            hash2 = h2(element) % self.m\n            if self.data[hash1] == 0 or self.data[hash2] == 0:\n                return \"Not in Bloom Filter\"\n        prob = (1.0 - ((1.0 - 1.0/self.m) * (self.k*self.n))) * self.k\n        return \"Might be in Bloom Filter with false positive probability \"+str(prob)\n\ndef h1(w):\n    h = hashlib.md5(w)\n    return hash(h.digest().encode('base64')[:6])%10\n\ndef h2(w):\n    h = hashlib.sha256(w)\n    return hash(h.digest().encode('base64')[:6])%10\n```\n\nIn this sample, we implement a simple Bloom Filter which has two hash function(controlled by *k*) to compute the position of input element in the bit array (named data, array size is *m*).\n\nWhen an element inserts into BloomFilter, two positions will be changed to 1 in the bit array computed by hash function so that if the element comes again, BloomFilter will get and check if the two positions are both 1. So when other element comes, if any position is not 1, the element doesn't exist in the data array.\n\nIt is worth noting that different elements can get same hash values computed by hash function. It means one grid in the bit array is not independently used by unique element but shared. In other words, it can be covered.\n\n## drawback\n\nAs mentioned above, Bloom Filter can creat false positives that Bloom Filter has a certain probability of mistakenly identifying non-existent elements.\n\nWe have two choices of parameters when building a bloom filter, `m` and `k`. They should each be chosen to dampen the number of false positives as much as possible while still maintaining whatever space requirement the filter needs.\n\nIf we have a bloom filter with `m` bits and `k` hash functions, the probability that a certain bit will still be zero after one insertion is\n\n$(1-1/m)^k$\n\nThen, after `n` insertions, the probability of it still being zero after `n` insertions is\n\n$(1-1/m)^{nk}$\n\nSo, that means the probability of a false positive is\n\n$(1-(1-1/m)^{nk})^k$\n\nIn each of these equations, raising the value of k (the number of hash functions) will make the probability of a false positive less likely. However, it is not computationally efficient to have an enormous value for *k*. To minimize this equation, we must choose the best *k*. We do it this way because we assume that the programmer has already chosen an **m** based on their space constraints and that they have some idea what their potential *n* will be. So the *k* value that minimizes that equation is\n\n$k=ln(2)⋅m/n$\n\n## Use Cases\n\n### Hudi Upsert\n\n### LSM-Tree\n\nSee [LSM-Tree](https://adooobe.github.io/2023/10/02/lsm-tree/)\n\n### Others\n\n1. [White List Question](https://zhuanlan.zhihu.com/p/294069121)\n2. [Redis Cache Breakdown](https://www.51cto.com/article/753025.html)\n\n## Improvement\n\nThere are two limitations that have always restricted the usage of Bloom Filter, bounded source and append-only. The following are several improvement methods that revolve around addressing these two issues.\n\n#### Scalable Bloom Filters (SBF)\n\n![scalable bloom filter structure](scalable_bloom_filter.png#pic_center)\n\nwhen the filter reaches some fulfillness threshold, it becomes read-only and new bigger and writable filter is created in its place. If in its turn it becomes saturated, the operation is repeated. Every new filter, in order to keep the false positives rate close to the targeted one, has more hash functions than the previous filter.\n\nIn Scalable Bloom filter the membership test is applied on all created (read-only + writable) filters. If the item is missing in all filters, it's considered as not existing. In the other side, if one of the filters reports its existence, it means that the element may be in the dataset. An important point to notice here is that Scalable Bloom filter uses a variant of Bloom filters where the bit vector is divided in *k* slices where each stores *M/k* bits (*M* is the size of whole bit vector). Since the number of slices is equal to the number of hash functions, each hash function works on its own slice:\n\n![scalable bloom filter slices](scalable_bloom_filter_slices.png#pic_center)\n\nThanks to the slices each element is always described by *k* bits resulting on more robust filter without the elements more prone to the false positives than the others.\n\n#### Counting Bloom Filter (CBF)\n\nFor the second question, CBF provides ability to delete elements in Bloom Filters. But unfortunately, the premise is that we must ensure that the deleted element is present in the Bloom filter.\n\n![counting bloom filter structure](counting_bloom_filter.png#pic_center)\n\n## Reference\n\n1. https://www.waitingforcode.com/big-data-algorithms/scalable-bloom-filter/read\n2. Baquero, C., & Almeida, J. (2007, January). Scalable bloom filters. In European Conference on Principles of Data Mining and Knowledge Discovery (pp. 244-256). Springer, Berlin, Heidelberg.\n","slug":"bloom","published":1,"updated":"2023-10-03T08:44:27.406Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clnwr6ukk0009i5c302xp0f9e","content":"<h1 id=\"Bloom-Filter\"><a href=\"#Bloom-Filter\" class=\"headerlink\" title=\"Bloom Filter\"></a>Bloom Filter</h1><h2 id=\"What-is-Bloom-Filter\"><a href=\"#What-is-Bloom-Filter\" class=\"headerlink\" title=\"What is Bloom Filter\"></a>What is Bloom Filter</h2><p>A bloom filter is a probabilistic data structure that is based on hashing. It is extremely space efficient and is typically used to add elements to a set and test if an element is in a set. Though, the elements themselves are not added to a set. Instead a hash of the elements is added to the set.</p>\n<h2 id=\"Implementation\"><a href=\"#Implementation\" class=\"headerlink\" title=\"Implementation\"></a>Implementation</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> hashlib</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">BloomFilter</span>:</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, m, k</span>):</span><br><span class=\"line\">        self.m = m</span><br><span class=\"line\">        self.k = k</span><br><span class=\"line\">        self.data = [<span class=\"number\">0</span>]*m</span><br><span class=\"line\">        self.n = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">insert</span>(<span class=\"params\">self, element</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.k == <span class=\"number\">1</span>:</span><br><span class=\"line\">            hash1 = h1(element) % self.m</span><br><span class=\"line\">            self.data[hash1] = <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">elif</span> self.k == <span class=\"number\">2</span>:</span><br><span class=\"line\">            hash1 = h1(element) % self.m</span><br><span class=\"line\">            hash2 = h2(element) % self.m</span><br><span class=\"line\">            self.data[hash1] = <span class=\"number\">1</span></span><br><span class=\"line\">            self.data[hash2] = <span class=\"number\">1</span></span><br><span class=\"line\">        self.n += <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">search</span>(<span class=\"params\">self, element</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.k == <span class=\"number\">1</span>:</span><br><span class=\"line\">            hash1 = h1(element) % self.m</span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.data[hash1] == <span class=\"number\">0</span>:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"string\">&quot;Not in Bloom Filter&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">elif</span> self.k == <span class=\"number\">2</span>:</span><br><span class=\"line\">            hash1 = h1(element) % self.m</span><br><span class=\"line\">            hash2 = h2(element) % self.m</span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.data[hash1] == <span class=\"number\">0</span> <span class=\"keyword\">or</span> self.data[hash2] == <span class=\"number\">0</span>:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"string\">&quot;Not in Bloom Filter&quot;</span></span><br><span class=\"line\">        prob = (<span class=\"number\">1.0</span> - ((<span class=\"number\">1.0</span> - <span class=\"number\">1.0</span>/self.m) * (self.k*self.n))) * self.k</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">&quot;Might be in Bloom Filter with false positive probability &quot;</span>+<span class=\"built_in\">str</span>(prob)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">h1</span>(<span class=\"params\">w</span>):</span><br><span class=\"line\">    h = hashlib.md5(w)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">hash</span>(h.digest().encode(<span class=\"string\">&#x27;base64&#x27;</span>)[:<span class=\"number\">6</span>])%<span class=\"number\">10</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">h2</span>(<span class=\"params\">w</span>):</span><br><span class=\"line\">    h = hashlib.sha256(w)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">hash</span>(h.digest().encode(<span class=\"string\">&#x27;base64&#x27;</span>)[:<span class=\"number\">6</span>])%<span class=\"number\">10</span></span><br></pre></td></tr></table></figure>\n\n<p>In this sample, we implement a simple Bloom Filter which has two hash function(controlled by <em>k</em>) to compute the position of input element in the bit array (named data, array size is <em>m</em>).</p>\n<p>When an element inserts into BloomFilter, two positions will be changed to 1 in the bit array computed by hash function so that if the element comes again, BloomFilter will get and check if the two positions are both 1. So when other element comes, if any position is not 1, the element doesn’t exist in the data array.</p>\n<p>It is worth noting that different elements can get same hash values computed by hash function. It means one grid in the bit array is not independently used by unique element but shared. In other words, it can be covered.</p>\n<h2 id=\"drawback\"><a href=\"#drawback\" class=\"headerlink\" title=\"drawback\"></a>drawback</h2><p>As mentioned above, Bloom Filter can creat false positives that Bloom Filter has a certain probability of mistakenly identifying non-existent elements.</p>\n<p>We have two choices of parameters when building a bloom filter, <code>m</code> and <code>k</code>. They should each be chosen to dampen the number of false positives as much as possible while still maintaining whatever space requirement the filter needs.</p>\n<p>If we have a bloom filter with <code>m</code> bits and <code>k</code> hash functions, the probability that a certain bit will still be zero after one insertion is</p>\n<p>$(1-1&#x2F;m)^k$</p>\n<p>Then, after <code>n</code> insertions, the probability of it still being zero after <code>n</code> insertions is</p>\n<p>$(1-1&#x2F;m)^{nk}$</p>\n<p>So, that means the probability of a false positive is</p>\n<p>$(1-(1-1&#x2F;m)^{nk})^k$</p>\n<p>In each of these equations, raising the value of k (the number of hash functions) will make the probability of a false positive less likely. However, it is not computationally efficient to have an enormous value for <em>k</em>. To minimize this equation, we must choose the best <em>k</em>. We do it this way because we assume that the programmer has already chosen an <strong>m</strong> based on their space constraints and that they have some idea what their potential <em>n</em> will be. So the <em>k</em> value that minimizes that equation is</p>\n<p>$k&#x3D;ln(2)⋅m&#x2F;n$</p>\n<h2 id=\"Use-Cases\"><a href=\"#Use-Cases\" class=\"headerlink\" title=\"Use Cases\"></a>Use Cases</h2><h3 id=\"Hudi-Upsert\"><a href=\"#Hudi-Upsert\" class=\"headerlink\" title=\"Hudi Upsert\"></a>Hudi Upsert</h3><h3 id=\"LSM-Tree\"><a href=\"#LSM-Tree\" class=\"headerlink\" title=\"LSM-Tree\"></a>LSM-Tree</h3><p>See <a href=\"https://adooobe.github.io/2023/10/02/lsm-tree/\">LSM-Tree</a></p>\n<h3 id=\"Others\"><a href=\"#Others\" class=\"headerlink\" title=\"Others\"></a>Others</h3><ol>\n<li><a href=\"https://zhuanlan.zhihu.com/p/294069121\">White List Question</a></li>\n<li><a href=\"https://www.51cto.com/article/753025.html\">Redis Cache Breakdown</a></li>\n</ol>\n<h2 id=\"Improvement\"><a href=\"#Improvement\" class=\"headerlink\" title=\"Improvement\"></a>Improvement</h2><p>There are two limitations that have always restricted the usage of Bloom Filter, bounded source and append-only. The following are several improvement methods that revolve around addressing these two issues.</p>\n<h4 id=\"Scalable-Bloom-Filters-SBF\"><a href=\"#Scalable-Bloom-Filters-SBF\" class=\"headerlink\" title=\"Scalable Bloom Filters (SBF)\"></a>Scalable Bloom Filters (SBF)</h4><p><img src=\"/2023/10/02/bloom/scalable_bloom_filter.png#pic_center\" alt=\"scalable bloom filter structure\"></p>\n<p>when the filter reaches some fulfillness threshold, it becomes read-only and new bigger and writable filter is created in its place. If in its turn it becomes saturated, the operation is repeated. Every new filter, in order to keep the false positives rate close to the targeted one, has more hash functions than the previous filter.</p>\n<p>In Scalable Bloom filter the membership test is applied on all created (read-only + writable) filters. If the item is missing in all filters, it’s considered as not existing. In the other side, if one of the filters reports its existence, it means that the element may be in the dataset. An important point to notice here is that Scalable Bloom filter uses a variant of Bloom filters where the bit vector is divided in <em>k</em> slices where each stores <em>M&#x2F;k</em> bits (<em>M</em> is the size of whole bit vector). Since the number of slices is equal to the number of hash functions, each hash function works on its own slice:</p>\n<p><img src=\"/2023/10/02/bloom/scalable_bloom_filter_slices.png#pic_center\" alt=\"scalable bloom filter slices\"></p>\n<p>Thanks to the slices each element is always described by <em>k</em> bits resulting on more robust filter without the elements more prone to the false positives than the others.</p>\n<h4 id=\"Counting-Bloom-Filter-CBF\"><a href=\"#Counting-Bloom-Filter-CBF\" class=\"headerlink\" title=\"Counting Bloom Filter (CBF)\"></a>Counting Bloom Filter (CBF)</h4><p>For the second question, CBF provides ability to delete elements in Bloom Filters. But unfortunately, the premise is that we must ensure that the deleted element is present in the Bloom filter.</p>\n<p><img src=\"/2023/10/02/bloom/counting_bloom_filter.png#pic_center\" alt=\"counting bloom filter structure\"></p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li><a href=\"https://www.waitingforcode.com/big-data-algorithms/scalable-bloom-filter/read\">https://www.waitingforcode.com/big-data-algorithms/scalable-bloom-filter/read</a></li>\n<li>Baquero, C., &amp; Almeida, J. (2007, January). Scalable bloom filters. In European Conference on Principles of Data Mining and Knowledge Discovery (pp. 244-256). Springer, Berlin, Heidelberg.</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Bloom-Filter\"><a href=\"#Bloom-Filter\" class=\"headerlink\" title=\"Bloom Filter\"></a>Bloom Filter</h1><h2 id=\"What-is-Bloom-Filter\"><a href=\"#What-is-Bloom-Filter\" class=\"headerlink\" title=\"What is Bloom Filter\"></a>What is Bloom Filter</h2><p>A bloom filter is a probabilistic data structure that is based on hashing. It is extremely space efficient and is typically used to add elements to a set and test if an element is in a set. Though, the elements themselves are not added to a set. Instead a hash of the elements is added to the set.</p>\n<h2 id=\"Implementation\"><a href=\"#Implementation\" class=\"headerlink\" title=\"Implementation\"></a>Implementation</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> hashlib</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">BloomFilter</span>:</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, m, k</span>):</span><br><span class=\"line\">        self.m = m</span><br><span class=\"line\">        self.k = k</span><br><span class=\"line\">        self.data = [<span class=\"number\">0</span>]*m</span><br><span class=\"line\">        self.n = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">insert</span>(<span class=\"params\">self, element</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.k == <span class=\"number\">1</span>:</span><br><span class=\"line\">            hash1 = h1(element) % self.m</span><br><span class=\"line\">            self.data[hash1] = <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">elif</span> self.k == <span class=\"number\">2</span>:</span><br><span class=\"line\">            hash1 = h1(element) % self.m</span><br><span class=\"line\">            hash2 = h2(element) % self.m</span><br><span class=\"line\">            self.data[hash1] = <span class=\"number\">1</span></span><br><span class=\"line\">            self.data[hash2] = <span class=\"number\">1</span></span><br><span class=\"line\">        self.n += <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">search</span>(<span class=\"params\">self, element</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.k == <span class=\"number\">1</span>:</span><br><span class=\"line\">            hash1 = h1(element) % self.m</span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.data[hash1] == <span class=\"number\">0</span>:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"string\">&quot;Not in Bloom Filter&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">elif</span> self.k == <span class=\"number\">2</span>:</span><br><span class=\"line\">            hash1 = h1(element) % self.m</span><br><span class=\"line\">            hash2 = h2(element) % self.m</span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.data[hash1] == <span class=\"number\">0</span> <span class=\"keyword\">or</span> self.data[hash2] == <span class=\"number\">0</span>:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"string\">&quot;Not in Bloom Filter&quot;</span></span><br><span class=\"line\">        prob = (<span class=\"number\">1.0</span> - ((<span class=\"number\">1.0</span> - <span class=\"number\">1.0</span>/self.m) * (self.k*self.n))) * self.k</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">&quot;Might be in Bloom Filter with false positive probability &quot;</span>+<span class=\"built_in\">str</span>(prob)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">h1</span>(<span class=\"params\">w</span>):</span><br><span class=\"line\">    h = hashlib.md5(w)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">hash</span>(h.digest().encode(<span class=\"string\">&#x27;base64&#x27;</span>)[:<span class=\"number\">6</span>])%<span class=\"number\">10</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">h2</span>(<span class=\"params\">w</span>):</span><br><span class=\"line\">    h = hashlib.sha256(w)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">hash</span>(h.digest().encode(<span class=\"string\">&#x27;base64&#x27;</span>)[:<span class=\"number\">6</span>])%<span class=\"number\">10</span></span><br></pre></td></tr></table></figure>\n\n<p>In this sample, we implement a simple Bloom Filter which has two hash function(controlled by <em>k</em>) to compute the position of input element in the bit array (named data, array size is <em>m</em>).</p>\n<p>When an element inserts into BloomFilter, two positions will be changed to 1 in the bit array computed by hash function so that if the element comes again, BloomFilter will get and check if the two positions are both 1. So when other element comes, if any position is not 1, the element doesn’t exist in the data array.</p>\n<p>It is worth noting that different elements can get same hash values computed by hash function. It means one grid in the bit array is not independently used by unique element but shared. In other words, it can be covered.</p>\n<h2 id=\"drawback\"><a href=\"#drawback\" class=\"headerlink\" title=\"drawback\"></a>drawback</h2><p>As mentioned above, Bloom Filter can creat false positives that Bloom Filter has a certain probability of mistakenly identifying non-existent elements.</p>\n<p>We have two choices of parameters when building a bloom filter, <code>m</code> and <code>k</code>. They should each be chosen to dampen the number of false positives as much as possible while still maintaining whatever space requirement the filter needs.</p>\n<p>If we have a bloom filter with <code>m</code> bits and <code>k</code> hash functions, the probability that a certain bit will still be zero after one insertion is</p>\n<p>$(1-1&#x2F;m)^k$</p>\n<p>Then, after <code>n</code> insertions, the probability of it still being zero after <code>n</code> insertions is</p>\n<p>$(1-1&#x2F;m)^{nk}$</p>\n<p>So, that means the probability of a false positive is</p>\n<p>$(1-(1-1&#x2F;m)^{nk})^k$</p>\n<p>In each of these equations, raising the value of k (the number of hash functions) will make the probability of a false positive less likely. However, it is not computationally efficient to have an enormous value for <em>k</em>. To minimize this equation, we must choose the best <em>k</em>. We do it this way because we assume that the programmer has already chosen an <strong>m</strong> based on their space constraints and that they have some idea what their potential <em>n</em> will be. So the <em>k</em> value that minimizes that equation is</p>\n<p>$k&#x3D;ln(2)⋅m&#x2F;n$</p>\n<h2 id=\"Use-Cases\"><a href=\"#Use-Cases\" class=\"headerlink\" title=\"Use Cases\"></a>Use Cases</h2><h3 id=\"Hudi-Upsert\"><a href=\"#Hudi-Upsert\" class=\"headerlink\" title=\"Hudi Upsert\"></a>Hudi Upsert</h3><h3 id=\"LSM-Tree\"><a href=\"#LSM-Tree\" class=\"headerlink\" title=\"LSM-Tree\"></a>LSM-Tree</h3><p>See <a href=\"https://adooobe.github.io/2023/10/02/lsm-tree/\">LSM-Tree</a></p>\n<h3 id=\"Others\"><a href=\"#Others\" class=\"headerlink\" title=\"Others\"></a>Others</h3><ol>\n<li><a href=\"https://zhuanlan.zhihu.com/p/294069121\">White List Question</a></li>\n<li><a href=\"https://www.51cto.com/article/753025.html\">Redis Cache Breakdown</a></li>\n</ol>\n<h2 id=\"Improvement\"><a href=\"#Improvement\" class=\"headerlink\" title=\"Improvement\"></a>Improvement</h2><p>There are two limitations that have always restricted the usage of Bloom Filter, bounded source and append-only. The following are several improvement methods that revolve around addressing these two issues.</p>\n<h4 id=\"Scalable-Bloom-Filters-SBF\"><a href=\"#Scalable-Bloom-Filters-SBF\" class=\"headerlink\" title=\"Scalable Bloom Filters (SBF)\"></a>Scalable Bloom Filters (SBF)</h4><p><img src=\"/2023/10/02/bloom/scalable_bloom_filter.png#pic_center\" alt=\"scalable bloom filter structure\"></p>\n<p>when the filter reaches some fulfillness threshold, it becomes read-only and new bigger and writable filter is created in its place. If in its turn it becomes saturated, the operation is repeated. Every new filter, in order to keep the false positives rate close to the targeted one, has more hash functions than the previous filter.</p>\n<p>In Scalable Bloom filter the membership test is applied on all created (read-only + writable) filters. If the item is missing in all filters, it’s considered as not existing. In the other side, if one of the filters reports its existence, it means that the element may be in the dataset. An important point to notice here is that Scalable Bloom filter uses a variant of Bloom filters where the bit vector is divided in <em>k</em> slices where each stores <em>M&#x2F;k</em> bits (<em>M</em> is the size of whole bit vector). Since the number of slices is equal to the number of hash functions, each hash function works on its own slice:</p>\n<p><img src=\"/2023/10/02/bloom/scalable_bloom_filter_slices.png#pic_center\" alt=\"scalable bloom filter slices\"></p>\n<p>Thanks to the slices each element is always described by <em>k</em> bits resulting on more robust filter without the elements more prone to the false positives than the others.</p>\n<h4 id=\"Counting-Bloom-Filter-CBF\"><a href=\"#Counting-Bloom-Filter-CBF\" class=\"headerlink\" title=\"Counting Bloom Filter (CBF)\"></a>Counting Bloom Filter (CBF)</h4><p>For the second question, CBF provides ability to delete elements in Bloom Filters. But unfortunately, the premise is that we must ensure that the deleted element is present in the Bloom filter.</p>\n<p><img src=\"/2023/10/02/bloom/counting_bloom_filter.png#pic_center\" alt=\"counting bloom filter structure\"></p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li><a href=\"https://www.waitingforcode.com/big-data-algorithms/scalable-bloom-filter/read\">https://www.waitingforcode.com/big-data-algorithms/scalable-bloom-filter/read</a></li>\n<li>Baquero, C., &amp; Almeida, J. (2007, January). Scalable bloom filters. In European Conference on Principles of Data Mining and Knowledge Discovery (pp. 244-256). Springer, Berlin, Heidelberg.</li>\n</ol>\n"},{"title":"hudi-cdc","date":"2023-10-23T03:16:06.000Z","_content":"## Reference\n\n1. https://github.com/apache/hudi/blob/master/rfc/rfc-51/rfc-51.md#approvers\n2. https://github.com/apache/hudi/pull/5885/commits\n","source":"_posts/hudi-cdc.md","raw":"---\ntitle: hudi-cdc\ndate: 2023-10-23 11:16:06\ntags:\n---\n## Reference\n\n1. https://github.com/apache/hudi/blob/master/rfc/rfc-51/rfc-51.md#approvers\n2. https://github.com/apache/hudi/pull/5885/commits\n","slug":"hudi-cdc","published":1,"updated":"2023-10-23T06:31:31.321Z","_id":"clo2bsnpi0000iwc3bzaybvvz","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li><a href=\"https://github.com/apache/hudi/blob/master/rfc/rfc-51/rfc-51.md#approvers\">https://github.com/apache/hudi/blob/master/rfc/rfc-51/rfc-51.md#approvers</a></li>\n<li><a href=\"https://github.com/apache/hudi/pull/5885/commits\">https://github.com/apache/hudi/pull/5885/commits</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li><a href=\"https://github.com/apache/hudi/blob/master/rfc/rfc-51/rfc-51.md#approvers\">https://github.com/apache/hudi/blob/master/rfc/rfc-51/rfc-51.md#approvers</a></li>\n<li><a href=\"https://github.com/apache/hudi/pull/5885/commits\">https://github.com/apache/hudi/pull/5885/commits</a></li>\n</ol>\n"}],"PostAsset":[{"_id":"source/_posts/lsm-tree/leveled_compaction.png","slug":"leveled_compaction.png","post":"clnwr6ukh0001i5c36ui51o86","modified":0,"renderable":0},{"_id":"source/_posts/lsm-tree/lsm_tree.png","slug":"lsm_tree.png","post":"clnwr6ukh0001i5c36ui51o86","modified":0,"renderable":0},{"_id":"source/_posts/lsm-tree/merge_policy.png","slug":"merge_policy.png","post":"clnwr6ukh0001i5c36ui51o86","modified":0,"renderable":0},{"_id":"source/_posts/lsm-tree/range_query.png","slug":"range_query.png","post":"clnwr6ukh0001i5c36ui51o86","modified":0,"renderable":0},{"_id":"source/_posts/lsm-tree/size_tiered_compaction.png","slug":"size_tiered_compaction.png","post":"clnwr6ukh0001i5c36ui51o86","modified":0,"renderable":0},{"_id":"source/_posts/lsm-tree/sorted_runs.png","slug":"sorted_runs.png","post":"clnwr6ukh0001i5c36ui51o86","modified":0,"renderable":0},{"_id":"source/_posts/lsm-tree/sparse_index.png","slug":"sparse_index.png","post":"clnwr6ukh0001i5c36ui51o86","modified":0,"renderable":0},{"_id":"source/_posts/lsm-tree/sstable.png","slug":"sstable.png","post":"clnwr6ukh0001i5c36ui51o86","modified":0,"renderable":0},{"_id":"source/_posts/z-order/linear-order.png","slug":"linear-order.png","post":"clnwr6uki0003i5c3bh48ab32","modified":0,"renderable":0},{"_id":"source/_posts/z-order/z-order.png","slug":"z-order.png","post":"clnwr6uki0003i5c3bh48ab32","modified":0,"renderable":0},{"_id":"source/_posts/bloom/counting_bloom_filter.png","slug":"counting_bloom_filter.png","post":"clnwr6ukk0009i5c302xp0f9e","modified":0,"renderable":0},{"_id":"source/_posts/bloom/scalable_bloom_filter.png","slug":"scalable_bloom_filter.png","post":"clnwr6ukk0009i5c302xp0f9e","modified":0,"renderable":0},{"_id":"source/_posts/bloom/scalable_bloom_filter_slices.png","slug":"scalable_bloom_filter_slices.png","post":"clnwr6ukk0009i5c302xp0f9e","modified":0,"renderable":0}],"PostCategory":[],"PostTag":[{"post_id":"clnwr6uke0000i5c337167pvz","tag_id":"clnwr6uki0002i5c36xt2c8hj","_id":"clnwr6ukj0005i5c3dn8ncd65"},{"post_id":"clnwr6ukh0001i5c36ui51o86","tag_id":"clnwr6ukj0004i5c3ficrdf56","_id":"clnwr6ukj0007i5c3geyp1wbe"},{"post_id":"clnwr6uki0003i5c3bh48ab32","tag_id":"clnwr6ukj0006i5c34pkdcjat","_id":"clnwr6ukj0008i5c3ee5zgift"},{"post_id":"clnwr6ukk0009i5c302xp0f9e","tag_id":"clnwr6ukk000ai5c33mp52iln","_id":"clnwr6ukk000bi5c36bd6dy36"}],"Tag":[{"name":"bigdata","_id":"clnwr6uki0002i5c36xt2c8hj"},{"name":"LSM Storage","_id":"clnwr6ukj0004i5c3ficrdf56"},{"name":"big data, index","_id":"clnwr6ukj0006i5c34pkdcjat"},{"name":"Bloom-Filter","_id":"clnwr6ukk000ai5c33mp52iln"}]}}