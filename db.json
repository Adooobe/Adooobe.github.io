{"meta":{"version":1,"warehouse":"4.0.2"},"models":{"Asset":[{"_id":"themes/apollo/source/favicon.png","path":"favicon.png","modified":1,"renderable":1},{"_id":"themes/apollo/source/favicon.png_origin","path":"favicon.png_origin","modified":1,"renderable":1},{"_id":"themes/apollo/source/font/sourcesanspro.woff","path":"font/sourcesanspro.woff","modified":1,"renderable":1},{"_id":"themes/apollo/source/font/sourcesanspro.woff2","path":"font/sourcesanspro.woff2","modified":1,"renderable":1},{"_id":"themes/apollo/source/scss/apollo.scss","path":"scss/apollo.scss","modified":1,"renderable":1},{"_id":"themes/apollo/source/css/apollo.css","path":"css/apollo.css","modified":1,"renderable":1}],"Cache":[{"_id":"source/_posts/apache-arrow.md","hash":"58696f71cce26a6f0eaca505d9d568d39431f67c","modified":1696767495466},{"_id":"source/_posts/bloom.md","hash":"c6b73c846f4ec7d7bdb77203e87bcb5e2262f64f","modified":1696322667406},{"_id":"source/_posts/lsm-tree.md","hash":"c5ae4ad1b2a1676137091e733865e1ef8716d680","modified":1697088996868},{"_id":"source/_posts/lsm-tree/lsm_tree.png","hash":"a05ee23c764f9ec6b8bbd8beb7966a0d1d948d53","modified":1696336037896},{"_id":"source/_posts/bloom/scalable_bloom_filter.png","hash":"b3465257148d1e6220207fe4905b12be5be476ee","modified":1696319286528},{"_id":"source/_posts/bloom/counting_bloom_filter.png","hash":"4341a36a29824acb03930ce2f78859b0e2552e54","modified":1696322485561},{"_id":"source/_posts/bloom/scalable_bloom_filter_slices.png","hash":"cceccf33246196109f987e7c2a6d79e8c1991baf","modified":1696320101570},{"_id":"source/_posts/lsm-tree/range_query.png","hash":"77509946cf1f8c4bf0cb35688ba35c074b84ae55","modified":1696730612527},{"_id":"source/_posts/lsm-tree/merge_policy.png","hash":"1722696f22a890231bb1c3dd0afdcc1c8672770d","modified":1696731591775},{"_id":"source/_posts/lsm-tree/sstable.png","hash":"cb853e8d7b8d27de9758742434584beb24ef58a0","modified":1696667281071},{"_id":"source/_posts/lsm-tree/sorted_runs.png","hash":"e14bf61c546d7831745d6231b4f79c70b13d2e39","modified":1696742589621},{"_id":"source/_posts/lsm-tree/sparse_index.png","hash":"c8ff0e09d371b6aeacba785d84f50f9f3396fad2","modified":1696657426319},{"_id":"source/_posts/lsm-tree/size_tiered_compaction.png","hash":"70926f3735c514da0b78b52f4c7a5feb9c8e4aae","modified":1697078833192},{"_id":"themes/apollo/LICENSE","hash":"6e31ac9076bfc8f09ae47977419eee4edfb63e5b","modified":1696033836450},{"_id":"themes/apollo/README.md","hash":"b2cdcb158721c13eee6a2ae237d0488a738ed8c5","modified":1696033836458},{"_id":"themes/apollo/package.json","hash":"eb1e76ec0b7ed6c6c7b2bd32b4f1e1bbe15800ca","modified":1696033836459},{"_id":"themes/apollo/_config.yml","hash":"d381543761117d88d3254855612dc9dd3f35c332","modified":1696300027974},{"_id":"themes/apollo/gulpfile.js","hash":"857a026b6643a2cd52c65d4ae0dc7fe9618206ee","modified":1696033836459},{"_id":"themes/apollo/.gitignore","hash":"a006beea0877a0aa3610ee00e73f62cb1d45125b","modified":1696033836459},{"_id":"themes/apollo/doc/doc-zh.md","hash":"2a6a81840cdaf497969268a12d8f62c98cc38103","modified":1696033836460},{"_id":"themes/apollo/doc/doc-en.md","hash":"409e931a444c02a57b64a0a44dde6e66c1881ca0","modified":1696033836460},{"_id":"themes/apollo/languages/zh-cn.yml","hash":"22a2d16fe8c0dddb016b5325b9b9c182a1b49ae1","modified":1696033836458},{"_id":"themes/apollo/languages/en.yml","hash":"ca168b190932229884db1de755ec2f793c758a16","modified":1696033836458},{"_id":"themes/apollo/layout/archive.jade","hash":"62797414355bf4474092bc3a32726c8340820ffb","modified":1696033836453},{"_id":"themes/apollo/layout/index.jade","hash":"55f2f1b4b5364a0e09cb18e1112664c6415fb881","modified":1696033836453},{"_id":"themes/apollo/layout/post.jade","hash":"245c26244c075c3632d1545c3b228ee9d112f15d","modified":1696033836453},{"_id":"themes/apollo/source/favicon.png_origin","hash":"a9cdcb22d1e74d5480323e19d1983de5a6873b8c","modified":1696033836457},{"_id":"themes/apollo/layout/mixins/paginator.jade","hash":"f4ee2fb61a32e199b48cf93771749edc8a007391","modified":1696033836453},{"_id":"themes/apollo/layout/partial/comment.jade","hash":"ff0a2c269c2434da2ac5529872f1d6184a71f96d","modified":1696033836451},{"_id":"themes/apollo/layout/partial/copyright.jade","hash":"74c38c649c1781669f30a05587f07459ae8944db","modified":1696048884120},{"_id":"themes/apollo/layout/mixins/post.jade","hash":"f23c6c40e14cdf16783b3c2baf736c9dce18408c","modified":1696033836453},{"_id":"themes/apollo/layout/partial/head.jade","hash":"43d2db73f0247a9a3ed00ecb95b537a872b7201a","modified":1696033836451},{"_id":"themes/apollo/layout/partial/layout.jade","hash":"529c2ec06cfbc3d5b6d66dd320db50dfab5577a6","modified":1696033836452},{"_id":"themes/apollo/layout/partial/nav.jade","hash":"f4842d9d3d763fbb823d112a6f49f24cc42a0ad4","modified":1696033836451},{"_id":"themes/apollo/layout/partial/scripts.jade","hash":"6bff591ae3d1ff6750f239c4c933ad61f009f36a","modified":1696033836452},{"_id":"themes/apollo/source/scss/apollo.scss","hash":"e0092f469264b55b25e0d441274f1c812147e7d1","modified":1696033836454},{"_id":"themes/apollo/source/font/sourcesanspro.woff2","hash":"da65f527a8da65d5eb6721626d28cfdb46ab104a","modified":1696033836457},{"_id":"themes/apollo/source/css/apollo.css","hash":"714851e684a7fb17e21ddeed56a1112b432eaf94","modified":1696666644460},{"_id":"themes/apollo/source/font/sourcesanspro.woff","hash":"a6722c9b6439b7a020a9be3d3178970757a9265c","modified":1696033836457},{"_id":"themes/apollo/source/scss/_partial/archive-post-list.scss","hash":"d2f740a7d48349b7536777c795f82ab740836d0f","modified":1696033836455},{"_id":"themes/apollo/source/scss/_partial/base.scss","hash":"ae967b2049ecb9b8c4e139ecce32fd9fb5358ac5","modified":1696033836455},{"_id":"themes/apollo/source/scss/_partial/copyright.scss","hash":"1309667e3000037170cfbb5b8c9c65f4ffcf6814","modified":1696033836455},{"_id":"themes/apollo/source/scss/_partial/footer.scss","hash":"094aca6e52f11b139ac7980ca03fa7b9d8fc7b2f","modified":1696033836456},{"_id":"themes/apollo/source/scss/_partial/header.scss","hash":"153bde88bf8ffeae4ffd813d8cc694dd83d33d94","modified":1696033836456},{"_id":"themes/apollo/source/scss/_partial/mq.scss","hash":"fc5dda52cfbb10e27e2471e03f4606fb3d588225","modified":1696033836455},{"_id":"themes/apollo/source/scss/_partial/home-post-list.scss","hash":"6b5c59f3d2295944f934aee2c1156012a3306d5d","modified":1696033836456},{"_id":"themes/apollo/source/scss/_partial/normalize.scss","hash":"fd0b27bed6f103ea95b08f698ea663ff576dbcf1","modified":1696033836456},{"_id":"themes/apollo/source/scss/_partial/post.scss","hash":"7a2579935781913e4c7bce71e3de9f1449107c51","modified":1696666424231},{"_id":"themes/apollo/source/favicon.png","hash":"db29234c8b16c9f841ad8f0801d77bbf40024ef7","modified":1696744340339},{"_id":"public/atom.xml","hash":"027806f87db374d7d2362932968bdad6f2680f8f","modified":1697089604742},{"_id":"public/sitemap.xml","hash":"67b2e7197f0d8221dda5d2929a18a439dddb52a3","modified":1697089604742},{"_id":"public/sitemap.txt","hash":"31a76ad53f4620e4e507d8397789cf0b23057d89","modified":1697089604742},{"_id":"public/2023/10/08/apache-arrow/index.html","hash":"e71c5f4696a27aa70a275f3a11a922c9a750ed3a","modified":1697089604742},{"_id":"public/2023/10/02/lsm-tree/index.html","hash":"40da720cbcf7d098a337639f4b82d9babfb948e0","modified":1697089604742},{"_id":"public/2023/10/02/bloom/index.html","hash":"7885b57f661fa54971dc22bc6ef60ef54335c2e1","modified":1697089604742},{"_id":"public/archives/index.html","hash":"1a205cb601d62d6b062784a8686174d9190e28b3","modified":1697089604742},{"_id":"public/index.html","hash":"69c6ce71b6582a9968c29aee59427fe4efaa876e","modified":1697089604742},{"_id":"public/tags/Bloom-Filter/index.html","hash":"68661348f155031ce56ebe97efe340e335828217","modified":1697089604742},{"_id":"public/tags/LSM-Storage/index.html","hash":"3aa5448d0c7d63c7588496b93aa8280af47118c8","modified":1697089604742},{"_id":"public/favicon.png","hash":"db29234c8b16c9f841ad8f0801d77bbf40024ef7","modified":1697089604742},{"_id":"public/favicon.png_origin","hash":"a9cdcb22d1e74d5480323e19d1983de5a6873b8c","modified":1697089604742},{"_id":"public/font/sourcesanspro.woff","hash":"a6722c9b6439b7a020a9be3d3178970757a9265c","modified":1697089604742},{"_id":"public/scss/apollo.scss","hash":"e0092f469264b55b25e0d441274f1c812147e7d1","modified":1697089604742},{"_id":"public/font/sourcesanspro.woff2","hash":"da65f527a8da65d5eb6721626d28cfdb46ab104a","modified":1697089604742},{"_id":"public/css/apollo.css","hash":"714851e684a7fb17e21ddeed56a1112b432eaf94","modified":1697089604742}],"Category":[],"Data":[],"Page":[],"Post":[{"title":"Apache Arrow For the first time","date":"2023-10-08T12:17:14.000Z","_content":"","source":"_posts/apache-arrow.md","raw":"---\ntitle: Apache Arrow For the first time\ndate: 2023-10-08 20:17:14\ntags:\n---\n","slug":"apache-arrow","published":1,"updated":"2023-10-08T12:18:15.466Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clnmrc00a0000icc3crieffiu","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"Bloom Filter Implementation and Optimization","date":"2023-10-02T11:11:52.000Z","mathjax":true,"_content":"# Bloom Filter\n\n## What is Bloom Filter\n\nA bloom filter is a probabilistic data structure that is based on hashing. It is extremely space efficient and is typically used to add elements to a set and test if an element is in a set. Though, the elements themselves are not added to a set. Instead a hash of the elements is added to the set.\n\n## Implementation\n\n```python\nimport hashlib\n\nclass BloomFilter:\n    def __init__(self, m, k):\n        self.m = m\n        self.k = k\n        self.data = [0]*m\n        self.n = 0\n    def insert(self, element):\n        if self.k == 1:\n            hash1 = h1(element) % self.m\n            self.data[hash1] = 1\n        elif self.k == 2:\n            hash1 = h1(element) % self.m\n            hash2 = h2(element) % self.m\n            self.data[hash1] = 1\n            self.data[hash2] = 1\n        self.n += 1\n    def search(self, element):\n        if self.k == 1:\n            hash1 = h1(element) % self.m\n            if self.data[hash1] == 0:\n                return \"Not in Bloom Filter\"\n        elif self.k == 2:\n            hash1 = h1(element) % self.m\n            hash2 = h2(element) % self.m\n            if self.data[hash1] == 0 or self.data[hash2] == 0:\n                return \"Not in Bloom Filter\"\n        prob = (1.0 - ((1.0 - 1.0/self.m) * (self.k*self.n))) * self.k\n        return \"Might be in Bloom Filter with false positive probability \"+str(prob)\n\ndef h1(w):\n    h = hashlib.md5(w)\n    return hash(h.digest().encode('base64')[:6])%10\n\ndef h2(w):\n    h = hashlib.sha256(w)\n    return hash(h.digest().encode('base64')[:6])%10\n```\n\nIn this sample, we implement a simple Bloom Filter which has two hash function(controlled by *k*) to compute the position of input element in the bit array (named data, array size is *m*).\n\nWhen an element inserts into BloomFilter, two positions will be changed to 1 in the bit array computed by hash function so that if the element comes again, BloomFilter will get and check if the two positions are both 1. So when other element comes, if any position is not 1, the element doesn't exist in the data array.\n\nIt is worth noting that different elements can get same hash values computed by hash function. It means one grid in the bit array is not independently used by unique element but shared. In other words, it can be covered.\n\n## drawback\n\nAs mentioned above, Bloom Filter can creat false positives that Bloom Filter has a certain probability of mistakenly identifying non-existent elements.\n\nWe have two choices of parameters when building a bloom filter, `m` and `k`. They should each be chosen to dampen the number of false positives as much as possible while still maintaining whatever space requirement the filter needs.\n\nIf we have a bloom filter with `m` bits and `k` hash functions, the probability that a certain bit will still be zero after one insertion is\n\n$(1-1/m)^k$\n\nThen, after `n` insertions, the probability of it still being zero after `n` insertions is\n\n$(1-1/m)^{nk}$\n\nSo, that means the probability of a false positive is\n\n$(1-(1-1/m)^{nk})^k$\n\nIn each of these equations, raising the value of k (the number of hash functions) will make the probability of a false positive less likely. However, it is not computationally efficient to have an enormous value for *k*. To minimize this equation, we must choose the best *k*. We do it this way because we assume that the programmer has already chosen an **m** based on their space constraints and that they have some idea what their potential *n* will be. So the *k* value that minimizes that equation is\n\n$k=ln(2)⋅m/n$\n\n## Use Cases\n\n### Hudi Upsert\n\n### LSM-Tree\n\nSee [LSM-Tree](https://adooobe.github.io/2023/10/02/lsm-tree/)\n\n### Others\n\n1. [White List Question](https://zhuanlan.zhihu.com/p/294069121)\n2. [Redis Cache Breakdown](https://www.51cto.com/article/753025.html)\n\n## Improvement\n\nThere are two limitations that have always restricted the usage of Bloom Filter, bounded source and append-only. The following are several improvement methods that revolve around addressing these two issues.\n\n#### Scalable Bloom Filters (SBF)\n\n![scalable bloom filter structure](scalable_bloom_filter.png#pic_center)\n\nwhen the filter reaches some fulfillness threshold, it becomes read-only and new bigger and writable filter is created in its place. If in its turn it becomes saturated, the operation is repeated. Every new filter, in order to keep the false positives rate close to the targeted one, has more hash functions than the previous filter.\n\nIn Scalable Bloom filter the membership test is applied on all created (read-only + writable) filters. If the item is missing in all filters, it's considered as not existing. In the other side, if one of the filters reports its existence, it means that the element may be in the dataset. An important point to notice here is that Scalable Bloom filter uses a variant of Bloom filters where the bit vector is divided in *k* slices where each stores *M/k* bits (*M* is the size of whole bit vector). Since the number of slices is equal to the number of hash functions, each hash function works on its own slice:\n\n![scalable bloom filter slices](scalable_bloom_filter_slices.png#pic_center)\n\nThanks to the slices each element is always described by *k* bits resulting on more robust filter without the elements more prone to the false positives than the others.\n\n#### Counting Bloom Filter (CBF)\n\nFor the second question, CBF provides ability to delete elements in Bloom Filters. But unfortunately, the premise is that we must ensure that the deleted element is present in the Bloom filter.\n\n![counting bloom filter structure](counting_bloom_filter.png#pic_center)\n\n## Reference\n\n1. https://www.waitingforcode.com/big-data-algorithms/scalable-bloom-filter/read\n2. Baquero, C., & Almeida, J. (2007, January). Scalable bloom filters. In European Conference on Principles of Data Mining and Knowledge Discovery (pp. 244-256). Springer, Berlin, Heidelberg.\n","source":"_posts/bloom.md","raw":"---\ntitle: Bloom Filter Implementation and Optimization\ndate: 2023-10-02 19:11:52\ntags: Bloom-Filter\nmathjax: true\n---\n# Bloom Filter\n\n## What is Bloom Filter\n\nA bloom filter is a probabilistic data structure that is based on hashing. It is extremely space efficient and is typically used to add elements to a set and test if an element is in a set. Though, the elements themselves are not added to a set. Instead a hash of the elements is added to the set.\n\n## Implementation\n\n```python\nimport hashlib\n\nclass BloomFilter:\n    def __init__(self, m, k):\n        self.m = m\n        self.k = k\n        self.data = [0]*m\n        self.n = 0\n    def insert(self, element):\n        if self.k == 1:\n            hash1 = h1(element) % self.m\n            self.data[hash1] = 1\n        elif self.k == 2:\n            hash1 = h1(element) % self.m\n            hash2 = h2(element) % self.m\n            self.data[hash1] = 1\n            self.data[hash2] = 1\n        self.n += 1\n    def search(self, element):\n        if self.k == 1:\n            hash1 = h1(element) % self.m\n            if self.data[hash1] == 0:\n                return \"Not in Bloom Filter\"\n        elif self.k == 2:\n            hash1 = h1(element) % self.m\n            hash2 = h2(element) % self.m\n            if self.data[hash1] == 0 or self.data[hash2] == 0:\n                return \"Not in Bloom Filter\"\n        prob = (1.0 - ((1.0 - 1.0/self.m) * (self.k*self.n))) * self.k\n        return \"Might be in Bloom Filter with false positive probability \"+str(prob)\n\ndef h1(w):\n    h = hashlib.md5(w)\n    return hash(h.digest().encode('base64')[:6])%10\n\ndef h2(w):\n    h = hashlib.sha256(w)\n    return hash(h.digest().encode('base64')[:6])%10\n```\n\nIn this sample, we implement a simple Bloom Filter which has two hash function(controlled by *k*) to compute the position of input element in the bit array (named data, array size is *m*).\n\nWhen an element inserts into BloomFilter, two positions will be changed to 1 in the bit array computed by hash function so that if the element comes again, BloomFilter will get and check if the two positions are both 1. So when other element comes, if any position is not 1, the element doesn't exist in the data array.\n\nIt is worth noting that different elements can get same hash values computed by hash function. It means one grid in the bit array is not independently used by unique element but shared. In other words, it can be covered.\n\n## drawback\n\nAs mentioned above, Bloom Filter can creat false positives that Bloom Filter has a certain probability of mistakenly identifying non-existent elements.\n\nWe have two choices of parameters when building a bloom filter, `m` and `k`. They should each be chosen to dampen the number of false positives as much as possible while still maintaining whatever space requirement the filter needs.\n\nIf we have a bloom filter with `m` bits and `k` hash functions, the probability that a certain bit will still be zero after one insertion is\n\n$(1-1/m)^k$\n\nThen, after `n` insertions, the probability of it still being zero after `n` insertions is\n\n$(1-1/m)^{nk}$\n\nSo, that means the probability of a false positive is\n\n$(1-(1-1/m)^{nk})^k$\n\nIn each of these equations, raising the value of k (the number of hash functions) will make the probability of a false positive less likely. However, it is not computationally efficient to have an enormous value for *k*. To minimize this equation, we must choose the best *k*. We do it this way because we assume that the programmer has already chosen an **m** based on their space constraints and that they have some idea what their potential *n* will be. So the *k* value that minimizes that equation is\n\n$k=ln(2)⋅m/n$\n\n## Use Cases\n\n### Hudi Upsert\n\n### LSM-Tree\n\nSee [LSM-Tree](https://adooobe.github.io/2023/10/02/lsm-tree/)\n\n### Others\n\n1. [White List Question](https://zhuanlan.zhihu.com/p/294069121)\n2. [Redis Cache Breakdown](https://www.51cto.com/article/753025.html)\n\n## Improvement\n\nThere are two limitations that have always restricted the usage of Bloom Filter, bounded source and append-only. The following are several improvement methods that revolve around addressing these two issues.\n\n#### Scalable Bloom Filters (SBF)\n\n![scalable bloom filter structure](scalable_bloom_filter.png#pic_center)\n\nwhen the filter reaches some fulfillness threshold, it becomes read-only and new bigger and writable filter is created in its place. If in its turn it becomes saturated, the operation is repeated. Every new filter, in order to keep the false positives rate close to the targeted one, has more hash functions than the previous filter.\n\nIn Scalable Bloom filter the membership test is applied on all created (read-only + writable) filters. If the item is missing in all filters, it's considered as not existing. In the other side, if one of the filters reports its existence, it means that the element may be in the dataset. An important point to notice here is that Scalable Bloom filter uses a variant of Bloom filters where the bit vector is divided in *k* slices where each stores *M/k* bits (*M* is the size of whole bit vector). Since the number of slices is equal to the number of hash functions, each hash function works on its own slice:\n\n![scalable bloom filter slices](scalable_bloom_filter_slices.png#pic_center)\n\nThanks to the slices each element is always described by *k* bits resulting on more robust filter without the elements more prone to the false positives than the others.\n\n#### Counting Bloom Filter (CBF)\n\nFor the second question, CBF provides ability to delete elements in Bloom Filters. But unfortunately, the premise is that we must ensure that the deleted element is present in the Bloom filter.\n\n![counting bloom filter structure](counting_bloom_filter.png#pic_center)\n\n## Reference\n\n1. https://www.waitingforcode.com/big-data-algorithms/scalable-bloom-filter/read\n2. Baquero, C., & Almeida, J. (2007, January). Scalable bloom filters. In European Conference on Principles of Data Mining and Knowledge Discovery (pp. 244-256). Springer, Berlin, Heidelberg.\n","slug":"bloom","published":1,"updated":"2023-10-03T08:44:27.406Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clnmrc00c0001icc385lefpc2","content":"<h1 id=\"Bloom-Filter\"><a href=\"#Bloom-Filter\" class=\"headerlink\" title=\"Bloom Filter\"></a>Bloom Filter</h1><h2 id=\"What-is-Bloom-Filter\"><a href=\"#What-is-Bloom-Filter\" class=\"headerlink\" title=\"What is Bloom Filter\"></a>What is Bloom Filter</h2><p>A bloom filter is a probabilistic data structure that is based on hashing. It is extremely space efficient and is typically used to add elements to a set and test if an element is in a set. Though, the elements themselves are not added to a set. Instead a hash of the elements is added to the set.</p>\n<h2 id=\"Implementation\"><a href=\"#Implementation\" class=\"headerlink\" title=\"Implementation\"></a>Implementation</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> hashlib</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">BloomFilter</span>:</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, m, k</span>):</span><br><span class=\"line\">        self.m = m</span><br><span class=\"line\">        self.k = k</span><br><span class=\"line\">        self.data = [<span class=\"number\">0</span>]*m</span><br><span class=\"line\">        self.n = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">insert</span>(<span class=\"params\">self, element</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.k == <span class=\"number\">1</span>:</span><br><span class=\"line\">            hash1 = h1(element) % self.m</span><br><span class=\"line\">            self.data[hash1] = <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">elif</span> self.k == <span class=\"number\">2</span>:</span><br><span class=\"line\">            hash1 = h1(element) % self.m</span><br><span class=\"line\">            hash2 = h2(element) % self.m</span><br><span class=\"line\">            self.data[hash1] = <span class=\"number\">1</span></span><br><span class=\"line\">            self.data[hash2] = <span class=\"number\">1</span></span><br><span class=\"line\">        self.n += <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">search</span>(<span class=\"params\">self, element</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.k == <span class=\"number\">1</span>:</span><br><span class=\"line\">            hash1 = h1(element) % self.m</span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.data[hash1] == <span class=\"number\">0</span>:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"string\">&quot;Not in Bloom Filter&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">elif</span> self.k == <span class=\"number\">2</span>:</span><br><span class=\"line\">            hash1 = h1(element) % self.m</span><br><span class=\"line\">            hash2 = h2(element) % self.m</span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.data[hash1] == <span class=\"number\">0</span> <span class=\"keyword\">or</span> self.data[hash2] == <span class=\"number\">0</span>:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"string\">&quot;Not in Bloom Filter&quot;</span></span><br><span class=\"line\">        prob = (<span class=\"number\">1.0</span> - ((<span class=\"number\">1.0</span> - <span class=\"number\">1.0</span>/self.m) * (self.k*self.n))) * self.k</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">&quot;Might be in Bloom Filter with false positive probability &quot;</span>+<span class=\"built_in\">str</span>(prob)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">h1</span>(<span class=\"params\">w</span>):</span><br><span class=\"line\">    h = hashlib.md5(w)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">hash</span>(h.digest().encode(<span class=\"string\">&#x27;base64&#x27;</span>)[:<span class=\"number\">6</span>])%<span class=\"number\">10</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">h2</span>(<span class=\"params\">w</span>):</span><br><span class=\"line\">    h = hashlib.sha256(w)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">hash</span>(h.digest().encode(<span class=\"string\">&#x27;base64&#x27;</span>)[:<span class=\"number\">6</span>])%<span class=\"number\">10</span></span><br></pre></td></tr></table></figure>\n\n<p>In this sample, we implement a simple Bloom Filter which has two hash function(controlled by <em>k</em>) to compute the position of input element in the bit array (named data, array size is <em>m</em>).</p>\n<p>When an element inserts into BloomFilter, two positions will be changed to 1 in the bit array computed by hash function so that if the element comes again, BloomFilter will get and check if the two positions are both 1. So when other element comes, if any position is not 1, the element doesn’t exist in the data array.</p>\n<p>It is worth noting that different elements can get same hash values computed by hash function. It means one grid in the bit array is not independently used by unique element but shared. In other words, it can be covered.</p>\n<h2 id=\"drawback\"><a href=\"#drawback\" class=\"headerlink\" title=\"drawback\"></a>drawback</h2><p>As mentioned above, Bloom Filter can creat false positives that Bloom Filter has a certain probability of mistakenly identifying non-existent elements.</p>\n<p>We have two choices of parameters when building a bloom filter, <code>m</code> and <code>k</code>. They should each be chosen to dampen the number of false positives as much as possible while still maintaining whatever space requirement the filter needs.</p>\n<p>If we have a bloom filter with <code>m</code> bits and <code>k</code> hash functions, the probability that a certain bit will still be zero after one insertion is</p>\n<p>$(1-1&#x2F;m)^k$</p>\n<p>Then, after <code>n</code> insertions, the probability of it still being zero after <code>n</code> insertions is</p>\n<p>$(1-1&#x2F;m)^{nk}$</p>\n<p>So, that means the probability of a false positive is</p>\n<p>$(1-(1-1&#x2F;m)^{nk})^k$</p>\n<p>In each of these equations, raising the value of k (the number of hash functions) will make the probability of a false positive less likely. However, it is not computationally efficient to have an enormous value for <em>k</em>. To minimize this equation, we must choose the best <em>k</em>. We do it this way because we assume that the programmer has already chosen an <strong>m</strong> based on their space constraints and that they have some idea what their potential <em>n</em> will be. So the <em>k</em> value that minimizes that equation is</p>\n<p>$k&#x3D;ln(2)⋅m&#x2F;n$</p>\n<h2 id=\"Use-Cases\"><a href=\"#Use-Cases\" class=\"headerlink\" title=\"Use Cases\"></a>Use Cases</h2><h3 id=\"Hudi-Upsert\"><a href=\"#Hudi-Upsert\" class=\"headerlink\" title=\"Hudi Upsert\"></a>Hudi Upsert</h3><h3 id=\"LSM-Tree\"><a href=\"#LSM-Tree\" class=\"headerlink\" title=\"LSM-Tree\"></a>LSM-Tree</h3><p>See <a href=\"https://adooobe.github.io/2023/10/02/lsm-tree/\">LSM-Tree</a></p>\n<h3 id=\"Others\"><a href=\"#Others\" class=\"headerlink\" title=\"Others\"></a>Others</h3><ol>\n<li><a href=\"https://zhuanlan.zhihu.com/p/294069121\">White List Question</a></li>\n<li><a href=\"https://www.51cto.com/article/753025.html\">Redis Cache Breakdown</a></li>\n</ol>\n<h2 id=\"Improvement\"><a href=\"#Improvement\" class=\"headerlink\" title=\"Improvement\"></a>Improvement</h2><p>There are two limitations that have always restricted the usage of Bloom Filter, bounded source and append-only. The following are several improvement methods that revolve around addressing these two issues.</p>\n<h4 id=\"Scalable-Bloom-Filters-SBF\"><a href=\"#Scalable-Bloom-Filters-SBF\" class=\"headerlink\" title=\"Scalable Bloom Filters (SBF)\"></a>Scalable Bloom Filters (SBF)</h4><p><img src=\"/Adooobe.github.io/scalable_bloom_filter.png#pic_center\" alt=\"scalable bloom filter structure\"></p>\n<p>when the filter reaches some fulfillness threshold, it becomes read-only and new bigger and writable filter is created in its place. If in its turn it becomes saturated, the operation is repeated. Every new filter, in order to keep the false positives rate close to the targeted one, has more hash functions than the previous filter.</p>\n<p>In Scalable Bloom filter the membership test is applied on all created (read-only + writable) filters. If the item is missing in all filters, it’s considered as not existing. In the other side, if one of the filters reports its existence, it means that the element may be in the dataset. An important point to notice here is that Scalable Bloom filter uses a variant of Bloom filters where the bit vector is divided in <em>k</em> slices where each stores <em>M&#x2F;k</em> bits (<em>M</em> is the size of whole bit vector). Since the number of slices is equal to the number of hash functions, each hash function works on its own slice:</p>\n<p><img src=\"/Adooobe.github.io/scalable_bloom_filter_slices.png#pic_center\" alt=\"scalable bloom filter slices\"></p>\n<p>Thanks to the slices each element is always described by <em>k</em> bits resulting on more robust filter without the elements more prone to the false positives than the others.</p>\n<h4 id=\"Counting-Bloom-Filter-CBF\"><a href=\"#Counting-Bloom-Filter-CBF\" class=\"headerlink\" title=\"Counting Bloom Filter (CBF)\"></a>Counting Bloom Filter (CBF)</h4><p>For the second question, CBF provides ability to delete elements in Bloom Filters. But unfortunately, the premise is that we must ensure that the deleted element is present in the Bloom filter.</p>\n<p><img src=\"/Adooobe.github.io/counting_bloom_filter.png#pic_center\" alt=\"counting bloom filter structure\"></p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li><a href=\"https://www.waitingforcode.com/big-data-algorithms/scalable-bloom-filter/read\">https://www.waitingforcode.com/big-data-algorithms/scalable-bloom-filter/read</a></li>\n<li>Baquero, C., &amp; Almeida, J. (2007, January). Scalable bloom filters. In European Conference on Principles of Data Mining and Knowledge Discovery (pp. 244-256). Springer, Berlin, Heidelberg.</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Bloom-Filter\"><a href=\"#Bloom-Filter\" class=\"headerlink\" title=\"Bloom Filter\"></a>Bloom Filter</h1><h2 id=\"What-is-Bloom-Filter\"><a href=\"#What-is-Bloom-Filter\" class=\"headerlink\" title=\"What is Bloom Filter\"></a>What is Bloom Filter</h2><p>A bloom filter is a probabilistic data structure that is based on hashing. It is extremely space efficient and is typically used to add elements to a set and test if an element is in a set. Though, the elements themselves are not added to a set. Instead a hash of the elements is added to the set.</p>\n<h2 id=\"Implementation\"><a href=\"#Implementation\" class=\"headerlink\" title=\"Implementation\"></a>Implementation</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> hashlib</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">BloomFilter</span>:</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, m, k</span>):</span><br><span class=\"line\">        self.m = m</span><br><span class=\"line\">        self.k = k</span><br><span class=\"line\">        self.data = [<span class=\"number\">0</span>]*m</span><br><span class=\"line\">        self.n = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">insert</span>(<span class=\"params\">self, element</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.k == <span class=\"number\">1</span>:</span><br><span class=\"line\">            hash1 = h1(element) % self.m</span><br><span class=\"line\">            self.data[hash1] = <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">elif</span> self.k == <span class=\"number\">2</span>:</span><br><span class=\"line\">            hash1 = h1(element) % self.m</span><br><span class=\"line\">            hash2 = h2(element) % self.m</span><br><span class=\"line\">            self.data[hash1] = <span class=\"number\">1</span></span><br><span class=\"line\">            self.data[hash2] = <span class=\"number\">1</span></span><br><span class=\"line\">        self.n += <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">search</span>(<span class=\"params\">self, element</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.k == <span class=\"number\">1</span>:</span><br><span class=\"line\">            hash1 = h1(element) % self.m</span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.data[hash1] == <span class=\"number\">0</span>:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"string\">&quot;Not in Bloom Filter&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">elif</span> self.k == <span class=\"number\">2</span>:</span><br><span class=\"line\">            hash1 = h1(element) % self.m</span><br><span class=\"line\">            hash2 = h2(element) % self.m</span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.data[hash1] == <span class=\"number\">0</span> <span class=\"keyword\">or</span> self.data[hash2] == <span class=\"number\">0</span>:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"string\">&quot;Not in Bloom Filter&quot;</span></span><br><span class=\"line\">        prob = (<span class=\"number\">1.0</span> - ((<span class=\"number\">1.0</span> - <span class=\"number\">1.0</span>/self.m) * (self.k*self.n))) * self.k</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">&quot;Might be in Bloom Filter with false positive probability &quot;</span>+<span class=\"built_in\">str</span>(prob)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">h1</span>(<span class=\"params\">w</span>):</span><br><span class=\"line\">    h = hashlib.md5(w)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">hash</span>(h.digest().encode(<span class=\"string\">&#x27;base64&#x27;</span>)[:<span class=\"number\">6</span>])%<span class=\"number\">10</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">h2</span>(<span class=\"params\">w</span>):</span><br><span class=\"line\">    h = hashlib.sha256(w)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">hash</span>(h.digest().encode(<span class=\"string\">&#x27;base64&#x27;</span>)[:<span class=\"number\">6</span>])%<span class=\"number\">10</span></span><br></pre></td></tr></table></figure>\n\n<p>In this sample, we implement a simple Bloom Filter which has two hash function(controlled by <em>k</em>) to compute the position of input element in the bit array (named data, array size is <em>m</em>).</p>\n<p>When an element inserts into BloomFilter, two positions will be changed to 1 in the bit array computed by hash function so that if the element comes again, BloomFilter will get and check if the two positions are both 1. So when other element comes, if any position is not 1, the element doesn’t exist in the data array.</p>\n<p>It is worth noting that different elements can get same hash values computed by hash function. It means one grid in the bit array is not independently used by unique element but shared. In other words, it can be covered.</p>\n<h2 id=\"drawback\"><a href=\"#drawback\" class=\"headerlink\" title=\"drawback\"></a>drawback</h2><p>As mentioned above, Bloom Filter can creat false positives that Bloom Filter has a certain probability of mistakenly identifying non-existent elements.</p>\n<p>We have two choices of parameters when building a bloom filter, <code>m</code> and <code>k</code>. They should each be chosen to dampen the number of false positives as much as possible while still maintaining whatever space requirement the filter needs.</p>\n<p>If we have a bloom filter with <code>m</code> bits and <code>k</code> hash functions, the probability that a certain bit will still be zero after one insertion is</p>\n<p>$(1-1&#x2F;m)^k$</p>\n<p>Then, after <code>n</code> insertions, the probability of it still being zero after <code>n</code> insertions is</p>\n<p>$(1-1&#x2F;m)^{nk}$</p>\n<p>So, that means the probability of a false positive is</p>\n<p>$(1-(1-1&#x2F;m)^{nk})^k$</p>\n<p>In each of these equations, raising the value of k (the number of hash functions) will make the probability of a false positive less likely. However, it is not computationally efficient to have an enormous value for <em>k</em>. To minimize this equation, we must choose the best <em>k</em>. We do it this way because we assume that the programmer has already chosen an <strong>m</strong> based on their space constraints and that they have some idea what their potential <em>n</em> will be. So the <em>k</em> value that minimizes that equation is</p>\n<p>$k&#x3D;ln(2)⋅m&#x2F;n$</p>\n<h2 id=\"Use-Cases\"><a href=\"#Use-Cases\" class=\"headerlink\" title=\"Use Cases\"></a>Use Cases</h2><h3 id=\"Hudi-Upsert\"><a href=\"#Hudi-Upsert\" class=\"headerlink\" title=\"Hudi Upsert\"></a>Hudi Upsert</h3><h3 id=\"LSM-Tree\"><a href=\"#LSM-Tree\" class=\"headerlink\" title=\"LSM-Tree\"></a>LSM-Tree</h3><p>See <a href=\"https://adooobe.github.io/2023/10/02/lsm-tree/\">LSM-Tree</a></p>\n<h3 id=\"Others\"><a href=\"#Others\" class=\"headerlink\" title=\"Others\"></a>Others</h3><ol>\n<li><a href=\"https://zhuanlan.zhihu.com/p/294069121\">White List Question</a></li>\n<li><a href=\"https://www.51cto.com/article/753025.html\">Redis Cache Breakdown</a></li>\n</ol>\n<h2 id=\"Improvement\"><a href=\"#Improvement\" class=\"headerlink\" title=\"Improvement\"></a>Improvement</h2><p>There are two limitations that have always restricted the usage of Bloom Filter, bounded source and append-only. The following are several improvement methods that revolve around addressing these two issues.</p>\n<h4 id=\"Scalable-Bloom-Filters-SBF\"><a href=\"#Scalable-Bloom-Filters-SBF\" class=\"headerlink\" title=\"Scalable Bloom Filters (SBF)\"></a>Scalable Bloom Filters (SBF)</h4><p><img src=\"/Adooobe.github.io/scalable_bloom_filter.png#pic_center\" alt=\"scalable bloom filter structure\"></p>\n<p>when the filter reaches some fulfillness threshold, it becomes read-only and new bigger and writable filter is created in its place. If in its turn it becomes saturated, the operation is repeated. Every new filter, in order to keep the false positives rate close to the targeted one, has more hash functions than the previous filter.</p>\n<p>In Scalable Bloom filter the membership test is applied on all created (read-only + writable) filters. If the item is missing in all filters, it’s considered as not existing. In the other side, if one of the filters reports its existence, it means that the element may be in the dataset. An important point to notice here is that Scalable Bloom filter uses a variant of Bloom filters where the bit vector is divided in <em>k</em> slices where each stores <em>M&#x2F;k</em> bits (<em>M</em> is the size of whole bit vector). Since the number of slices is equal to the number of hash functions, each hash function works on its own slice:</p>\n<p><img src=\"/Adooobe.github.io/scalable_bloom_filter_slices.png#pic_center\" alt=\"scalable bloom filter slices\"></p>\n<p>Thanks to the slices each element is always described by <em>k</em> bits resulting on more robust filter without the elements more prone to the false positives than the others.</p>\n<h4 id=\"Counting-Bloom-Filter-CBF\"><a href=\"#Counting-Bloom-Filter-CBF\" class=\"headerlink\" title=\"Counting Bloom Filter (CBF)\"></a>Counting Bloom Filter (CBF)</h4><p>For the second question, CBF provides ability to delete elements in Bloom Filters. But unfortunately, the premise is that we must ensure that the deleted element is present in the Bloom filter.</p>\n<p><img src=\"/Adooobe.github.io/counting_bloom_filter.png#pic_center\" alt=\"counting bloom filter structure\"></p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li><a href=\"https://www.waitingforcode.com/big-data-algorithms/scalable-bloom-filter/read\">https://www.waitingforcode.com/big-data-algorithms/scalable-bloom-filter/read</a></li>\n<li>Baquero, C., &amp; Almeida, J. (2007, January). Scalable bloom filters. In European Conference on Principles of Data Mining and Knowledge Discovery (pp. 244-256). Springer, Berlin, Heidelberg.</li>\n</ol>\n"},{"title":"the principle and application of LSM Tree","date":"2023-10-02T11:24:02.000Z","_content":"# LSM Tree\n\n## Feature\n\n* Ordered\n* Block Storage(Disk)-oriented\n* Hierarchical\n\n## Structure\n\n![LSM-Tree Structure](lsm_tree.png#pic_center)\n\n## Workflow\n\n### WAL\n\nWhen LSM Tree received a input(insert/update/delete) operation, to avoid accidental crashing or shutdown，It is neccessary to write ahead log(WAL) that saves operation records into log files.\n\n```go\ntype Wal struct {\n\tf    *os.File\n\tpath string\n\tlock sync.Locker\n}\n```\n\n### Memtable\n\nMemtable is an append-only data structure (every node inserted cannot be changed by logical delete and removing duplicates based on update time.) like Binary search tree (e.g. RedBlack tree - LevelDB) or SkipList (e.g. HBase, more popular).\n\n### Sorted String Table (SSTables)\n\nUsually, a SSTable consists of an index block and multiple data blocks. Data block structure is as follows:\n\n![Sorted Strings Table](sstable.png#pic_center)\n\nwhere data is only ordered in segment layer rather than global.\n\n### Optimization\n\n#### improve read performance\n\nFor LSM Tree, what we concerned is read performance rather than write. As data increases, some methods to improve read performance need to come into our pictures.\n\n* **Sparse Index**\n\nAs mentioned above, SStable has several segments that data are orderly stored. Without any optimization, we can also use binary search algorithm to find a certain element by scanning all the SStables and every Segments in each SStable unitl finding it. But unfortunately, for binary search algorithm, the minimum memory unit is segment for us to find the key in *O(logn)* or spent a Disk IO, both are too expensive in big data scenarios. So it is neccessary to  build a sparse index in memory to accelerate query efficiency.\n\n![sparse index](sparse_index.png#pic_center)\n\n> Sparse indexes only contain entries for documents that have the indexed field, even if the index field contains a null value. The index skips over any document that is missing the indexed field. The index is \"sparse\" because it does not include all documents of a collection.\n\n* **Bloom Filter**\n\nWhen the number of SStable increases in Disk, if some key is not present in the records, we need to scan all the SStable to find that key. Bloom filter is to overcome this issue. Unlike sparse indexes, [Bloom filters](https://adooobe.github.io/2023/10/02/bloom/) are designed to address the performance issues that arise when querying for non-existent keys.\n\n### Compaction\n\n#### questions in query\n\nLet's talk about query in LSM Tree first. There are two query methods: *point lookup query* and *range query*.\n\n* **point lookup query**: find the element what we want from new segment to old one.\n* **range query**: when a big range query is executed, data have to be found in memtable, immutable memtable and multiple SSTalbes in different levels. (Notice: range query should be **key range query** like the follow picture)\n\n![LSM Tree range query](range_query.png#pic_center)\n\nDuring range reads, the iterator will seek to the start range similar to point lookup (using Binary search with in SSTs) using`SeekTo()` call. After seeking to start range, there will be series of iterators created one for each memtable, one for each Level-0 files (because of overlapping nature of SSTs in L0) and one for each level later on. A merging iterator will collect keys from each of these iterators and gives the data in sorted order till the End range is reached.\n\n```go\nitr := txn.NewIterator(badger.DefaultIteratorOptions)   \nfor itr.Seek(\"startKey\"); itr.Valid(); itr.Next() {\n    item := itr.Item()\n    key := item.Key()\n    if bytes.Compare(key, \"endKey\") > 0 {\n      break\n    }\n    // rest of the logic.\n}\n```\n\nStep1. find *startkey* position (write as startPosition) by `seek()` and move sub iterator to `startPosition + 1`\n\nStep2. compare the sub iterators' element, return the minimal value and move the itr pointer.\n\nStep3. repeat Step2 until the returned element > endkey\n\nSo in range query, as SSTables become more and more, query execution also becomes heavier and heavier.\n\n#### Sorted Run\n\n> LSM tree organizes files into several sorted runs. A sorted run consists of one or multiple [data file](https://nightlies.apache.org/flink/flink-table-store-docs-release-0.3/docs/concepts/file-layouts/#data-files)s and each data file belongs to exactly one sorted run.\n>\n> ![sorted runs](sorted_runs.png#pic_center)\n>\n> As you can see, different sorted runs may have overlapping primary key ranges, and may even contain the same primary key. When querying the LSM tree, all sorted runs must be combined and all records with the same primary key must be merged according to the user-specified [merge engine](https://nightlies.apache.org/flink/flink-table-store-docs-release-0.3/docs/features/table-types/#merge-engines) and the timestamp of each record.\n\nIn my opinion, in LSM Tree, a single logically ordered and no-repeat structure can be regarded as a `sorted run`.\n\n#### Methods\n\n![LSM Tree merge policies](merge_policy.png#pic_center)\n\n* tiered compaction(low write amplification)\n  ![size_tiered_compaction](size_tiered_compaction.png#pic_center)\n  * high read and space amplification\n* leveled compaction\n\nLike the picture above, leveled merge policies will merge SSTables into next level with the same range.\n\n## Improvement\n\n* **bLSM**\n* **Diff-Index LSM**\n\n## Reference\n\n1. https://www.cnblogs.com/whuanle/p/16297025.html\n2. https://www.geeksforgeeks.org/introduction-to-log-structured-merge-lsm-tree/\n3. https://www.mongodb.com/docs/manual/core/index-sparse/#:~:text=Sparse%20indexes%20only%20contain%20entries,all%20documents%20of%20a%20collection.\n4. https://hzhu212.github.io/posts/2d7c5edb/\n5. https://zhuanlan.zhihu.com/p/380013595\n6. https://github.com/facebook/rocksdb/wiki/Iterator-Implementation\n","source":"_posts/lsm-tree.md","raw":"---\ntitle: the principle and application of LSM Tree \ndate: 2023-10-02 19:24:02\ntags: LSM Storage\n---\n# LSM Tree\n\n## Feature\n\n* Ordered\n* Block Storage(Disk)-oriented\n* Hierarchical\n\n## Structure\n\n![LSM-Tree Structure](lsm_tree.png#pic_center)\n\n## Workflow\n\n### WAL\n\nWhen LSM Tree received a input(insert/update/delete) operation, to avoid accidental crashing or shutdown，It is neccessary to write ahead log(WAL) that saves operation records into log files.\n\n```go\ntype Wal struct {\n\tf    *os.File\n\tpath string\n\tlock sync.Locker\n}\n```\n\n### Memtable\n\nMemtable is an append-only data structure (every node inserted cannot be changed by logical delete and removing duplicates based on update time.) like Binary search tree (e.g. RedBlack tree - LevelDB) or SkipList (e.g. HBase, more popular).\n\n### Sorted String Table (SSTables)\n\nUsually, a SSTable consists of an index block and multiple data blocks. Data block structure is as follows:\n\n![Sorted Strings Table](sstable.png#pic_center)\n\nwhere data is only ordered in segment layer rather than global.\n\n### Optimization\n\n#### improve read performance\n\nFor LSM Tree, what we concerned is read performance rather than write. As data increases, some methods to improve read performance need to come into our pictures.\n\n* **Sparse Index**\n\nAs mentioned above, SStable has several segments that data are orderly stored. Without any optimization, we can also use binary search algorithm to find a certain element by scanning all the SStables and every Segments in each SStable unitl finding it. But unfortunately, for binary search algorithm, the minimum memory unit is segment for us to find the key in *O(logn)* or spent a Disk IO, both are too expensive in big data scenarios. So it is neccessary to  build a sparse index in memory to accelerate query efficiency.\n\n![sparse index](sparse_index.png#pic_center)\n\n> Sparse indexes only contain entries for documents that have the indexed field, even if the index field contains a null value. The index skips over any document that is missing the indexed field. The index is \"sparse\" because it does not include all documents of a collection.\n\n* **Bloom Filter**\n\nWhen the number of SStable increases in Disk, if some key is not present in the records, we need to scan all the SStable to find that key. Bloom filter is to overcome this issue. Unlike sparse indexes, [Bloom filters](https://adooobe.github.io/2023/10/02/bloom/) are designed to address the performance issues that arise when querying for non-existent keys.\n\n### Compaction\n\n#### questions in query\n\nLet's talk about query in LSM Tree first. There are two query methods: *point lookup query* and *range query*.\n\n* **point lookup query**: find the element what we want from new segment to old one.\n* **range query**: when a big range query is executed, data have to be found in memtable, immutable memtable and multiple SSTalbes in different levels. (Notice: range query should be **key range query** like the follow picture)\n\n![LSM Tree range query](range_query.png#pic_center)\n\nDuring range reads, the iterator will seek to the start range similar to point lookup (using Binary search with in SSTs) using`SeekTo()` call. After seeking to start range, there will be series of iterators created one for each memtable, one for each Level-0 files (because of overlapping nature of SSTs in L0) and one for each level later on. A merging iterator will collect keys from each of these iterators and gives the data in sorted order till the End range is reached.\n\n```go\nitr := txn.NewIterator(badger.DefaultIteratorOptions)   \nfor itr.Seek(\"startKey\"); itr.Valid(); itr.Next() {\n    item := itr.Item()\n    key := item.Key()\n    if bytes.Compare(key, \"endKey\") > 0 {\n      break\n    }\n    // rest of the logic.\n}\n```\n\nStep1. find *startkey* position (write as startPosition) by `seek()` and move sub iterator to `startPosition + 1`\n\nStep2. compare the sub iterators' element, return the minimal value and move the itr pointer.\n\nStep3. repeat Step2 until the returned element > endkey\n\nSo in range query, as SSTables become more and more, query execution also becomes heavier and heavier.\n\n#### Sorted Run\n\n> LSM tree organizes files into several sorted runs. A sorted run consists of one or multiple [data file](https://nightlies.apache.org/flink/flink-table-store-docs-release-0.3/docs/concepts/file-layouts/#data-files)s and each data file belongs to exactly one sorted run.\n>\n> ![sorted runs](sorted_runs.png#pic_center)\n>\n> As you can see, different sorted runs may have overlapping primary key ranges, and may even contain the same primary key. When querying the LSM tree, all sorted runs must be combined and all records with the same primary key must be merged according to the user-specified [merge engine](https://nightlies.apache.org/flink/flink-table-store-docs-release-0.3/docs/features/table-types/#merge-engines) and the timestamp of each record.\n\nIn my opinion, in LSM Tree, a single logically ordered and no-repeat structure can be regarded as a `sorted run`.\n\n#### Methods\n\n![LSM Tree merge policies](merge_policy.png#pic_center)\n\n* tiered compaction(low write amplification)\n  ![size_tiered_compaction](size_tiered_compaction.png#pic_center)\n  * high read and space amplification\n* leveled compaction\n\nLike the picture above, leveled merge policies will merge SSTables into next level with the same range.\n\n## Improvement\n\n* **bLSM**\n* **Diff-Index LSM**\n\n## Reference\n\n1. https://www.cnblogs.com/whuanle/p/16297025.html\n2. https://www.geeksforgeeks.org/introduction-to-log-structured-merge-lsm-tree/\n3. https://www.mongodb.com/docs/manual/core/index-sparse/#:~:text=Sparse%20indexes%20only%20contain%20entries,all%20documents%20of%20a%20collection.\n4. https://hzhu212.github.io/posts/2d7c5edb/\n5. https://zhuanlan.zhihu.com/p/380013595\n6. https://github.com/facebook/rocksdb/wiki/Iterator-Implementation\n","slug":"lsm-tree","published":1,"updated":"2023-10-12T05:36:36.868Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clnmrc00c0002icc3cqnkhkfo","content":"<h1 id=\"LSM-Tree\"><a href=\"#LSM-Tree\" class=\"headerlink\" title=\"LSM Tree\"></a>LSM Tree</h1><h2 id=\"Feature\"><a href=\"#Feature\" class=\"headerlink\" title=\"Feature\"></a>Feature</h2><ul>\n<li>Ordered</li>\n<li>Block Storage(Disk)-oriented</li>\n<li>Hierarchical</li>\n</ul>\n<h2 id=\"Structure\"><a href=\"#Structure\" class=\"headerlink\" title=\"Structure\"></a>Structure</h2><p><img src=\"/Adooobe.github.io/lsm_tree.png#pic_center\" alt=\"LSM-Tree Structure\"></p>\n<h2 id=\"Workflow\"><a href=\"#Workflow\" class=\"headerlink\" title=\"Workflow\"></a>Workflow</h2><h3 id=\"WAL\"><a href=\"#WAL\" class=\"headerlink\" title=\"WAL\"></a>WAL</h3><p>When LSM Tree received a input(insert&#x2F;update&#x2F;delete) operation, to avoid accidental crashing or shutdown，It is neccessary to write ahead log(WAL) that saves operation records into log files.</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">type</span> Wal <span class=\"keyword\">struct</span> &#123;</span><br><span class=\"line\">\tf    *os.File</span><br><span class=\"line\">\tpath <span class=\"type\">string</span></span><br><span class=\"line\">\tlock sync.Locker</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Memtable\"><a href=\"#Memtable\" class=\"headerlink\" title=\"Memtable\"></a>Memtable</h3><p>Memtable is an append-only data structure (every node inserted cannot be changed by logical delete and removing duplicates based on update time.) like Binary search tree (e.g. RedBlack tree - LevelDB) or SkipList (e.g. HBase, more popular).</p>\n<h3 id=\"Sorted-String-Table-SSTables\"><a href=\"#Sorted-String-Table-SSTables\" class=\"headerlink\" title=\"Sorted String Table (SSTables)\"></a>Sorted String Table (SSTables)</h3><p>Usually, a SSTable consists of an index block and multiple data blocks. Data block structure is as follows:</p>\n<p><img src=\"/Adooobe.github.io/sstable.png#pic_center\" alt=\"Sorted Strings Table\"></p>\n<p>where data is only ordered in segment layer rather than global.</p>\n<h3 id=\"Optimization\"><a href=\"#Optimization\" class=\"headerlink\" title=\"Optimization\"></a>Optimization</h3><h4 id=\"improve-read-performance\"><a href=\"#improve-read-performance\" class=\"headerlink\" title=\"improve read performance\"></a>improve read performance</h4><p>For LSM Tree, what we concerned is read performance rather than write. As data increases, some methods to improve read performance need to come into our pictures.</p>\n<ul>\n<li><strong>Sparse Index</strong></li>\n</ul>\n<p>As mentioned above, SStable has several segments that data are orderly stored. Without any optimization, we can also use binary search algorithm to find a certain element by scanning all the SStables and every Segments in each SStable unitl finding it. But unfortunately, for binary search algorithm, the minimum memory unit is segment for us to find the key in <em>O(logn)</em> or spent a Disk IO, both are too expensive in big data scenarios. So it is neccessary to  build a sparse index in memory to accelerate query efficiency.</p>\n<p><img src=\"/Adooobe.github.io/sparse_index.png#pic_center\" alt=\"sparse index\"></p>\n<blockquote>\n<p>Sparse indexes only contain entries for documents that have the indexed field, even if the index field contains a null value. The index skips over any document that is missing the indexed field. The index is “sparse” because it does not include all documents of a collection.</p>\n</blockquote>\n<ul>\n<li><strong>Bloom Filter</strong></li>\n</ul>\n<p>When the number of SStable increases in Disk, if some key is not present in the records, we need to scan all the SStable to find that key. Bloom filter is to overcome this issue. Unlike sparse indexes, <a href=\"https://adooobe.github.io/2023/10/02/bloom/\">Bloom filters</a> are designed to address the performance issues that arise when querying for non-existent keys.</p>\n<h3 id=\"Compaction\"><a href=\"#Compaction\" class=\"headerlink\" title=\"Compaction\"></a>Compaction</h3><h4 id=\"questions-in-query\"><a href=\"#questions-in-query\" class=\"headerlink\" title=\"questions in query\"></a>questions in query</h4><p>Let’s talk about query in LSM Tree first. There are two query methods: <em>point lookup query</em> and <em>range query</em>.</p>\n<ul>\n<li><strong>point lookup query</strong>: find the element what we want from new segment to old one.</li>\n<li><strong>range query</strong>: when a big range query is executed, data have to be found in memtable, immutable memtable and multiple SSTalbes in different levels. (Notice: range query should be <strong>key range query</strong> like the follow picture)</li>\n</ul>\n<p><img src=\"/Adooobe.github.io/range_query.png#pic_center\" alt=\"LSM Tree range query\"></p>\n<p>During range reads, the iterator will seek to the start range similar to point lookup (using Binary search with in SSTs) using<code>SeekTo()</code> call. After seeking to start range, there will be series of iterators created one for each memtable, one for each Level-0 files (because of overlapping nature of SSTs in L0) and one for each level later on. A merging iterator will collect keys from each of these iterators and gives the data in sorted order till the End range is reached.</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">itr := txn.NewIterator(badger.DefaultIteratorOptions)   </span><br><span class=\"line\"><span class=\"keyword\">for</span> itr.Seek(<span class=\"string\">&quot;startKey&quot;</span>); itr.Valid(); itr.Next() &#123;</span><br><span class=\"line\">    item := itr.Item()</span><br><span class=\"line\">    key := item.Key()</span><br><span class=\"line\">    <span class=\"keyword\">if</span> bytes.Compare(key, <span class=\"string\">&quot;endKey&quot;</span>) &gt; <span class=\"number\">0</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">break</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// rest of the logic.</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>Step1. find <em>startkey</em> position (write as startPosition) by <code>seek()</code> and move sub iterator to <code>startPosition + 1</code></p>\n<p>Step2. compare the sub iterators’ element, return the minimal value and move the itr pointer.</p>\n<p>Step3. repeat Step2 until the returned element &gt; endkey</p>\n<p>So in range query, as SSTables become more and more, query execution also becomes heavier and heavier.</p>\n<h4 id=\"Sorted-Run\"><a href=\"#Sorted-Run\" class=\"headerlink\" title=\"Sorted Run\"></a>Sorted Run</h4><blockquote>\n<p>LSM tree organizes files into several sorted runs. A sorted run consists of one or multiple <a href=\"https://nightlies.apache.org/flink/flink-table-store-docs-release-0.3/docs/concepts/file-layouts/#data-files\">data file</a>s and each data file belongs to exactly one sorted run.</p>\n<p><img src=\"/Adooobe.github.io/sorted_runs.png#pic_center\" alt=\"sorted runs\"></p>\n<p>As you can see, different sorted runs may have overlapping primary key ranges, and may even contain the same primary key. When querying the LSM tree, all sorted runs must be combined and all records with the same primary key must be merged according to the user-specified <a href=\"https://nightlies.apache.org/flink/flink-table-store-docs-release-0.3/docs/features/table-types/#merge-engines\">merge engine</a> and the timestamp of each record.</p>\n</blockquote>\n<p>In my opinion, in LSM Tree, a single logically ordered and no-repeat structure can be regarded as a <code>sorted run</code>.</p>\n<h4 id=\"Methods\"><a href=\"#Methods\" class=\"headerlink\" title=\"Methods\"></a>Methods</h4><p><img src=\"/Adooobe.github.io/merge_policy.png#pic_center\" alt=\"LSM Tree merge policies\"></p>\n<ul>\n<li>tiered compaction(low write amplification)<br><img src=\"/Adooobe.github.io/size_tiered_compaction.png#pic_center\" alt=\"size_tiered_compaction\"><ul>\n<li>high read and space amplification</li>\n</ul>\n</li>\n<li>leveled compaction</li>\n</ul>\n<p>Like the picture above, leveled merge policies will merge SSTables into next level with the same range.</p>\n<h2 id=\"Improvement\"><a href=\"#Improvement\" class=\"headerlink\" title=\"Improvement\"></a>Improvement</h2><ul>\n<li><strong>bLSM</strong></li>\n<li><strong>Diff-Index LSM</strong></li>\n</ul>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li><a href=\"https://www.cnblogs.com/whuanle/p/16297025.html\">https://www.cnblogs.com/whuanle/p/16297025.html</a></li>\n<li><a href=\"https://www.geeksforgeeks.org/introduction-to-log-structured-merge-lsm-tree/\">https://www.geeksforgeeks.org/introduction-to-log-structured-merge-lsm-tree/</a></li>\n<li><a href=\"https://www.mongodb.com/docs/manual/core/index-sparse/#:~:text=Sparse%20indexes%20only%20contain%20entries,all%20documents%20of%20a%20collection\">https://www.mongodb.com/docs/manual/core/index-sparse/#:~:text=Sparse%20indexes%20only%20contain%20entries,all%20documents%20of%20a%20collection</a>.</li>\n<li><a href=\"https://hzhu212.github.io/posts/2d7c5edb/\">https://hzhu212.github.io/posts/2d7c5edb/</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/380013595\">https://zhuanlan.zhihu.com/p/380013595</a></li>\n<li><a href=\"https://github.com/facebook/rocksdb/wiki/Iterator-Implementation\">https://github.com/facebook/rocksdb/wiki/Iterator-Implementation</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"LSM-Tree\"><a href=\"#LSM-Tree\" class=\"headerlink\" title=\"LSM Tree\"></a>LSM Tree</h1><h2 id=\"Feature\"><a href=\"#Feature\" class=\"headerlink\" title=\"Feature\"></a>Feature</h2><ul>\n<li>Ordered</li>\n<li>Block Storage(Disk)-oriented</li>\n<li>Hierarchical</li>\n</ul>\n<h2 id=\"Structure\"><a href=\"#Structure\" class=\"headerlink\" title=\"Structure\"></a>Structure</h2><p><img src=\"/Adooobe.github.io/lsm_tree.png#pic_center\" alt=\"LSM-Tree Structure\"></p>\n<h2 id=\"Workflow\"><a href=\"#Workflow\" class=\"headerlink\" title=\"Workflow\"></a>Workflow</h2><h3 id=\"WAL\"><a href=\"#WAL\" class=\"headerlink\" title=\"WAL\"></a>WAL</h3><p>When LSM Tree received a input(insert&#x2F;update&#x2F;delete) operation, to avoid accidental crashing or shutdown，It is neccessary to write ahead log(WAL) that saves operation records into log files.</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">type</span> Wal <span class=\"keyword\">struct</span> &#123;</span><br><span class=\"line\">\tf    *os.File</span><br><span class=\"line\">\tpath <span class=\"type\">string</span></span><br><span class=\"line\">\tlock sync.Locker</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Memtable\"><a href=\"#Memtable\" class=\"headerlink\" title=\"Memtable\"></a>Memtable</h3><p>Memtable is an append-only data structure (every node inserted cannot be changed by logical delete and removing duplicates based on update time.) like Binary search tree (e.g. RedBlack tree - LevelDB) or SkipList (e.g. HBase, more popular).</p>\n<h3 id=\"Sorted-String-Table-SSTables\"><a href=\"#Sorted-String-Table-SSTables\" class=\"headerlink\" title=\"Sorted String Table (SSTables)\"></a>Sorted String Table (SSTables)</h3><p>Usually, a SSTable consists of an index block and multiple data blocks. Data block structure is as follows:</p>\n<p><img src=\"/Adooobe.github.io/sstable.png#pic_center\" alt=\"Sorted Strings Table\"></p>\n<p>where data is only ordered in segment layer rather than global.</p>\n<h3 id=\"Optimization\"><a href=\"#Optimization\" class=\"headerlink\" title=\"Optimization\"></a>Optimization</h3><h4 id=\"improve-read-performance\"><a href=\"#improve-read-performance\" class=\"headerlink\" title=\"improve read performance\"></a>improve read performance</h4><p>For LSM Tree, what we concerned is read performance rather than write. As data increases, some methods to improve read performance need to come into our pictures.</p>\n<ul>\n<li><strong>Sparse Index</strong></li>\n</ul>\n<p>As mentioned above, SStable has several segments that data are orderly stored. Without any optimization, we can also use binary search algorithm to find a certain element by scanning all the SStables and every Segments in each SStable unitl finding it. But unfortunately, for binary search algorithm, the minimum memory unit is segment for us to find the key in <em>O(logn)</em> or spent a Disk IO, both are too expensive in big data scenarios. So it is neccessary to  build a sparse index in memory to accelerate query efficiency.</p>\n<p><img src=\"/Adooobe.github.io/sparse_index.png#pic_center\" alt=\"sparse index\"></p>\n<blockquote>\n<p>Sparse indexes only contain entries for documents that have the indexed field, even if the index field contains a null value. The index skips over any document that is missing the indexed field. The index is “sparse” because it does not include all documents of a collection.</p>\n</blockquote>\n<ul>\n<li><strong>Bloom Filter</strong></li>\n</ul>\n<p>When the number of SStable increases in Disk, if some key is not present in the records, we need to scan all the SStable to find that key. Bloom filter is to overcome this issue. Unlike sparse indexes, <a href=\"https://adooobe.github.io/2023/10/02/bloom/\">Bloom filters</a> are designed to address the performance issues that arise when querying for non-existent keys.</p>\n<h3 id=\"Compaction\"><a href=\"#Compaction\" class=\"headerlink\" title=\"Compaction\"></a>Compaction</h3><h4 id=\"questions-in-query\"><a href=\"#questions-in-query\" class=\"headerlink\" title=\"questions in query\"></a>questions in query</h4><p>Let’s talk about query in LSM Tree first. There are two query methods: <em>point lookup query</em> and <em>range query</em>.</p>\n<ul>\n<li><strong>point lookup query</strong>: find the element what we want from new segment to old one.</li>\n<li><strong>range query</strong>: when a big range query is executed, data have to be found in memtable, immutable memtable and multiple SSTalbes in different levels. (Notice: range query should be <strong>key range query</strong> like the follow picture)</li>\n</ul>\n<p><img src=\"/Adooobe.github.io/range_query.png#pic_center\" alt=\"LSM Tree range query\"></p>\n<p>During range reads, the iterator will seek to the start range similar to point lookup (using Binary search with in SSTs) using<code>SeekTo()</code> call. After seeking to start range, there will be series of iterators created one for each memtable, one for each Level-0 files (because of overlapping nature of SSTs in L0) and one for each level later on. A merging iterator will collect keys from each of these iterators and gives the data in sorted order till the End range is reached.</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">itr := txn.NewIterator(badger.DefaultIteratorOptions)   </span><br><span class=\"line\"><span class=\"keyword\">for</span> itr.Seek(<span class=\"string\">&quot;startKey&quot;</span>); itr.Valid(); itr.Next() &#123;</span><br><span class=\"line\">    item := itr.Item()</span><br><span class=\"line\">    key := item.Key()</span><br><span class=\"line\">    <span class=\"keyword\">if</span> bytes.Compare(key, <span class=\"string\">&quot;endKey&quot;</span>) &gt; <span class=\"number\">0</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">break</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// rest of the logic.</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>Step1. find <em>startkey</em> position (write as startPosition) by <code>seek()</code> and move sub iterator to <code>startPosition + 1</code></p>\n<p>Step2. compare the sub iterators’ element, return the minimal value and move the itr pointer.</p>\n<p>Step3. repeat Step2 until the returned element &gt; endkey</p>\n<p>So in range query, as SSTables become more and more, query execution also becomes heavier and heavier.</p>\n<h4 id=\"Sorted-Run\"><a href=\"#Sorted-Run\" class=\"headerlink\" title=\"Sorted Run\"></a>Sorted Run</h4><blockquote>\n<p>LSM tree organizes files into several sorted runs. A sorted run consists of one or multiple <a href=\"https://nightlies.apache.org/flink/flink-table-store-docs-release-0.3/docs/concepts/file-layouts/#data-files\">data file</a>s and each data file belongs to exactly one sorted run.</p>\n<p><img src=\"/Adooobe.github.io/sorted_runs.png#pic_center\" alt=\"sorted runs\"></p>\n<p>As you can see, different sorted runs may have overlapping primary key ranges, and may even contain the same primary key. When querying the LSM tree, all sorted runs must be combined and all records with the same primary key must be merged according to the user-specified <a href=\"https://nightlies.apache.org/flink/flink-table-store-docs-release-0.3/docs/features/table-types/#merge-engines\">merge engine</a> and the timestamp of each record.</p>\n</blockquote>\n<p>In my opinion, in LSM Tree, a single logically ordered and no-repeat structure can be regarded as a <code>sorted run</code>.</p>\n<h4 id=\"Methods\"><a href=\"#Methods\" class=\"headerlink\" title=\"Methods\"></a>Methods</h4><p><img src=\"/Adooobe.github.io/merge_policy.png#pic_center\" alt=\"LSM Tree merge policies\"></p>\n<ul>\n<li>tiered compaction(low write amplification)<br><img src=\"/Adooobe.github.io/size_tiered_compaction.png#pic_center\" alt=\"size_tiered_compaction\"><ul>\n<li>high read and space amplification</li>\n</ul>\n</li>\n<li>leveled compaction</li>\n</ul>\n<p>Like the picture above, leveled merge policies will merge SSTables into next level with the same range.</p>\n<h2 id=\"Improvement\"><a href=\"#Improvement\" class=\"headerlink\" title=\"Improvement\"></a>Improvement</h2><ul>\n<li><strong>bLSM</strong></li>\n<li><strong>Diff-Index LSM</strong></li>\n</ul>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ol>\n<li><a href=\"https://www.cnblogs.com/whuanle/p/16297025.html\">https://www.cnblogs.com/whuanle/p/16297025.html</a></li>\n<li><a href=\"https://www.geeksforgeeks.org/introduction-to-log-structured-merge-lsm-tree/\">https://www.geeksforgeeks.org/introduction-to-log-structured-merge-lsm-tree/</a></li>\n<li><a href=\"https://www.mongodb.com/docs/manual/core/index-sparse/#:~:text=Sparse%20indexes%20only%20contain%20entries,all%20documents%20of%20a%20collection\">https://www.mongodb.com/docs/manual/core/index-sparse/#:~:text=Sparse%20indexes%20only%20contain%20entries,all%20documents%20of%20a%20collection</a>.</li>\n<li><a href=\"https://hzhu212.github.io/posts/2d7c5edb/\">https://hzhu212.github.io/posts/2d7c5edb/</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/380013595\">https://zhuanlan.zhihu.com/p/380013595</a></li>\n<li><a href=\"https://github.com/facebook/rocksdb/wiki/Iterator-Implementation\">https://github.com/facebook/rocksdb/wiki/Iterator-Implementation</a></li>\n</ol>\n"}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"clnmrc00c0001icc385lefpc2","tag_id":"clnmrc00d0003icc3cfj328fr","_id":"clnmrc00e0005icc3h7r686do"},{"post_id":"clnmrc00c0002icc3cqnkhkfo","tag_id":"clnmrc00e0004icc3bwue5atu","_id":"clnmrc00e0006icc3emjt94c9"}],"Tag":[{"name":"Bloom-Filter","_id":"clnmrc00d0003icc3cfj328fr"},{"name":"LSM Storage","_id":"clnmrc00e0004icc3bwue5atu"}]}}